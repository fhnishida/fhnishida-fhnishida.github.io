---
date: "2018-09-09T00:00:00Z"
# icon: book
# icon_pack: fas
linktitle: Optimization
summary: The author covers topics such as grid search and steepest ascent methods for optimization to show three approaches to reach the OLS estimation formula. The page also includes examples and code snippets to illustrate the concepts discussed.
title: Optimization
weight: 7
output: md_document
type: book
---




## Otimiza√ß√£o num√©rica
- Essa se√ß√£o tem o objetivo para dar uma intui√ß√£o sobre alguns algoritmos de otimiza√ß√£o.
- Veremos os m√©todos de _grid search_ e _gradient descent_ (_ascent_) que representam fam√≠lias de m√©todos de otimiza√ß√£o.


### M√©todos livres de derivadas

#### _Grid Search_

- O m√©todo mais simples de otimiza√ß√£o num√©rica √© o _grid search_.
- Como o computador n√£o lida com problemas com infinitos valores, discretizamos diversos poss√≠veis valores dos par√¢metros de escolha dentro de intervalos.
- Para cada poss√≠vel combina√ß√£o de par√¢metros, calcula-se a fun√ß√£o objetivo e escolhe-se a combina√ß√£o de par√¢metros que maximizam (ou minimizam) a fun√ß√£o objetivo.
- O exemplo abaixo considera apenas um par√¢metro de escolha {{<math>}}$\theta${{</math>}} e, para cada ponto escolhido dentro do intervalo {{<math>}}$[-1, 1]${{</math>}}, calcula-se a fun√ß√£o objetivo:

<center><img src="../grid_search.png"></center>

- Este √© um m√©todo robusto a fun√ß√µes com descontinuidades e quinas (n√£o diferenci√°veis).
- Por√©m, este m√©todo depende da defini√ß√£o de intervalo para busca do valor √≥timo e fica mais preciso com maiores quantidades de pontos.
- Ent√£o, como √© necess√°rio fazer o c√°lculo da fun√ß√£o objetivo para cada ponto, o _grid search_ tende a ser menos eficiente computacionalmente, sobretudo com o aumento de dimens√µes:

<center><img src="../multigrid_search.png" width=60%></center>



#### Nelder-Mead
- [Stats 102A Lesson 8-2 Nelder Mead Method / Algorithm](https://www.youtube.com/watch?v=vOYlVvT3W80)
- Nelder-Mead tamb√©m conhecido como m√©todo simplex downhill, √© um m√©todo de busca direta que √© frequentemente aplicado a problemas de otimiza√ß√£o n√£o linear para os quais as derivadas podem n√£o ser conhecidas.
- Ele opera em um simplex de _n + 1_ pontos em um espa√ßo _n_-dimensional e move e transforma iterativamente o simplex para encontrar o m√≠nimo ou m√°ximo de uma fun√ß√£o objetivo.

<center><img src="../nelder-mead_iter.png" width=60%></center>
<center><img src="../nelder-mead_example.gif" ></center>


<!-- #### _Simulated Annealing_ (SANN) -->
<!-- - O _simulated annealing_ √© um algoritmo de otimiza√ß√£o probabil√≠stico que busca aproximar o √≥timo global de uma fun√ß√£o dada. -->
<!-- - O nome do algoritmo vem do recozimento (_annealing_) em metalurgia, uma t√©cnica que envolve o aquecimento e resfriamento controlado de um material para alterar suas propriedades f√≠sicas. -->
<!-- - O algoritmo come√ßa com uma solu√ß√£o inicial e, em seguida, melhora iterativamente a solu√ß√£o atual perturbando-a aleatoriamente e aceitando a perturba√ß√£o com uma certa probabilidade. A probabilidade de aceitar uma solu√ß√£o pior √© inicialmente alta e diminui gradualmente √† medida que o n√∫mero de itera√ß√µes aumenta. -->

<!-- <center><img src="../sann.gif" width=60%></center> -->



### M√©todos baseados em gradiente
- [BFGS in a Nutshell: An Introduction to Quasi-Newton Methods](https://towardsdatascience.com/bfgs-in-a-nutshell-an-introduction-to-quasi-newton-methods-21b0e13ee504)
- H√° uma outra fam√≠lia de algoritmos de otimiza√ß√£o que utilizam o gradiente


#### _Gradient Ascent (Descent)_
- O algoritmo desta fam√≠lia mais simples √© o _gradient ascent_ (_descent_).
- Queremos encontrar o {{<math>}}${\theta}^{**}${{</math>}} que √© o par√¢metro que maximiza globalmente a fun√ß√£o objetivo
- Passos para encontrar um m√°ximo:
  1. Comece com algum valor inicial de par√¢metro, {{<math>}}${\theta}^0${{</math>}}
  2. Calcula-se o gradiente (vetor de derivadas parciais) e a hessiana (matriz de segundas derivadas parciais) e avalia-se a possibilidade de "andar para cima" a um valor mais alto
  3. Caso possa, anda para {{<math>}}${\theta}^1${{</math>}}
  {{<math>}}$$\theta^1 = \theta^0 + \alpha f'(\theta^0)$${{</math>}}
  ou, no caso multivariado:
  {{<math>}}$$\boldsymbol{\theta}^1 = \boldsymbol{\theta}^0 + \alpha \nabla f(\boldsymbol{\theta}^0),$${{</math>}}
  em que {{<math>}}$\nabla f(\cdot)${{</math>}} √© o gradiente (vetor de derivadas parciais).
  4. Repita os passos (2) e (3), andando para um novo {{<math>}}${\theta}^2, {\theta}^3, ...${{</math>}} at√© atingir um ponto m√°ximo

<center><img src="../steepest_ascent.png"></center>

- Note que esse m√©todo de otimiza√ß√£o √© sens√≠vel ao par√¢metro inicial e √†s descontinuidades da fun√ß√£o objetivo.
    - No exemplo, se os chutes iniciais forem {{<math>}}${\theta}^0_A${{</math>}} ou {{<math>}}${\theta}^0_B${{</math>}}, ent√£o consegue atingir o m√°ximo global.
    - J√° se o chute inicial for {{<math>}}${\theta}^0_C${{</math>}}, ent√£o ele acaba atingindo um m√°ximo local com {{<math>}}${\theta}^*${{</math>}} (menor do que o m√°ximo global em {{<math>}}${\theta}^{**}${{</math>}}).

<video width="500px" height="500px" controls="controls"/>
    <source src="../local-maxima.mp4" type="video/mp4">
</video>

- Por outro lado, √© um m√©todo mais eficiente, pois calcula-se a fun√ß√£o objetivo uma vez a cada passo, al√©m de ser mais preciso nas estima√ß√µes.


#### M√©todo de Newton
- O m√©todo de Newton √© um algoritmo de segunda ordem que usa tanto o gradiente quanto a matriz Hessiana da fun√ß√£o objetivo para iterativamente atualizar a solu√ß√£o.
- Agora, a segunda derivada permite dar "passos" mais otimizados, acelerando a converg√™ncia:
{{<math>}}$$\theta^{n+1} = \theta^n + \frac{1}{f''(\theta^n)} f'(\theta^n)$${{</math>}}
  ou, no caso multivariado:
  {{<math>}}$$\boldsymbol{\theta}^{n+1} = \boldsymbol{\theta}^n + \mathcal{H}^{-1}(\theta^n) \nabla f(\boldsymbol{\theta}^n),$${{</math>}}
  em que {{<math>}}$\mathcal{H}(\cdot)${{</math>}} √© a Hessiana (matriz de segundas derivadas parciais).

<center><img src="../gradient_newton.png"></center>


#### M√©todos de quasi-Newton
- Como o c√°lculo da Hessiana (e a sua invers√£o) √© computacionalmente demandante, diversos m√©todos prop√µem c√°lculos para aproxima√ß√µes da Hessiana a partir do gradiente para agilizar o algoritmo.
- A qualidade da aproxima√ß√£o da matriz Hessiana pode afetar a efic√°cia destes m√©todos e suas taxas de converg√™ncia.
- Alguns exemplos s√£o:
  - `BFGS` (Boryden-Fletcher-Goldfarb-Shanno): um dos m√©todos quasi-newtonianos mais populares
  - `nlminb` (Nonlinear Minimization subject to Box Constraints): otimiza√ß√£o sem restri√ß√µes ou com restri√ß√µes de caixa usando rotinas PORT do FORTRAN. 





</br>

## Encontrando MQO por diferentes estrat√©gias
- Nesta se√ß√£o, encontraremos as estimativas de MQO usando as estrat√©gias da (a) minimiza√ß√£o da fun√ß√£o perda, de (b) m√©todo dos momentos e de (c) m√°xima verossimilhan√ßa.
- Em cada uma delas, temos uma fun√ß√£o objetivo distinta, que ser√° avaliada a partir de um vetor com dois par√¢metros, {{<math>}}$ \hat{\boldsymbol{\theta}} = \{ \hat{\beta}_0, \hat{\beta}_1 \}. ${{</math>}} No R, vamos chamar esse vetor de `theta`.



### Base `mtcars`

Usaremos dados extra√≠dos da _Motor Trend_ US magazine de 1974, que analisa o
consumo de combust√≠vel e 10 aspectos t√©cnicos de 32 autom√≥veis.

No _R_, a base de dados `mtcars` j√° est√° pr√©-carregada no programa e queremos estimar o seguinte modelo:
{{<math>}} $$ \text{mpg} = \beta_0 + \beta_1 \text{hp} + \varepsilon, $$ {{</math>}}
em que:

- _mpg_: consumo de combust√≠vel (milhas por gal√£o)
- _hp_: pot√™ncia (cavalos-vapor)


```r
## Regressao MQO
reg = lm(formula = mpg ~ hp, data = mtcars)
reg$coef
```

```
## (Intercept)          hp 
## 30.09886054 -0.06822828
```



### (a) Minimiza√ß√£o da fun√ß√£o perda
- A fun√ß√£o perda adotada pela Teoria da Decis√£o √© a **fun√ß√£o de soma dos quadrados dos res√≠duos**
- Por essa estrat√©gia, queremos encontrar as estimativas que **minimizam** essa fun√ß√£o.


#### 1. Criar fun√ß√£o perda que calcula a soma dos res√≠duos quadr√°ticos
- A fun√ß√£o para calcular a soma dos res√≠duos quadr√°ticos recebe como inputs:
  - um **vetor** de poss√≠veis valores {{<math>}}$\hat{\boldsymbol{\theta}} = \left\{ \hat{\beta}_0,\ \hat{\beta}_1 \right\}${{</math>}}
  - uma **lista** com
    - um *texto* com o nome da vari√°vel dependente
    - um *vetor de texto* com os nomes das vari√°veis explicativas
    - uma *base de dados*

```r
resid_quad = function(theta, fn_args) {
  # Extraindo argumentos da lista fn_args
  yname = fn_args[[1]]
  xname = fn_args[[2]]
  dta = fn_args[[3]]
  
  # Extraindo as vari√°veis da base em vetores
  y = dta[,yname]
  x = dta[,xname]
  
  # Extraindo os par√¢metros de theta
  b0hat = theta[1]
  b1hat = theta[2]
  
  yhat = b0hat + b1hat * x # valores ajustados
  ehat = y - yhat # desvios = observados - ajustados
  sum(ehat^2)
}
```


#### 2. Otimiza√ß√£o
- Agora encontraremos os par√¢metros que minimizam a fun√ß√£o perda

{{<math>}}$$ \underset{\hat{\boldsymbol{\theta}}}{\text{argmin}} \sum_{i=1}^{N}\hat{\varepsilon}^2_i \quad = \quad \underset{\hat{\boldsymbol{\theta}}}{\text{argmin}} \sum_{i=1}^{N}\left( \text{mpg}_i - \widehat{\text{mpg}}_i \right)^2 $${{</math>}}

- Para isto usaremos a fun√ß√£o `opm()` do pacote `optimx` que retorna os par√¢metros que minimizam uma fun√ß√£o (equivalente ao _argmin_):
```yaml
opm(par, fn, gr=NULL, hess=NULL, lower=-Inf, upper=Inf, 
            method=c("Nelder-Mead","BFGS"), hessian=FALSE,
            control=list(),
             ...)

- par: a vector of initial values for the parameters.
- fn: A function to be minimized (or maximized), with a first argument the vector of parameters over which minimization is to take place. It should return a scalar result.
- gr: A function to return (as a vector) the gradient for those methods that can use this information.
- hess: A function to return (as a symmetric matrix) the Hessian of the objective function for those methods that can use this information.
- lower, upper: Bounds on the variables for methods such as "L-BFGS-B" that can handle box (or bounds) constraints. These are vectors.
- method: A vector of the methods to be used, each as a character string. Possible method codes are "Nelder-Mead", "BFGS", "CG", "L-BFGS-B", "nlm", "nlminb", "spg", "ucminf", "newuoa", "bobyqa", "nmkb", "hjkb", "Rcgmin", and/or "Rvmmin". It may be needed to install some optimization packages to perform them.
- hessian: A logical control that if TRUE forces the computation of an approximation to the Hessian at the final set of parameters.
- control: A list of control parameters. See ‚ÄòDetails‚Äô.
```
- Colocaremos como input:
  - a fun√ß√£o perda criada `resid_quad()`
  - um chute inicial dos par√¢metros
    - Note que a estima√ß√£o pode ser mais ou menos sens√≠vel ao valores iniciais, dependendo do m√©todo de otimiza√ß√£o utilizado
    - O mais comum √© encontrar como chute inicial um vetor de zeros `c(0, 0)`, por ser mais neutro em rela√ß√£o ao sinal das estimativas
  - Por padr√£o, temos o argumento `hessian = FALSE`, coloque `TRUE` se quiser calcular o erro padr√£o, estat√≠stica t e p-valor das estimativas.


```r
# Estima√ß√£o por BFGS
theta_ini = c(0, 0) # Chute inicial de b0, b1

min_loss = optimx::opm(par=theta_ini, fn=resid_quad,
                      fn_args=list("mpg", "hp", mtcars),
                      method=c("Nelder-Mead", "BFGS", "nlminb"))
round(min_loss, 4)
```

```
##                  p1      p2    value fevals gevals convergence kkt1 kkt2 xtime
## Nelder-Mead 30.0964 -0.0682 447.6744     93     NA           0    0    1  0.03
## BFGS        30.0989 -0.0682 447.6743     31      5           0    1    1  0.00
## nlminb      30.0989 -0.0682 447.6743     11     16           0    1    1  0.00
```


</br>

### (b) M√©todo dos Momentos
- [Computing Generalized Method of Moments and Generalized Empirical Likelihood with R (Pierre Chauss√©)](https://cran.r-project.org/web/packages/gmm/vignettes/gmm_with_R.pdf)
- [Generalized Method of Moments (GMM) in R - Part 1 (Alfred F. SAM)](https://medium.com/codex/generalized-method-of-moments-gmm-in-r-part-1-of-3-c65f41b6199)


- Para estimar via GMM com **dois momentos** precisamos construir vetores relacionados aos seguintes momentos:

{{<math>}}$$ E(\boldsymbol{\varepsilon}) = 0 \qquad \text{ e } \qquad E(\boldsymbol{x \varepsilon}) = 0 $${{</math>}}

- Note que estes s√£o os momentos relacionados ao MQO, dado que este √© um caso particular do GMM.
- Os an√°logos amostrais s√£o:

{{<math>}}$$ \frac{1}{N} \sum^N_{i=1}{\hat{\varepsilon}_i} = 0 \qquad \text{ e } \qquad \frac{1}{N} \sum^N_{i=1}{x_i.\hat{\varepsilon}_i} = 0 $${{</math>}}

- Podemos calcular estes dois momentos amostrais em uma √∫nica multiplica√ß√£o matricial.
- Primeiro, considere:

{{<math>}}$$ \hat{\boldsymbol{\varepsilon}} = \begin{bmatrix} \varepsilon_1 \\ \varepsilon_2 \\ \vdots \\ \varepsilon_N \end{bmatrix} \qquad \text{e} \qquad \boldsymbol{x} = \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_N \end{bmatrix} $${{</math>}}

- Vamos juntar uma coluna de 1's com {{<math>}}$\boldsymbol{x}${{</math>}} e definir a matriz
{{<math>}}$$ \boldsymbol{X} = \begin{bmatrix} 1 & x_1 \\ 1 & x_2 \\ \vdots & \vdots \\ 1 & x_N \end{bmatrix} $${{</math>}}

- Fazendo a multiplica√ß√£o matricial entre {{<math>}}$\hat{\boldsymbol{\varepsilon}}${{</math>}} e {{<math>}}$\boldsymbol{X}${{</math>}}, temos o vetor dos momentos amostrais:

{{<math>}}\begin{align} \boldsymbol{m} \equiv \boldsymbol{X}' \hat{\boldsymbol{\varepsilon}} &= \begin{bmatrix} 1 & 1 & \cdots & 1 \\ x_1 & x_2 & \cdots & x_N  \end{bmatrix} \begin{bmatrix} \hat{\varepsilon}_1 \\ \hat{\varepsilon}_2 \\ \vdots \\ \hat{\varepsilon}_N \end{bmatrix} \\\
&= \begin{bmatrix}  \sum^N_{i=1}{\hat{\varepsilon}_i} \\ \sum^N_{i=1}{x_i.\hat{\varepsilon}_i} \end{bmatrix}  \propto \begin{bmatrix} \frac{1}{N} \sum^N_{i=1}{\hat{\varepsilon}_i} \\ \frac{1}{N} \sum^N_{i=1}{x_i.\hat{\varepsilon}_i} \end{bmatrix} \end{align}{{</math>}}

- Agora, suponha a matriz de pesos (cuja soma n√£o precisa ser igual a 1)
{{<math>}}$$ W = \begin{bmatrix} \alpha & 0 \\ 0 & \beta \end{bmatrix} $${{</math>}}
em que {{<math>}}$\alpha${{</math>}} e {{<math>}}$\beta${{</math>}} s√£o dois escalares.

- No GMM, queremos fazer com que esses momentos sejam o mais pr√≥ximos de zero. Um forma de fazer isso √© minimizar a soma (ponderada) dos quadrados dos momentos:

{{<math>}}\begin{align} m' W m &= \begin{bmatrix} \sum^N_{i=1}{\hat{\varepsilon}_i} & \sum^N_{i=1}{x_i.\hat{\varepsilon}_i} \end{bmatrix} \begin{bmatrix} \alpha & 0 \\ 0 & \beta \end{bmatrix} \begin{bmatrix} \sum^N_{i=1}{\hat{\varepsilon}_i} \\ \sum^N_{i=1}{x_i.\hat{\varepsilon}_i} \end{bmatrix} \\
&= \begin{bmatrix} \alpha \sum^N_{i=1}{\hat{\varepsilon}_i} & \beta \sum^N_{i=1}{x_i.\hat{\varepsilon}_i} \end{bmatrix} \begin{bmatrix} \sum^N_{i=1}{\hat{\varepsilon}_i} \\ \sum^N_{i=1}{x_i.\hat{\varepsilon}_i} \end{bmatrix} \\
&= \alpha \left(\sum^N_{i=1}{\hat{\varepsilon}_i}\right)^2 + \beta \left(\sum^N_{i=1}{x_i.\hat{\varepsilon}_i}\right)^2
\end{align}{{</math>}}

- Note que, usamos o quadrado dos momentos amostrais, pois minimizar o valor absoluto tende a formar "quinas" (ponto n√£o diferenci√°veis) na fun√ß√£o objetivo.



#### Otimiza√ß√£o Num√©rica para GMM

##### 1. Chute de valores iniciais para {{<math>}}$\hat{\beta}_0${{</math>}} e {{<math>}}$\hat{\beta}_1${{</math>}}


```r
theta = c(30, -0.05)
yname = "mpg"
xname = "hp"
dta = mtcars
```

##### 2. Sele√ß√£o da base de dados e vari√°veis

```r
# Extraindo as vari√°veis da base em vetores
y = dta[,yname]
x = dta[,xname]

# Extraindo os par√¢metros de theta
b0hat = theta[1]
b1hat = theta[2]
```

##### 3. C√°lculo dos valores ajustados e dos res√≠duos

```r
## Valores ajustados de y
yhat = b0hat + b1hat * x

## Res√≠duos
ehat = y - yhat
```


##### 4. Soma dos quadrados dos momentos amostrais

```r
m1 = ehat # momento 1
m2 = ehat * x # momento 2

sum(m1)^2 + sum(m2)^2 # soma dos quadrados com mesmos pesos (1 e 1)
```

```
## [1] 217374633
```
- Note que, como multiplicamos a constante igual a 1 com os res√≠duos {{<math>}}$\hat{\varepsilon}${{</math>}}, a 1¬™ coluna corresponde ao momento amostral {{<math>}}$\sum^N_{i=1}{\hat{\varepsilon}_i}${{</math>}} (mas sem dividir por _N_).
- J√° a coluna 2 correspode ao momento amostral {{<math>}}$\sum^N_{i=1}{x_i.\hat{\varepsilon}_i}${{</math>}} para a vari√°vel _hp_ (mas sem dividir por _N_).
- Logicamente, para estimar por GMM, precisamos escolher os par√¢metros {{<math>}}$\hat{\boldsymbol{\theta}} = \{ \hat{\beta}_0, \hat{\beta}_1 \}${{</math>}} que, ao calcular a soma/m√©dia das colunas, se aproximem ao m√°ximo de zero. Isso ser√° feito via `gmm()` (semelhante √† fun√ß√£o `opm()`)


##### 5a. Cria√ß√£o de fun√ß√£o com os momentos
- Vamos criar uma fun√ß√£o que tem como input um vetor de par√¢metros (`theta`) e uma base de dados (`dta`), e que retorna uma matriz em que cada coluna representa um momento.
- Essa fun√ß√£o incluir√° todos os comandos descritos nos itens 1 a 4 (que, na verdade, apenas foram feitos por did√°tica).

```r
mom_ols1 = function(theta, fn_args) {
  # No gmm(), s√≥ pode ter 1 input dos argumentos dessa fun√ß√£o
  # Extraindo argumentos da lista fn_args
  yname = fn_args[[1]]
  xname = fn_args[[2]]
  dta = fn_args[[3]]
  
  # Extraindo as vari√°veis da base em vetores
  y = dta[,yname]
  x = dta[,xname]
  
  # Extraindo os par√¢metros de theta
  b0hat = theta[1]
  b1hat = theta[2]
  
  ## Valores ajustados de y
  yhat = b0hat + b1hat * x
  
  ## Res√≠duos
  ehat = y - yhat
  
  ## Momentos
  m1 = ehat # momento 1
  m2 = ehat * x # momento 2
  sum(m1)^2 + sum(m2)^2 # soma dos quadrados com mesmos pesos (1 e 1)
}
```

##### 6a. Otimiza√ß√£o via `opm()`
- Assim como na minimiza√ß√£o da fun√ß√£o perda, vamos usar a fun√ß√£o `opm()` do pacote `optimx`

```r
theta_ini = c(0,0)
gmm1 = optimx::opm(theta_ini, fn=mom_ols1,
                   fn_args = list("mpg", "hp", mtcars),
                   method = c("Nelder-Mead", "BFGS", "nlminb"))
round(gmm1, 4)
```

```
##                  p1      p2    value fevals gevals convergence kkt1 kkt2 xtime
## Nelder-Mead  0.0320  0.1009 28256.45     39     NA           0    0    0  0.02
## BFGS        30.0989 -0.0682     0.00     66     11           0    0    0  0.01
## nlminb      30.0989 -0.0682     0.00     54     42           0    0    0  0.00
```


##### 5b. Cria√ß√£o de fun√ß√£o com os momentos para `gmm()`

- Note que {{<math>}}$X' \hat{\boldsymbol{\varepsilon}}${{</math>}} um vetor dos momentos amostrais, mas a fun√ß√£o `gmm()` exige uma matriz de dimens√£o {{<math>}}$g \times N${{</math>}}, sendo {{<math>}}$g${{</math>}} o n√∫mero de momentos e {{<math>}}$N${{</math>}} o tamanho da amostra.
- No R, precisamos fazer **multiplica√ß√£o elemento a elemento por linha** do vetor de res√≠duos {{<math>}}$\hat{\boldsymbol{\varepsilon}}${{</math>}} com a matriz de covariadas {{<math>}}$\boldsymbol{X}${{</math>}} (neste caso: constante _1_ e _hp_), na forma:

{{<math>}}\begin{align} \hat{\boldsymbol{\varepsilon}} \odot \boldsymbol{X}\ =\ \begin{bmatrix} \hat{\varepsilon}_1 \\ \hat{\varepsilon}_2 \\ \vdots \\ \hat{\varepsilon}_N \end{bmatrix} \odot \begin{bmatrix} 1 & x_1 \\ 1 & x_2 \\ \vdots & \vdots \\ 1 & x_N \end{bmatrix}  \ =\ &\begin{bmatrix} \hat{\varepsilon}_1 & x_1.\hat{\varepsilon}_1  \\ \hat{\varepsilon}_2 & x_2.\hat{\varepsilon}_2 \\ \vdots & \vdots \\ \hat{\varepsilon}_N & x_N.\hat{\varepsilon}_N \end{bmatrix}\\
\\ &\quad \Big\Downarrow \text{(Soma por coluna)} \\
&\begin{bmatrix}  \sum^N_{i=1}{\hat{\varepsilon}_i} & \sum^N_{i=1}{x_i.\hat{\varepsilon}_i} \end{bmatrix}, \end{align}{{</math>}}
em que {{<math>}}$\odot${{</math>}} denota a multiplica√ß√£o elemento a elemento por linha. Note que se fizermos as somas de cada coluna, obtemos os dois momentos amostrais.

Note que, para fazer o GMM no R, n√£o devemos fazer a soma/m√©dia de cada coluna (a pr√≥pria fun√ß√£o `gmm()` far√° isso).


```r
mom_ols2 = function(theta, fn_args) {
  # No gmm(), s√≥ pode ter 1 input dos argumentos dessa fun√ß√£o
  # Extraindo argumentos da lista fn_args
  yname = fn_args[[1]]
  xname = fn_args[[2]]
  dta = fn_args[[3]]
  
  # Extraindo as vari√°veis da base em vetores
  y = dta[,yname]
  x = dta[,xname]
  
  # Extraindo os par√¢metros de theta
  b0hat = theta[1]
  b1hat = theta[2]
  
  ## Valores ajustados de y
  yhat = b0hat + b1hat * x
  
  ## Res√≠duos
  ehat = y - yhat
  
  ## Matriz de momentos
  m = as.numeric(ehat) * cbind(1,x)
  m # output da fun√ß√£o
}
```


##### 6b. Otimiza√ß√£o via `gmm()`
- A fun√ß√£o `gmm()` do pacote `gmm`, assim como a `opm()`, recebe uma fun√ß√£o como argumento.
- No entanto, a fun√ß√£o que entra no `gmm()` deve gerar uma matriz como output, cujas somas/m√©dias das colunas queremos aproximar de zero.
- O argumento de fun√ß√£o de otimiza√ß√£o deve ser `fctopt = "nlminb"`, pois `fctopt = "optim"` √© mais inst√°vel
- Note que, al√©m do vetor de par√¢metros, a fun√ß√£o que entra como argumento (`gmm_ols()` neste caso) deve ter, no m√°ximo, mais um argumento.

```r
gmm2 = gmm::gmm(
  g=mom_ols2, 
  x=list("mpg", "hp", mtcars), # joga no 2o arg de ols_mom (fn_args)
  t0=c(0,0), # chute inicial de theta
  optfct = "nlminb" # fun√ß√£o de otimiza√ß√£o
  )
gmm2$coef
```

```
##    Theta[1]    Theta[2] 
## 30.09886038 -0.06822828
```

```r
# Comparando com estimativas via lm()
reg$coef
```

```
## (Intercept)          hp 
## 30.09886054 -0.06822828
```


</br>

### (c) M√°xima Verossimilhan√ßa
- [ResEcon 703](https://github.com/woerman/ResEcon703) - Week 6 (University of Massachusetts Amherst)
- A fun√ß√£o objetivo √© a fun√ß√£o de verossimilhan√ßa que, ao contr√°rio da fun√ß√£o de soma de quadrado dos res√≠duos, queremos maximiz√°-la


#### Intui√ß√£o do c√°lculo da fun√ß√£o de verossimilhan√ßa
- Apenas para ilustrar a constru√ß√£o da fun√ß√£o de verossimilhan√ßa, {{<math>}}$\mathcal{L}${{</math>}}, considere um modelo de probabilidade linear:
{{<math>}}$$ \text{am} = \beta_0 + \beta_1 \text{cyl} + \varepsilon, $${{</math>}}
em que _cyl_ √© a quantidade de cilindros do carro, e _am_ √© uma vari√°vel _dummy_ que √© igual a 1 se o carro for autom√°tico e 0 caso contr√°rio.

- Queremos encontrar {{<math>}}$\hat{\boldsymbol{\theta}} = \left\{ \hat{\beta}_0, \hat{\beta}_1 \right\}${{</math>}} que maximizam a fun√ß√£o de verossimilhan√ßa.
- Considere um chute de par√¢metros {{<math>}}$\hat{\boldsymbol{\theta}}_A = \left\{ \hat{\beta}^A_0 = 1.3, \hat{\beta}^A_1 = -0.14 \right\}${{</math>}} que gerem os seguintes valores preditos/ajustados (probabilidades):

<center><img src="../likelihood_A.png" width=80%></center>


- Logo, a verossimilhan√ßa, dado os par√¢metros {{<math>}}$\hat{\boldsymbol{\theta}}_A${{</math>}} √©
{{<math>}}$$ \mathcal{L}(\hat{\boldsymbol{\theta}}_A) = 46\% \times 46\% \times 74\% \times 54\% \times 82\% = 6,9\% $${{</math>}}

- Agora, considere um segundo chute de par√¢metros {{<math>}}$\hat{\boldsymbol{\theta}}_B = \left\{ \hat{\beta}^B_0=1.0, \hat{\beta}^B_1=-0.10 \right\}${{</math>}} que gerem as seguintes probabilidades:

<center><img src="../likelihood_B.png" width=80%></center>

- Ent√£o, a verossimilhan√ßa, dado {{<math>}}$\hat{\boldsymbol{\theta}}_B${{</math>}}, √©
{{<math>}}$$ \mathcal{L}(\hat{\boldsymbol{\theta}}_B) = 40\% \times 40\% \times 60\% \times 60\% \times 80\% = 4,6\% $${{</math>}}
- Como {{<math>}}$\mathcal{L}\left(\hat{\boldsymbol{\theta}}_A\right) = 6,9\% > 4,6\% = \mathcal{L}\left(\hat{\boldsymbol{\theta}}_B\right)${{</math>}}, ent√£o os par√¢metros {{<math>}}$\hat{\boldsymbol{\theta}}_A${{</math>}} se mostram mais adequados em rela√ß√£o a {{<math>}}$\hat{\boldsymbol{\theta}}_B${{</math>}}
- Na estrat√©gia de m√°xima verossimilhan√ßa (ML), escolhe-se o conjunto de par√¢metros {{<math>}}$\hat{\boldsymbol{\theta}}^*${{</math>}} que maximiza a fun√ß√£o de verossimilhan√ßa (ou log-verossimilhan√ßa).






#### Otimiza√ß√£o Num√©rica para M√°xima Verossimilhan√ßa

- Em nosso modelo
{{<math>}} $$ \text{mpg} = \beta_0 + \beta_1 \text{hp} + \varepsilon, $$ {{</math>}}
queremos estimar 3 par√¢metros
{{<math>}}$$ \hat{\boldsymbol{\theta}} = \left\{ \hat{\beta}_0, \hat{\beta}_1, \hat{\sigma} \right\}, $${{</math>}}
em que {{<math>}}$\hat{\sigma}${{</math>}} √© desvio padr√£o do res√≠duo.

- No modelo de probabilidade linear, as probabilidades usadas para calcular a verossimilhan√ßa s√£o os pr√≥prios valores ajustados (probabilidades) dos carros serem autom√°ticos (manuais), dado que s√£o autom√°ticos (manuais).
- J√° no modelo linear "comum", usamos a fun√ß√£o de densidade de probabilidade, a partir de uma distribui√ß√£o normal com uma vari√¢ncia {{<math>}}$\hat{\sigma}^2${{</math>}}, para avaliar a "probabilidade" de cada observa√ß√£o, {{<math>}}$y_i${{</math>}}, ser o valor ajustado {{<math>}}$\hat{y}_i${{</math>}}.

A fun√ß√£o log-verossimilhan√ßa √© dada por
{{<math>}}$$ \mathcal{l}(\hat{\boldsymbol{\theta}}) = \ln{L(y_i | x_i, \hat{\beta}_0, \hat{\beta}_1, \hat{\sigma})} = \sum^n_{i=1}{\ln{f(y_i | x_i, \hat{\beta}_0, \hat{\beta}_1, \hat{\sigma})}}, $${{</math>}}
em que a distribui√ß√£o condicional de cada {{<math>}}$y_i${{</math>}} √©

{{<math>}}$$ y_i | x_i \sim N(\hat{\beta}_0 + \hat{\beta}_1 x_i, \hat{\sigma}^2) $${{</math>}}
o que implica que 

{{<math>}}$$\varepsilon_i | x_i \sim N(0, \sigma^2)$${{</math>}}


<center><img src="../mle.jpg"></center> 

- Como demonstra a figura acima, assumimos que o erro {{<math>}}$\varepsilon${{</math>}} √© normalmente distribu√≠do para todo {{<math>}}$x${{</math>}}, com a mesma vari√¢ncia {{<math>}}$\sigma^2${{</math>}} (homocedasticidade)



#### Otimiza√ß√£o Num√©rica via `mle2()`

- Nosso objetivo √©
{{<math>}}$$ \underset{\hat{\boldsymbol{\theta}}}{\text{argmax}} \ \mathcal{l}(\hat{\boldsymbol{\theta}}) = \underset{\hat{\boldsymbol{\theta}}}{\text{argmax}} \sum^n_{i=1}{\ln{f(y_i | x_i, \hat{\beta}_0, \hat{\beta}_1, \hat{\sigma})}}, $${{</math>}}

- A fun√ß√£o `mle2()` do pacote `bbmle`, que ser√° usada para desempenhar a otimiza√ß√£o num√©rica, assim como `opm()`. Precisamos usar como input:
  - Alguns valores inicias dos par√¢metros, {{<math>}}$\hat{\boldsymbol{\theta}}^0 = \left\{ \hat{\beta}^0_0, \hat{\beta}^0_1, \hat{\sigma}^0 \right\}${{</math>}}
  - Uma fun√ß√£o que tome esses par√¢metros como argumento e calcule a 
log-verossimilhan√ßa, {{<math>}}$\ln{L(\boldsymbol{\hat{\boldsymbol{\theta}}})}${{</math>}}.

> Como as fun√ß√µes de otimiza√ß√£o costumam encontrar o m√≠nimo de uma fun√ß√£o objetivo, precisamos adaptar o output para o negativo fun√ß√£o de log-verossimilhan√ßa. Ao minimizar o negativo de log-lik, estamos maximizando log-lik.

<!-- <center><img src="../mle.jpg"></center> -->

Passos para estimar uma regress√£o por m√°xima verossimilhan√ßa:

1. Chutar valores iniciais de 
2. Calcular os valores ajustados, {{<math>}}$\hat{y}${{</math>}}
3. Calcular a densidade para cada {{<math>}}$y_i${{</math>}}, usando {{<math>}}$f(y_i | x_i, \hat{\beta}_0, \hat{\beta}_1, \hat{\sigma})${{</math>}}
4. Calcular a log-verossimilhan√ßa, {{<math>}}$\sum^n_{i=1}{\ln{f(y_i | x_i, \hat{\beta}_0, \hat{\beta}_1, \hat{\sigma})}}${{</math>}}


##### 1. Chute de valores iniciais para {{<math>}}$\hat{\beta}_0, \hat{\beta}_1${{</math>}} e {{<math>}}$\hat{\sigma}^2${{</math>}}
- Note que, diferente da estima√ß√£o por MQO, um dos par√¢metros a ser estimado via MLE √© a vari√¢ncia ({{<math>}}$\hat{\sigma}^2${{</math>}}).

```r
theta = c(30, -.05, 2)
# (b0hat, b1hat , sighat)
```


##### 2. Sele√ß√£o da base de dados e vari√°veis

```r
## Inicializando
yname = "mpg"
xname = "hp"
dta = mtcars

# Extraindo as vari√°veis da base em vetores
y = dta[,yname]
x = dta[,xname]

# Extraindo os par√¢metros de theta
b0hat = theta[1]
b1hat = theta[2]
sighat = theta[3]
```

##### 3. C√°lculo dos valores ajustados e das densidades

```r
## Calculando valores ajustados de y
yhat = b0hat + b1hat * x
head(yhat)
```

```
## [1] 24.50 24.50 25.35 24.50 21.25 24.75
```

##### 4. C√°lculo das densidades
{{<math>}}$$ f(y_i | x_i, \hat{\beta}_0, \hat{\beta}_1, \hat{\sigma}) $${{</math>}}

```r
## Calculando as densidades de probabilidade de cada linha
ypdf = dnorm(y, mean = yhat, sd = sighat)

head(round(ypdf, 4)) # Primeiros valores da densidade
```

```
## [1] 0.0431 0.0431 0.0885 0.0600 0.0885 0.0008
```

```r
prod(ypdf) # Verossimilhan√ßa
```

```
## [1] 1.400141e-61
```

```r
sum(log(ypdf)) # Log-Verossimilhan√ßa
```

```
## [1] -140.1211
```

- Agora, vamos juntar visualizar os 6 primeiros elementos dos objetos trabalhados:

```r
# Juntando os vetores e visualizando os primeiros valores
tab = data.frame(y, x, yhat, ypdf=round(ypdf, 4))
head(tab)
```

```
##      y   x  yhat   ypdf
## 1 21.0 110 24.50 0.0431
## 2 21.0 110 24.50 0.0431
## 3 22.8  93 25.35 0.0885
## 4 21.4 110 24.50 0.0600
## 5 18.7 175 21.25 0.0885
## 6 18.1 105 24.75 0.0008
```
- Como pode ser visto na base de dados juntada e nos gr√°ficos abaixo, quanto mais pr√≥ximo o valor ajustado for do valor observado de cada observa√ß√£o, maior ser√° a densidade/probabilidade.
<img src="/project/rec5004/sec7/_index_files/figure-html/unnamed-chunk-18-1.png" width="672" /><img src="/project/rec5004/sec7/_index_files/figure-html/unnamed-chunk-18-2.png" width="672" /><img src="/project/rec5004/sec7/_index_files/figure-html/unnamed-chunk-18-3.png" width="672" />
- Logo, a verossimilhan√ßa (produto de todas densidades de probabilidade) ser√° maior quanto mais pr√≥ximos forem os valores ajustados dos seus respectivos valores observados.



##### 5. Calculando a Log-Verossimilhan√ßa

A log-verossimilhan√ßa √© a soma do log de todas probabilidades:

{{<math>}}$$ \mathcal{l}(\hat{\beta}_0, \hat{\beta}_1, \hat{\sigma}) = \sum^{N}_{i=1}{\ln\left[ f(y_i | x_i, \hat{\beta}_0, \hat{\beta}_1, \hat{\sigma}) \right]} $${{</math>}}

```r
## Calculando a log-verossimilhanca
loglik = sum(log(ypdf))
loglik
```

```
## [1] -140.1211
```


##### 6a. Criando a Fun√ß√£o de Log-Verossimilhan√ßa para `opm()`

- Aqui, vamos *minimizar o negativo* da fun√ß√£o de log-verossimilhan√ßa
{{<math>}}$$ \min_{(\hat{\beta}_0, \hat{\beta}_1, \hat{\sigma})} -\sum^n_{i=1}{\ln{f(y_i | x_i, \hat{\beta}_0, \hat{\beta}_1, \hat{\sigma})}} = \max_{(\hat{\beta}_0, \hat{\beta}_1, \hat{\sigma})} \sum^n_{i=1}{\ln{f(y_i | x_i, \hat{\beta}_0, \hat{\beta}_1, \hat{\sigma})}} $${{</math>}}
- Juntando tudo que fizemos anteriormente, podemos criar uma fun√ß√£o no R que calcular a fun√ß√£o de log-verossimilhan√ßa.
- **IMPORTANTE**: Prefira j√° calcular a log-densidade de probabilidade direto do `dnorm()`, pois otimiza√ß√£o fica mais est√°vel.
- Isso n√£o foi feito anteriormente por quest√£o did√°tica, mas ser√° feito abaixo:


```r
## Criando fun√ß√£o para calcular log-verossimilhanca de OLS
loglik1 = function(theta, fn_args) {
  yname = fn_args[[1]]
  xname = fn_args[[2]]
  dta = fn_args[[3]]
  
  # Extraindo as vari√°veis da base em vetores
  y = dta[,yname]
  x = dta[,xname]
  
  # Extraindo os par√¢metros de theta
  b0hat = theta[1]
  b1hat = theta[2]
  sighat = theta[3]

  ## Calculando valores ajustados de y
  yhat = b0hat + b1hat * x
  
  ## Calculando as densidades de probabilidade de cada linha
  log_ypdf = dnorm(y, mean = yhat, sd = sighat, log = TRUE)
  
  ## Calculando a log-verossimilhanca
  loglik = sum(log_ypdf)
  
  ## Retornando o negativo da log-verossimilanca
  -loglik # Negativo, pois mle2() minimiza e queremos maximizar
}
```


##### 7a. Otimiza√ß√£o via `opm()`
- **IMPORTANTE**: o chute inicial do erro padr√£o dos erros (_sighat_) deve ser um valor alto, pois o R tem um certo limite de casas decimais e acaba aproximando para zero (0) as probabilidades muito baixas (e o produt√≥rio da f√≥rmula da Verossimilhan√ßa acaba ficando igual a zero).
- Similar aos anteriores:


```r
theta_ini = c(0, 0, 10)
mle1 = optimx::opm(par=theta_ini, fn=loglik1,
                   fn_args = list("mpg", "hp", mtcars),
                   method = c("Nelder-Mead", "BFGS", "nlminb"))
round(mle1, 4)
```

```
##                  p1      p2     p3   value fevals gevals convergence kkt1 kkt2
## Nelder-Mead 30.1003 -0.0682 3.7400 87.6193    196     NA           0    0    1
## BFGS        30.0989 -0.0682 3.7403 87.6193     52     20           0    1    1
## nlminb      30.0989 -0.0682 3.7403 87.6193     33     67           0    1    1
##             xtime
## Nelder-Mead  0.02
## BFGS         0.01
## nlminb       0.00
```

##### 6b. Criando a Fun√ß√£o de Log-Verossimilhan√ßa para `mle2()`
- A fun√ß√£o `mle2()` do pacote `bbmle`, assim como a `opm()`, recebe uma fun√ß√£o como argumento.
- A fun√ß√£o que entra como argumento (`loglik()` neste caso) deve ter apenas como argumentos apenas os par√¢metros que queremos otimizar. Al√©m disso, caso seja necess√°rio incluir algum outro argumento, deve ser inserido no argumento `data` da fun√ß√£o `mle2()` como um objeto _list_.


```r
## Criando fun√ß√£o para calcular log-verossimilhanca de OLS
loglik = function(b0hat, b1hat, sighat) {
  # Extraindo as vari√°veis da base em vetores
  y = dta[,yname]
  x = dta[,xname]

  ## Calculando valores ajustados de y
  yhat = b0hat + b1hat * x
  
  ## Calculando as densidades de probabilidade de cada linha
  log_ypdf = dnorm(y, mean = yhat, sd = sighat, log = TRUE)
  
  ## Calculando a log-verossimilhanca
  loglik = sum(log_ypdf)
  
  ## Retornando o negativo da log-verossimilanca
  -loglik # Negativo, pois mle2() minimiza e queremos maximizar
}
```


##### 7b. Otimiza√ß√£o via `mle2()`


```r
## Maximizando a fun√ß√£o log-verossimilhan√ßa de OLS
mle2 = bbmle::mle2(
  minuslogl=loglik,
  start=list(b0hat=0, b1hat=0, sighat=1),
  data=list(yname = "mpg", xname = "hp", dta = mtcars),
  hessian=T
  )
mle2
```

```
## 
## Call:
## bbmle::mle2(minuslogl = loglik, start = list(b0hat = 0, b1hat = 0, 
##     sighat = 1), data = list(yname = "mpg", xname = "hp", dta = mtcars), 
##     hessian.opts = T)
## 
## Coefficients:
##       b0hat       b1hat      sighat 
## 30.09536167 -0.06820922  3.74137621 
## 
## Log-likelihood: -87.62
```



</br>

{{< cta cta_text="üëâ Proceed to Multiple Regression" cta_link="../sec8" >}}
