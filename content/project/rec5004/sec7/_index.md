---
date: "2018-09-09T00:00:00Z"
# icon: book
# icon_pack: fas
linktitle: Optimization
summary: The author covers topics such as grid search and steepest ascent methods for optimization to show three approaches to reach the OLS estimation formula. The page also includes examples and code snippets to illustrate the concepts discussed.
title: Optimization
weight: 7
output: md_document
type: book
---




## Otimiza√ß√£o num√©rica
- Essa se√ß√£o tem o objetivo para dar uma intui√ß√£o sobre m√©todos de otimiza√ß√£o.
- Veremos os m√©todos de _grid search_ e _gradient ascent_ (_descent_) que representam fam√≠lias de m√©todos de otimiza√ß√£o.


### _Grid Search_

- O m√©todo mais simples de otimiza√ß√£o num√©rica √© o _grid search_ (discretiza√ß√£o).
- Como o R n√£o lida com problemas com infinitos valores, uma forma lidar com isso √© discretizando diversos poss√≠veis valores dos par√¢metros de escolha dentro de intervalos.
- Para cada poss√≠vel combina√ß√£o de par√¢metros, calculam-se diversos valores a partir da fun√ß√£o objetivo. De todos os valores calculados, escolhe-se a combina√ß√£o de par√¢metros que maximizam (ou minimizam) a fun√ß√£o objetivo.
- O exemplo abaixo considera apenas um par√¢metro de escolha {{<math>}}$\beta${{</math>}} e, para cada ponto escolhido dentro do intervalo {{<math>}}$(-1, 1)${{</math>}}, calcula-se a fun√ß√£o objetivo:

<center><img src="../grid_search.png"></center>


- Este √© um m√©todo robusto a fun√ß√µes com descontinuidades e quinas (n√£o diferenci√°veis), e menos sens√≠vel a chutes de valores iniciais. (ver m√©todo abaixo)
- Por√©m, este m√©todo fica preciso apenas com maiores quantidades de pontos e, como √© necess√°rio fazer o c√°lculo da fun√ß√£o objetivo para cada ponto, o _grid search_ tende a ser menos eficiente computacionalmente (demora mais tempo para calcular).


### _Gradient Ascent (Descent)_

- Conforme o n√∫mero de par√¢metros do modelo cresce, aumenta o n√∫mero de poss√≠veis combina√ß√µes entre par√¢metros e torna o processo computacional cada vez mais demandante.
- Uma forma mais eficiente de encontrar o conjunto de par√¢metros que otimizam a fun√ß√£o objetivo √© por meio do m√©todo _gradient ascent_ (_descent_).
- Queremos encontrar o {{<math>}}${\beta}^{**}${{</math>}} que √© o par√¢metro que maximiza globalmente a fun√ß√£o objetivo
- Passos para encontrar um m√°ximo:
  1. Comece com algum valor inicial de par√¢metro, {{<math>}}${\beta}^0${{</math>}}
  2. Calcula-se o gradiente (vetor de derivadas parciais das vari√°veis) e a hessiana (matriz de segundas derivadas parciais) e avalia-se a possibilidade de "andar para cima" a um valor mais alto
  3. Caso possa, ande na dire√ß√£o correta a {{<math>}}${\beta}^1${{</math>}}
    3.1. gradiente d√° a dire√ß√£o do passo
    3.2. hessiana d√° o tamanho do passo
  4. Repita os passos (2) e (3), andando para um novo {{<math>}}${\beta}^2, {\beta}^3, ...${{</math>}} at√© atingir um ponto m√°ximo

<center><img src="../steepest_ascent.png"></center>


- Note que esse m√©todo de otimiza√ß√£o √© sens√≠vel ao par√¢metro inicial e √†s descontinuidades da fun√ß√£o objetivo.
    - No exemplo, se os chutes iniciais forem {{<math>}}${\beta}^0_A${{</math>}} ou {{<math>}}${\beta}^0_B${{</math>}}, ent√£o consegue atingir o m√°ximo global.
    - J√° se o chute inicial for {{<math>}}${\beta}^0_C${{</math>}}, ent√£o ele acaba atingindo um m√°ximo local com {{<math>}}${\beta}^*${{</math>}} (menor do que o m√°ximo global em {{<math>}}${\beta}^{**}${{</math>}}).


<video width="500px" height="500px" controls="controls"/>
    <source src="../local-maxima.mp4" type="video/mp4">
</video>

- Por outro lado, √© um m√©todo mais eficiente, pois calcula-se a fun√ß√£o objetivo uma vez a cada passo, al√©m de ser mais preciso nas estima√ß√µes.



</br>

## Encontrando MQO por diferentes estrat√©gias
- Nesta se√ß√£o, encontraremos as estimativas de MQO usando as estrat√©gias da (a) minimiza√ß√£o da fun√ß√£o perda, de (b) m√©todo dos momentos e de (c) m√°xima verossimilhan√ßa.
- Em cada uma delas, temos uma fun√ß√£o objetivo distinta, que ser√° avaliada a partir de um vetor com dois par√¢metros, {{<math>}}$ \hat{\boldsymbol{\beta}} = \left\{ \hat{\beta}_0, \hat{\beta}_1 \right\} ${{</math>}}. No R, vamos chamar esse vetor de `params`.



### Base `mtcars`

Usaremos dados extra√≠dos da _Motor Trend_ US magazine de 1974, que analisa o
consumo de combust√≠vel e 10 aspectos t√©cnicos de 32 autom√≥veis.

No _R_, a base de dados `mtcars` j√° est√° pr√©-carregada no programa e queremos estimar o seguinte modelo:
{{<math>}} $$ \text{mpg} = \beta_0 + \beta_1 \text{hp} + \varepsilon, $$ {{</math>}}
em que:

- _mpg_: consumo de combust√≠vel (milhas por gal√£o)
- _hp_: pot√™ncia (cavalos-vapor)


```r
## Regressao MQO
reg = lm(formula = mpg ~ hp, data = mtcars)
reg$coef
```

```
## (Intercept)          hp 
## 30.09886054 -0.06822828
```



### (a) Minimiza√ß√£o da fun√ß√£o perda
- A fun√ß√£o perda adotada pela Teoria da Decis√£o √© a **fun√ß√£o de soma dos quadrados dos res√≠duos**
- Por essa estrat√©gia, queremos encontrar as estimativas, {{<math>}}$\hat{\boldsymbol{\beta}} = \left\{ \hat{\beta}_0,\ \hat{\beta}_1 \right\}${{</math>}}, que **minimizam** essa fun√ß√£o.


#### 1. Criar fun√ß√£o perda que calcula a soma dos res√≠duos quadr√°ticos
- A fun√ß√£o para calcular a soma dos res√≠duos quadr√°ticos recebe como inputs:
  - um **vetor** de poss√≠veis valores {{<math>}}$\hat{\boldsymbol{\beta}} = \left\{ \hat{\beta}_0,\ \hat{\beta}_1 \right\}${{</math>}}
  - um **texto** com o nome da vari√°vel dependente
  - um **vetor de texto** com os nomes dos regressores
  - uma base de dados

```r
resid_quad = function(params, yname, xname, data) {
  # Extraindo as vari√°veis da base em vetores
  y = data[,yname]
  x = data[,xname]
  
  # Extraindo os par√¢metros de params
  b0 = params[1]
  b1 = params[2]
  
  yhat = b0 + b1 * x # valores ajustados
  e_hat = y - yhat # desvios = observados - ajustados
  sum(e_hat^2)
}
```


#### 2. Otimiza√ß√£o
- Agora encontraremos os par√¢metros que minimizam a fun√ß√£o perda

{{<math>}}$$ \underset{\hat{\beta}_0, \hat{\beta}_1}{\text{argmin}} \sum_{i=1}^{N}\hat{\varepsilon}^2_i \quad = \quad \underset{\hat{\beta}_0, \hat{\beta}_1}{\text{argmin}} \sum_{i=1}^{N}\left( \text{mpg}_i - \widehat{\text{mpg}}_i \right)^2 $${{</math>}}

- Para isto usaremos a fun√ß√£o `optim()` que retorna os par√¢metros que minimizam uma fun√ß√£o (equivalente ao _argmin_):
```yaml
optim(par, fn, gr = NULL, ...,
      method = c("Nelder-Mead", "BFGS", "CG", "L-BFGS-B", "SANN", "Brent"),
      lower = -Inf, upper = Inf,
      control = list(), hessian = FALSE)

- par: Initial values for the parameters to be optimized over.
- fn: A function to be minimized (or maximized), with first argument the vector of parameters over which minimization is to take place. It should return a scalar result.
- method: The method to be used. See ‚ÄòDetails‚Äô. Can be abbreviated.
- hessian: Logical. Should a numerically differentiated Hessian matrix be returned?
```
- Colocaremos como input:
  - a fun√ß√£o perda criada `resid_quad()`
  - um chute inicial dos par√¢metros
    - Note que a estima√ß√£o pode ser mais ou menos sens√≠vel ao valores iniciais, dependendo do m√©todo de otimiza√ß√£o utilizado
    - O mais comum √© encontrar como chute inicial um vetor de zeros `c(0, 0)`, por ser mais neutro em rela√ß√£o ao sinal das estimativas
    - Em Econometria III, prof. Laurini recomendou usar m√©todo "Nelder-Mead" (padr√£o) com um chute inicial de zeros e, depois, usar suas estimativas como chute inicial para o m√©todo "BFGS".
  - Por padr√£o, temos o argumento `hessian = FALSE`, coloque `TRUE` se quiser calcular o erro padr√£o, estat√≠stica t e p-valor das estimativas.


```r
# Estima√ß√£o por BFGS
theta_ini = c(0, 0) # Chute inicial de b0, b1

min_loss = optim(par=theta_ini, fn=resid_quad, 
                 yname="mpg", xname="hp", data=mtcars,
                 method="BFGS")
min_loss
```

```
## $par
## [1] 30.09886054 -0.06822828
## 
## $value
## [1] 447.6743
## 
## $counts
## function gradient 
##       31        5 
## 
## $convergence
## [1] 0
## 
## $message
## NULL
```

```r
# Comparando com estimativas via lm()
reg$coef
```

```
## (Intercept)          hp 
## 30.09886054 -0.06822828
```


</br>

### (b) M√©todo dos Momentos
- [Computing Generalized Method of Moments and Generalized Empirical Likelihood with R (Pierre Chauss√©)](https://cran.r-project.org/web/packages/gmm/vignettes/gmm_with_R.pdf)
- [Generalized Method of Moments (GMM) in R - Part 1 (Alfred F. SAM)](https://medium.com/codex/generalized-method-of-moments-gmm-in-r-part-1-of-3-c65f41b6199)

- Para estimar via GMM precisamos construir vetores relacionados aos seguintes momentos:

{{<math>}}$$ E(\boldsymbol{\varepsilon}) = 0 \qquad \text{ e } \qquad E(\boldsymbol{\varepsilon x}) = \boldsymbol{0} $${{</math>}}

Note que estes s√£o os momentos relacionados ao MQO, dado que este √© um caso particular do GMM. Os an√°logos amostrais s√£o:

{{<math>}}$$ \frac{1}{N} \sum^N_{i=1}{\hat{\varepsilon}_i} = 0 \qquad \text{ e } \qquad \frac{1}{N} \sum^N_{i=1}{\hat{\varepsilon}_i.x_i} = 0 $${{</math>}}

Podemos calcular os dois momentos amostrais em uma √∫nica multiplica√ß√£o matricial. Considere:

{{<math>}}$$ \hat{\boldsymbol{\varepsilon}} = \begin{bmatrix} \varepsilon_1 \\ \varepsilon_2 \\ \vdots \\ \varepsilon_N \end{bmatrix} \qquad \text{e} \qquad \boldsymbol{x} = \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_N \end{bmatrix} $${{</math>}}

Vamos juntar uma coluna de 1's com {{<math>}}$\boldsymbol{x}${{</math>}} e definir a matriz:
{{<math>}}$$ \boldsymbol{X} = \begin{bmatrix} 1 & x_1 \\ 1 & x_2 \\ \vdots & \vdots \\ 1 & x_N \end{bmatrix} $${{</math>}}

Fazendo a multiplica√ß√£o matricia entre {{<math>}}$\hat{\boldsymbol{\varepsilon}}${{</math>}} e {{<math>}}$\boldsymbol{X}${{</math>}}, temos:

{{<math>}}$$ \hat{\boldsymbol{\varepsilon}}' \boldsymbol{X}\ =\ \begin{bmatrix} \varepsilon_1 & \varepsilon_2 & \cdots & \varepsilon_N \end{bmatrix} \begin{bmatrix} 1 & x_1 \\ 1 & x_2 \\ \vdots & \vdots \\ 1 & x_N \end{bmatrix}\ =\ \begin{bmatrix}  \sum^N_{i=1}{\hat{\varepsilon}_i} &  \sum^N_{i=1}{\hat{\varepsilon}_i .x_i} \end{bmatrix} $${{</math>}}

Note que o vetor resultante s√£o exatamente os momentos amostrais, os quais queremos aproxim√°-los ao m√°ximo de zero (0).



#### Otimiza√ß√£o Num√©rica para GMM

##### 1. Chute de valores iniciais para {{<math>}}$\beta_0${{</math>}} e {{<math>}}$\beta_1${{</math>}}
- Vamos criar um vetor com poss√≠veis valores de {{<math>}}$\beta_0, \beta_1${{</math>}}:

```r
params = c(30, -0.05)
yname = "mpg"
xname = "hp"
data = mtcars
```

##### 2. Sele√ß√£o da base de dados e vari√°veis

```r
# Extraindo as vari√°veis da base em vetores
y = data[,yname]
x = data[,xname]

# Extraindo os par√¢metros de params
b0 = params[1]
b1 = params[2]
```

##### 3. C√°lculo dos valores ajustados e dos res√≠duos

```r
## Valores ajustados de y
yhat = b0 + b1 * x

## Res√≠duos
e_hat = y - yhat
```


##### 4. Cria√ß√£o da matriz de momentos
- Note que {{<math>}}$\hat{\boldsymbol{\varepsilon}}' X${{</math>}} um vetor dos momentos amostrais, mas a fun√ß√£o `gmm()` exige uma matriz com **multiplica√ß√£o elemento a elemento** do res√≠duo {{<math>}}$\hat{\boldsymbol{\varepsilon}}${{</math>}} com as covariadas {{<math>}}$\boldsymbol{X}${{</math>}} (neste caso: constante e hp), na forma:

{{<math>}}$$ \hat{\boldsymbol{\varepsilon}} \times \boldsymbol{X}\ =\ \begin{bmatrix} \varepsilon_1 \\ \varepsilon_2 \\ \vdots \\ \varepsilon_N \end{bmatrix} \times \begin{bmatrix} 1 & x_1 \\ 1 & x_2 \\ \vdots & \vdots \\ 1 & x_N \end{bmatrix}\ =\ \begin{bmatrix} \varepsilon_1 & \varepsilon_1.x_1  \\ \varepsilon_2 & \varepsilon_2.x_2 \\ \vdots & \vdots \\ \varepsilon_N & \varepsilon_N.x_N \end{bmatrix}, $${{</math>}}
em que {{<math>}}$\times${{</math>}} denota a multiplica√ß√£o padr√£o, elemento a elemento por linha. Note que se fizermos as somas de cada coluna, obtemos os dois momentos amostrais.

Note que, para fazer o GMM no R, n√£o devemos tirar a m√©dia de cada coluna (a pr√≥pria fun√ß√£o `gmm()` far√° isso).



```r
# Matriz de momentos
m = as.numeric(e_hat) * as.numeric(e_hat) * cbind(1,x) # multiplica√ß√£o por elemento
apply(m, 2, sum) # soma de cada coluna
```

```
##                     x 
##    708.275 106721.440
```
- Note que, como multiplicamos a constante igual a 1 com os res√≠duos {{<math>}}$\hat{\varepsilon}${{</math>}}, a 1¬™ coluna corresponde ao momento amostral {{<math>}}$\sum^N_{i=1}{\hat{\varepsilon}_i}${{</math>}} (mas sem dividir por _N_).
- J√° a coluna 2 correspode ao momento amostral {{<math>}}$\sum^N_{i=1}{\hat{\varepsilon}_i .x_i}=0${{</math>}} para a vari√°vel _hp_ (mas sem dividir por _N_).
- Logicamente, para estimar por GMM, precisamos escolher os par√¢metros {{<math>}}$\hat{\boldsymbol{\beta}} = \{ \hat{\beta}_0, \hat{\beta}_1 \}${{</math>}} que, ao tomar a esperan√ßa em cada um destas colunas, se aproximem ao m√°ximo de zero. Isso ser√° feito via fun√ß√£o `gmm()` (semelhante √† fun√ß√£o `optim()`)


##### 5. Cria√ß√£o de fun√ß√£o com os momentos
- Vamos criar uma fun√ß√£o que tem como input um vetor de par√¢metros (`params`) e uma base de dados (`data`), e que retorna uma matriz em que cada coluna representa um momento.
- Essa fun√ß√£o incluir√° todos os comandos descritos nos itens 1 a 4 (que, na verdade, apenas foram feitos por did√°tica).

```r
mom_ols = function(params, args) {
  # No gmm(), s√≥ pode ter 1 input dos argumentos dessa fun√ß√£o
  # Por isso, foi inclu√≠do uma lista com 3 argumentos
  yname = args[[1]]
  xname = args[[2]]
  data = args[[3]]
  
  # Extraindo as vari√°veis da base em vetores
  y = data[,yname]
  x = data[,xname]
  
  # Extraindo os par√¢metros de params
  b0 = params[1]
  b1 = params[2]
  
  ## Valores ajustados de y
  yhat = b0 + b1 * x
  
  ## Res√≠duos
  e_hat = y - yhat
  
  ## Matriz de momentos
  m = as.numeric(e_hat) * cbind(1,x)
  m # output da fun√ß√£o
}
```


##### 6. Otimiza√ß√£o via fun√ß√£o `gmm()`
- A fun√ß√£o `gmm()`, assim como a `optim()`, recebe uma fun√ß√£o como argumento.
- No entanto, ao inv√©s de retornar um valor, a fun√ß√£o que entra no `gmm()` retorna uma matriz, cujas m√©dias das colunas queremos aproximar de zero. 

```r
gmm_lm = gmm::gmm(
  g=mom_ols, 
  x=list(yname="mpg", xname="hp", data=mtcars), # argumentos fun√ß√£o
  t0=c(0,0), # chute inicial de params
  # wmatrix = "optimal", # matriz de pondera√ß√£o
  optfct = "nlminb" # fun√ß√£o de otimiza√ß√£o
  )
gmm_lm$coef
```

```
##    Theta[1]    Theta[2] 
## 30.09886038 -0.06822828
```

```r
# Comparando com estimativas via lm()
reg$coef
```

```
## (Intercept)          hp 
## 30.09886054 -0.06822828
```


</br>

### (c) M√°xima Verossimilhan√ßa
- [ResEcon 703](https://github.com/woerman/ResEcon703) - Week 6 (University of Massachusetts Amherst)
- A fun√ß√£o objetivo √© a fun√ß√£o de verossimilhan√ßa que, ao contr√°rio da fun√ß√£o de soma de quadrado dos res√≠duos, queremos maximiz√°-la


#### Intui√ß√£o do c√°lculo da fun√ß√£o de verossimilhan√ßa
- Apenas para ilustrar a constru√ß√£o da fun√ß√£o de verossimilhan√ßa, {{<math>}}$\mathcal{L}${{</math>}}, considere um modelo de probabilidade linear:
{{<math>}}$$ \text{am} = \beta_0 + \beta_1 \text{cyl} + \varepsilon, $${{</math>}}
em que _cyl_ √© a quantidade de cilindros do carro, e _am_ √© uma vari√°vel _dummy_ que √© igual a 1 se o carro for autom√°tico e 0 caso contr√°rio.

- Queremos encontrar {{<math>}}$\hat{\boldsymbol{\beta}} = \left\{ \hat{\beta}_0, \hat{\beta}_1 \right\}${{</math>}} que maximizam a fun√ß√£o de verossimilhan√ßa.
- Considere um chute de par√¢metros {{<math>}}$\hat{\boldsymbol{\beta}}_A = \left\{ \hat{\beta}^A_0 = 1.3, \hat{\beta}^A_1 = -0.14 \right\}${{</math>}} que gerem os seguintes valores preditos/ajustados (probabilidades):

<center><img src="../likelihood_A.png" width=80%></center>


- Logo, a verossimilhan√ßa, dado os par√¢metros {{<math>}}$\hat{\boldsymbol{\beta}}_A${{</math>}} √©
{{<math>}}$$ \mathcal{L}(\boldsymbol{\beta}^A) = 46\% \times 46\% \times 74\% \times 54\% \times 82\% = 6,9\% $${{</math>}}

- Agora, considere um segundo chute de par√¢metros {{<math>}}$\hat{\boldsymbol{\beta}}_B = \left\{ \beta^B_0=1.0, \beta^B_1=-0.10 \right\}${{</math>}} que gerem as seguintes probabilidades:

<center><img src="../likelihood_B.png" width=80%></center>

- Ent√£o, a verossimilhan√ßa, dado {{<math>}}$\hat{\boldsymbol{\beta}}_B${{</math>}}, √©
{{<math>}}$$ \mathcal{L}(\hat{\boldsymbol{\beta}}_B) = 40\% \times 40\% \times 60\% \times 60\% \times 80\% = 4,6\% $${{</math>}}
- Como {{<math>}}$\mathcal{L}\left(\hat{\boldsymbol{\beta}}_A\right) = 6,9\% > 4,6\% = \mathcal{L}\left(\hat{\boldsymbol{\beta}}_B\right)${{</math>}}, ent√£o os par√¢metros {{<math>}}$\hat{\boldsymbol{\beta}}_A${{</math>}} se mostram mais adequados em rela√ß√£o a {{<math>}}$\hat{\boldsymbol{\beta}}_B${{</math>}}
- Na estrat√©gia de m√°xima verossimilhan√ßa (ML), escolhe-se o conjunto de par√¢metros {{<math>}}$\hat{\boldsymbol{\beta}}^*${{</math>}} que maximiza a fun√ß√£o de verossimilhan√ßa (ou log-verossimilhan√ßa).






#### Otimiza√ß√£o Num√©rica para M√°xima Verossimilhan√ßa

- No modelo de probabilidade linear, as probabilidades usadas para calcular a verossimilhan√ßa s√£o as pr√≥prias probabilidades de ser autom√°tico (se for carro autom√°tico) ou de ser manual (se for carro manual), dado um conjunto de par√¢metros.
- J√° no modelo linear, usamos a fun√ß√£o de densidade de probabilidade para avaliar a "probabilidade" de cada observa√ß√£o, {{<math>}}$y_i${{</math>}}, ser o ser valor ajustado {{<math>}}$\hat y_i${{</math>}}, dado um conjunto de par√¢metros {{<math>}}$\hat{\boldsymbol{\beta}}${{</math>}} e {{<math>}}$\hat{\sigma}${{</math>}}.

<center><img src="../mle.jpg"></center>

- Como demonstra a figura acima, assumimos que o erro {{<math>}}$\varepsilon${{</math>}} √© normalmente distribu√≠do para todo `\(x\)`, com a mesma vari√¢ncia {{<math>}}$\sigma^2${{</math>}} (homocedasticidade)

- Em nosso modelo
{{<math>}} $$ \text{mpg} = \beta_0 + \beta_1 \text{hp} + \varepsilon, $$ {{</math>}}
queremos estimar 3 par√¢metros
{{<math>}}$$ \hat{\boldsymbol{\beta}} = \left\{ \hat{\beta}_0, \hat{\beta}_1, \hat{\sigma} \right\}, $${{</math>}}
em que {{<math>}}$\hat{\sigma}${{</math>}} √© desvio padr√£o do res√≠duo.

- A fun√ß√£o `ml2()` do pacote `bbmle`, que ser√° usada para desempenhar a otimiza√ß√£o num√©rica, assim como `optim()`. Precisamos usar como input:
  - Alguns valores inicias dos par√¢metros, {{<math>}}$\hat{\boldsymbol{\beta}}^0 = \left\{ \hat{\beta}^0_0, \hat{\beta}^0_1, \hat{\sigma}^0 \right\}${{</math>}}
  - Uma fun√ß√£o que tome esses par√¢metros como argumento e calcule a 
log-verossimilhan√ßa, {{<math>}}$\ln{L(\boldsymbol{\hat{\boldsymbol{\beta}}})}${{</math>}}.

> Como as fun√ß√µes de otimiza√ß√£o costumam encontrar o m√≠nimo de uma fun√ß√£o objetivo, precisamos adaptar o output para o negativo fun√ß√£o de log-verossimilhan√ßa. Ao minimizar o negativo de log-lik, estamos maximizando log-lik.

A fun√ß√£o log-verossimilhan√ßa √© dada por
{{<math>}}$$ \ln{L(\hat{\beta}_0, \hat{\beta}_1, \hat{\sigma} | y, x)} = \sum^n_{i=1}{\ln{f(y_i | x_i, \hat{\beta}_0, \hat{\beta}_1, \hat{\sigma})}}, $${{</math>}}
em que a distribui√ß√£o condicional de cada {{<math>}}$y_i${{</math>}} √©

{{<math>}}$$ y_i | x_i \sim \mathcal{N}(\hat{\beta}_0 + \hat{\beta}_1 x_i, \hat{\sigma}) $${{</math>}}
o que implica que 

{{<math>}}$$\varepsilon_i | x_i \sim N(0, \sigma^2)$${{</math>}}

<!-- <center><img src="../mle.jpg"></center> -->

Passos para estimar uma regress√£o por m√°xima verossimilhan√ßa:

1. Chutar valores iniciais de 
2. Calcular os valores ajustados, {{<math>}}$\hat{y}${{</math>}}
3. Calcular a densidade para cada {{<math>}}$y_i${{</math>}}, usando {{<math>}}$f(y_i | x_i, \hat{\beta}_0, \hat{\beta}_1, \hat{\sigma})${{</math>}}
4. Calcular a log-verossimilhan√ßa, {{<math>}}$\sum^n_{i=1}{\ln{f(y_i | x_i, \hat{\beta}_0, \hat{\beta}_1, \hat{\sigma})}}${{</math>}}


##### 1. Chute de valores iniciais para {{<math>}}$\beta_0, \beta_1${{</math>}} e {{<math>}}$\sigma^2${{</math>}}
- Note que, diferente da estima√ß√£o por MQO, um dos par√¢metros a ser estimado via MLE √© a vari√¢ncia ({{<math>}}$\sigma^2${{</math>}}).

```r
params = c(30, -0.06, 1)
# (b0, b1 , sig2)
```

##### 2. Sele√ß√£o da base de dados e vari√°veis

```r
## Inicializando
yname = "mpg"
xname = "hp"
data = mtcars

# Extraindo as vari√°veis da base em vetores
y = data[,yname]
x = data[,xname]

# Extraindo os par√¢metros de params
b0hat = params[1]
b1hat = params[2]
sighat = params[3]
```

##### 3. C√°lculo dos valores ajustados e das densidades

```r
## Calculando valores ajustados de y
yhat = b0hat + b1hat * x
head(yhat)
```

```
## [1] 23.40 23.40 24.42 23.40 19.50 23.70
```

##### 4. C√°lculo das densidades
{{<math>}}$$ f(y_i | x_i, \hat{\beta}_0, \hat{\beta}_1, \hat{\sigma}) $${{</math>}}

```r
## Calculando os pdf's de cada linha
ypdf = dnorm(y, mean = yhat, sd = sighat)

head(round(ypdf, 4)) # Primeiros valores da densidade
```

```
## [1] 0.0224 0.0224 0.1074 0.0540 0.2897 0.0000
```

```r
sum(ypdf) # Verossimilhan√ßa
```

```
## [1] 2.447628
```

```r
prod(ypdf) # Log-Verossimilhan√ßa
```

```
## [1] 2.201994e-121
```
- Agora, vamos juntar visualizar os 6 primeiros elementos dos objetos trabalhados:

```r
# Juntando as bases e visualizando os primeiros valores
tab = cbind(y, x, yhat, round(ypdf, 4)) # arredondando ypdf (4 d√≠gitos)
colnames(tab) = c("y", "x", "yhat", "ypdf") # renomeando colunas
head(tab)
```

```
##         y   x  yhat   ypdf
## [1,] 21.0 110 23.40 0.0224
## [2,] 21.0 110 23.40 0.0224
## [3,] 22.8  93 24.42 0.1074
## [4,] 21.4 110 23.40 0.0540
## [5,] 18.7 175 19.50 0.2897
## [6,] 18.1 105 23.70 0.0000
```
- Como pode ser visto na base de dados juntada e nos gr√°ficos abaixo, quanto mais pr√≥ximo o valor ajustado for do valor observado de cada observa√ß√£o, maior ser√° a densidade/probabilidade.
<img src="/project/rec5004/sec7/_index_files/figure-html/unnamed-chunk-16-1.png" width="672" /><img src="/project/rec5004/sec7/_index_files/figure-html/unnamed-chunk-16-2.png" width="672" /><img src="/project/rec5004/sec7/_index_files/figure-html/unnamed-chunk-16-3.png" width="672" />
- Logo, a verossimilhan√ßa (produto de todas densidades de probabilidade) ser√° maior quanto mais pr√≥ximos forem os valores ajustados dos seus respectivos valores observados.


##### 5. Calculando a Log-Verossimilhan√ßa

A log-verossimilhan√ßa √© a soma do log de todas probabilidades:

{{<math>}}$$ \mathcal{l}(\hat{\beta}_0, \hat{\beta}_1, \hat{\sigma}) = \sum^{N}_{i=1}{\ln\left[ f(y_i | x_i, \hat{\beta}_0, \hat{\beta}_1, \hat{\sigma}) \right]} $${{</math>}}

```r
## Calculando a log-verossimilhanca
loglik = sum(log(ypdf))
loglik
```

```
## [1] -277.8234
```


##### 6. Criando a Fun√ß√£o de Log-Verossimilhan√ßa

Juntando tudo que fizemos anteriormente, podemos criar uma fun√ß√£o no R que calcular a fun√ß√£o de log-verossimilhan√ßa.


```r
## Criando fun√ß√£o para calcular log-verossimilhanca de OLS
loglik_lm = function(b0hat, b1hat, sighat) {
  # Extraindo as vari√°veis da base em vetores
  y = as.matrix(data[,yname])
  x = as.matrix(data[,xname])

  ## Calculando valores ajustados de y
  yhat = b0hat + b1hat * x
  
  ## Calculando os pdf's de cada linha
  ypdf = dnorm(y, mean = yhat, sd = sighat)
  
  ## Calculando a log-verossimilhanca
  loglik = sum(log(ypdf))
  
  ## Retornando o negativo da log-verossimilanca
  -loglik # Negativo, pois mle2() minimiza e queremos maximizar
}
```


##### 7. Otimiza√ß√£o

Tendo a fun√ß√£o objetivo, usaremos `mle2()` para *minimizar o negativo* da fun√ß√£o de log-verossimilhan√ßa

{{<math>}}$$ \min_{(\hat{\beta}_0, \hat{\beta}_1, \hat{\sigma})} -\sum^n_{i=1}{\ln{f(y_i | x_i, \hat{\beta}_0, \hat{\beta}_1, \hat{\sigma})}} = \max_{(\hat{\beta}_0, \hat{\beta}_1, \hat{\sigma})} \sum^n_{i=1}{\ln{f(y_i | x_i, \hat{\beta}_0, \hat{\beta}_1, \hat{\sigma})}} $${{</math>}}



```r
## Maximizando a fun√ß√£o log-verossimilhan√ßa de OLS
mle_ols = bbmle::mle2(
  minuslogl=loglik_lm,
  start=list(b0hat=0, b1hat=0, sighat=1),
  data=list(yname = "mpg", xname = "hp", data = mtcars),
  hessian=T
  )
mle_ols
```

```
## 
## Call:
## bbmle::mle2(minuslogl = loglik_lm, start = list(b0hat = 0, b1hat = 0, 
##     sighat = 1), data = list(yname = "mpg", xname = "hp", data = mtcars), 
##     hessian.opts = T)
## 
## Coefficients:
##       b0hat       b1hat      sighat 
## 30.09886884 -0.06822833  3.74029716 
## 
## Log-likelihood: -87.62
```

```r
# Comparando com estimativas via lm()
reg$coef
```

```
## (Intercept)          hp 
## 30.09886054 -0.06822828
```



</br>

{{< cta cta_text="üëâ Proceed to Multiple Regression" cta_link="../sec8" >}}
