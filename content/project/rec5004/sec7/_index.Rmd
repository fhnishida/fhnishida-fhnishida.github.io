---
date: "2018-09-09T00:00:00Z"
# icon: book
# icon_pack: fas
linktitle: Optimization
summary: The author covers topics such as grid search and steepest ascent methods for optimization to show three approaches to reach the OLS estimation formula. The page also includes examples and code snippets to illustrate the concepts discussed.
title: Optimization
weight: 7
output: md_document
type: book
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Otimiza√ß√£o num√©rica
- Essa se√ß√£o tem o objetivo para dar uma intui√ß√£o sobre m√©todos de otimiza√ß√£o.
- Veremos os m√©todos de _grid search_ e _gradient ascent_ (_descent_) que representam fam√≠lias de m√©todos de otimiza√ß√£o.


### _Grid Search_

- O m√©todo mais simples de otimiza√ß√£o num√©rica √© o _grid search_ (discretiza√ß√£o).
- Como o R n√£o lida com problemas com infinitos valores, uma forma lidar com isso √© discretizando diversos poss√≠veis valores dos par√¢metros de escolha dentro de intervalos.
- Para cada poss√≠vel combina√ß√£o de par√¢metros, calculam-se diversos valores a partir da fun√ß√£o objetivo. De todos os valores calculados, escolhe-se a combina√ß√£o de par√¢metros que maximizam (ou minimizam) a fun√ß√£o objetivo.
- O exemplo abaixo considera apenas um par√¢metro de escolha {{<math>}}$\theta${{</math>}} e, para cada ponto escolhido dentro do intervalo {{<math>}}$(-1, 1)${{</math>}}, calcula-se a fun√ß√£o objetivo:

<center><img src="../grid_search.png"></center>


- Este √© um m√©todo robusto a fun√ß√µes com descontinuidades e quinas (n√£o diferenci√°veis), e menos sens√≠vel a chutes de valores iniciais. (ver m√©todo abaixo)
- Por√©m, este m√©todo fica preciso apenas com maiores quantidades de pontos e, como √© necess√°rio fazer o c√°lculo da fun√ß√£o objetivo para cada ponto, o _grid search_ tende a ser menos eficiente computacionalmente (demora mais tempo para calcular).


### _Gradient Ascent (Descent)_

- Conforme o n√∫mero de par√¢metros do modelo cresce, aumenta o n√∫mero de poss√≠veis combina√ß√µes entre par√¢metros e torna o processo computacional cada vez mais lento.
- Uma forma mais eficiente de encontrar o conjunto de par√¢metros que otimizam a fun√ß√£o objetivo √© por meio do m√©todo _gradient ascent_ (_descent_).
- Queremos encontrar o {{<math>}}${\theta}^{**}${{</math>}} que √© o par√¢metro que maximiza globalmente a fun√ß√£o objetivo
- Passos para encontrar um m√°ximo:
  1. Comece com algum valor inicial de par√¢metro, {{<math>}}${\theta}^0${{</math>}}
  2. Calcula-se a derivada e avalia-se a possibilidade de "andar para cima" a um valor mais alto
  3. Caso possa, ande na dire√ß√£o correta a {{<math>}}${\theta}^1${{</math>}}
  4. Repita os passos (2) e (3), andando para um novo {{<math>}}${\theta}^2, {\theta}^3, ...${{</math>}} at√© atingir um ponto m√°ximo cuja derivada √© igual a zero.

<center><img src="../steepest_ascent.png"></center>


- Note que esse m√©todo de otimiza√ß√£o √© sens√≠vel ao par√¢metro inicial e √†s descontinuidades da fun√ß√£o objetivo.
    - No exemplo, se os chutes iniciais forem {{<math>}}${\theta}^0_A${{</math>}} ou {{<math>}}${\theta}^0_B${{</math>}}, ent√£o consegue atingir o m√°ximo global.
    - J√° se o chute inicial for {{<math>}}${\theta}^0_C${{</math>}}, ent√£o ele acaba atingindo um m√°ximo local com {{<math>}}${\theta}^*${{</math>}} (menor do que o m√°ximo global em {{<math>}}${\theta}^{**}${{</math>}}).


<video width="500px" height="500px" controls="controls"/>
    <source src="../local-maxima.mp4" type="video/mp4">
</video>

- Por outro lado, √© um m√©todo mais eficiente, pois calcula-se a fun√ß√£o objetivo uma vez a cada passo, al√©m de ser mais preciso nas estima√ß√µes.



</br>

## Encontrando MQO por diferentes estrat√©gias
- Nesta se√ß√£o, encontraremos as estimativas de MQO usando as estrat√©gias da (a) minimiza√ß√£o da fun√ß√£o perda, de (b) m√°xima verossimilhan√ßa e de (c) m√©todo dos momentos.
- Em cada uma delas, usaremos uma fun√ß√£o objetivo distinta para encontrar o vetor com dois par√¢metros, {{<math>}}$ \boldsymbol{\theta} = \{ \beta_0, \beta_1 \} ${{</math>}}, que a otimiza. No R, vamos chamar esse vetor de `params`.


### Base `mtcars`
√â necess√°rio carregar o pacote `dplyr` para manipula√ß√£o da base de dados abaixo.
```{r message=FALSE, warning=FALSE}
library(dplyr)
```

Usaremos dados extra√≠dos da _Motor Trend_ US magazine de 1974, que analisa o
consumo de combust√≠vel e 10 aspectos t√©cnicos de 32 autom√≥veis.

No _R_, a base de dados j√° est√° incorporada ao programa e pode ser acessada pelo c√≥digo `mtcars`, contendo a seguinte estrutura:

> - _mpg_: milhas por gal√£o
> - _hp_: cavalos-vapor bruto

Queremos estimar o seguinte modelo:
{{<math>}} $$ \text{mpg} = \beta_0 + \beta_1 \text{hp} + \varepsilon $$ {{</math>}}

```{r}
## Regressao MQO
reg = lm(formula = mpg ~ hp, data = mtcars)
summary(reg)$coef
```



### (a) Minimiza√ß√£o da fun√ß√£o perda
- A fun√ß√£o perda adotada pela Teoria da Decis√£o √© a **fun√ß√£o de soma dos quadrados dos res√≠duos**
- Por essa estrat√©gia, queremos encontrar as estimativas, {{<math>}}$\boldsymbol{\theta} = \{ \hat{\beta}_0,\ \hat{\beta}_1 \}${{</math>}}, que **minimizam** essa fun√ß√£o.


#### 1. Criar fun√ß√£o perda que calcula a soma dos res√≠duos quadr√°ticos
- A fun√ß√£o para calcular a soma dos res√≠duos quadr√°ticos recebe como inputs:
  - um **vetor** de poss√≠veis valores {{<math>}}$\boldsymbol{\theta} = \{ \hat{\beta}_0,\ \hat{\beta}_1 \}${{</math>}}
  - um **texto** com o nome da vari√°vel dependente
  - um **vetor de texto** com os nomes dos regressores
  - uma base de dados
```{r}
resid_quad = function(params, yname, xname, data) {
  # Extraindo as vari√°veis da base em vetores
  y = as.matrix(data[yname])
  x = as.matrix(data[xname])
  
  # Extraindo os par√¢metros de params
  b0 = params[1]
  b1 = params[2]
  sig2 = params[3]
  
  yhat = b0 + b1 * x # valores ajustados
  e_hat = y - yhat # desvios = observados - ajustados
  sum(e_hat^2)
}
```


#### 2. Otimiza√ß√£o
- Agora encontraremos os par√¢metros que minimizam a fun√ß√£o perda

{{<math>}}$$ \underset{\hat{\beta}_0, \hat{\beta}_1}{\text{argmin}} \sum_{i=1}^{N}\hat{u}^2 \quad = \quad \underset{\hat{\beta}_0, \hat{\beta}_1}{\text{argmin}} \sum_{i=1}^{N}\left( \text{mpg}_i - \widehat{\text{mpg}}_i \right)^2 $${{</math>}}

- Para isto usaremos a fun√ß√£o `optim()` que retorna os par√¢metros que minimizam uma fun√ß√£o (equivalente ao _argmin_):
```yaml
optim(par, fn, gr = NULL, ...,
      method = c("Nelder-Mead", "BFGS", "CG", "L-BFGS-B", "SANN", "Brent"),
      lower = -Inf, upper = Inf,
      control = list(), hessian = FALSE)

par: Initial values for the parameters to be optimized over.
fn: A function to be minimized (or maximized), with first argument the vector of parameters over which minimization is to take place. It should return a scalar result.
method: The method to be used. See ‚ÄòDetails‚Äô. Can be abbreviated.
hessian: Logical. Should a numerically differentiated Hessian matrix be returned?
```
- Colocaremos como input:
  - a fun√ß√£o perda criada `resid_quad()`
  - um chute inicial dos par√¢metros
    - Note que a estima√ß√£o pode ser mais ou menos sens√≠vel ao valores iniciais, dependendo do m√©todo de otimiza√ß√£o utilizado
    - O mais comum √© encontrar como chute inicial um vetor de zeros `c(0, 0, 0)`, por ser mais neutro em rela√ß√£o ao sinal das estimativas
    - Em Econometria III, prof. Laurini recomendou usar m√©todo "Nelder-Mead" (padr√£o) com um chute inicial de zeros e, depois, usar suas estimativas como chute inicial para o m√©todo "BFGS".
  - Por padr√£o, temos o argumento `hessian = FALSE`, coloque `TRUE` para calcularmos o erro padr√£o, estat√≠stica t e p-valor das estimativas

```{r}
# Estima√ß√£o por BFGS
theta_ini = c(0, 0) # Chute inicial de b0, b1

fit_ols2 = optim(par=theta_ini, fn=resid_quad, 
                 yname="mpg", xname="hp", data=mtcars,
                 method="BFGS", hessian=TRUE)
fit_ols2
```



### (b) M√°xima Verossimilhan√ßa
- [ResEcon 703](https://github.com/woerman/ResEcon703) - Week 6 (University of Massachusetts Amherst)
- A fun√ß√£o objetivo √© a fun√ß√£o de verossimilhan√ßa e, ao contr√°rio da fun√ß√£o de soma de quadrado dos res√≠duos, queremos maximiz√°-la
- Em nosso exemplo, temos que estimar 3 par√¢metros

{{<math>}}$$ \boldsymbol{\theta} = \left\{ \beta_0, \beta_1, \sigma^2 \right\}. $${{</math>}}



#### Otimiza√ß√£o Num√©rica para M√°xima Verossimilhan√ßa
A fun√ß√£o `optim()` do R ser√° usada novamente para desempenhar a otimiza√ß√£o num√©rica. Precisamos usar como input:

- Alguns valores inicias dos par√¢metros, {{<math>}}$\boldsymbol{\theta}^0 = \{ \beta_0, \beta_1, \sigma^2 \}${{</math>}}
- Uma fun√ß√£o que tome esses par√¢metros como um argumento e calcule a 
log-verossimilhan√ßa, {{<math>}}$\ln{L(\boldsymbol{\theta})}${{</math>}}.

> Como `optim()` ir√° encontrar os par√¢metros que minimizem a fun√ß√£o objetivo, precisamos adaptar o output da fun√ß√£o de log-verossimilhan√ßa (minimizaremos o negativo da log-lik).

A fun√ß√£o log-verossimilhan√ßa √© dada por
{{<math>}}$$ \ln{L(\beta_0, \beta_1, \sigma^2 | y, x)} = \sum^n_{i=1}{\ln{f(y_i | x_i, \beta_0, \beta_1, \sigma^2)}}, $${{</math>}}

em que a distribui√ß√£o condicional de cada {{<math>}}$y_i${{</math>}} √©

{{<math>}}$$ y_i | x_i \sim \mathcal{N}(\beta_0 + \beta_1 x_i, \sigma^2) $${{</math>}}

o que implica que 

{{<math>}}$$\varepsilon_i | x_i \sim N(0, \sigma^2)$${{</math>}}

<center><img src="../mle.jpg"></center>

- Acima, podemos ver que, para cada {{<math>}}$x${{</math>}}, temos um valor ajustado {{<math>}}$\hat{y} = \beta_0 + \beta_1 x${{</math>}} e seus desvios {{<math>}}$\varepsilon${{</math>}} s√£o normalmente distribu√≠dos com a mesma vari√¢ncia {{<math>}}$\sigma^2${{</math>}}


Passos para estimar uma regress√£o por m√°xima verossimilhan√ßa:

1. Chutar valores iniciais de 
2. Calcular os valores ajustados, {{<math>}}$\hat{y}${{</math>}}
3. Calcular a densidade para cada {{<math>}}$y_i${{</math>}}, {{<math>}}$f(y_i | x_i, \beta_0, \beta_1, \sigma^2)${{</math>}}
4. Calcular a log-verossimilhan√ßa, {{<math>}}$\ln{L(\beta_0, \beta_1, \sigma^2 | y, x)} = \sum^n_{i=1}{\ln{f(y_i | x_i, \beta_0, \beta_1, \sigma^2)}}${{</math>}}


##### 1. Chute de valores iniciais para {{<math>}}$\beta_0, \beta_1${{</math>}} e {{<math>}}$\sigma^2${{</math>}}
- Note que, diferente da estima√ß√£o por MQO, um dos par√¢metros a ser estimado via MLE √© a vari√¢ncia ({{<math>}}$\sigma^2${{</math>}}).
```{r}
params = c(30, -0.06, 1)
# (b0, b1 , sig2)
```

##### 2. Sele√ß√£o da base de dados e vari√°veis
```{r}
## Inicializando
yname = "mpg"
xname = "hp"
data = mtcars

# Extraindo as vari√°veis da base em vetores
y = as.matrix(data[yname])
x = as.matrix(data[xname])

# Extraindo os par√¢metros de params
b0 = params[1]
b1 = params[2]
sig2 = params[3]
```

##### 3. C√°lculo dos valores ajustados e das densidades
```{r}
## Calculando valores ajustados de y
yhat = b0 + b1 * x
head(yhat)
```

##### 4. C√°lculo das densidades
{{<math>}}$$ f(y_i | x_i, \beta_0, \beta_1, \sigma^2) $${{</math>}}
```{r}
## Calculando os pdf's de cada linha
ypdf = dnorm(y, mean = yhat, sd = sqrt(sig2))

head(round(ypdf, 4)) # Primeiros valores da densidade
sum(ypdf) # Verossimilhan√ßa
prod(ypdf) # Log-Verossimilhan√ßa
```
- Agora, vamos juntar visualizar os 6 primeiros elementos dos objetos trabalhados:
```{r}
# Juntando as bases e visualizando os primeiros valores
tab = cbind(y, x, yhat, round(ypdf, 4)) # arredondando ypdf (4 d√≠gitos)
colnames(tab) = c("y", "x", "yhat", "ypdf") # renomeando colunas
head(tab)
```
- Como pode ser visto na base de dados juntada e nos gr√°ficos abaixo, quanto mais pr√≥ximo o valor ajustado for do valor observado de cada observa√ß√£o, maior ser√° a densidade/probabilidade.
```{r echo=FALSE}
# Criando gr√°fico para 3 carros (linhas 1, 4 e 5)
qt_norm = seq(16.5, 26.5, by=0.1) # valores de mpg ("escores Z")

# Mazda RX4
pdf_norm1 = dnorm(qt_norm, mean=yhat[1], sd=sqrt(sig2)) # pdf
plot(qt_norm, pdf_norm1, type="l", xlab="mpg", ylab="densidade",
     main="Mazda RX4 (linha 1)")
abline(v=c(yhat[1], y[1]), col="red")
text(c(yhat[1], y[1]), 0.2, 
     c(expression(widehat(mpg)[1]), expression(mpg[1])), 
     pos=2, srt=90, col="red")
# Hornet 4 Drive 
pdf_norm2 = dnorm(qt_norm, mean=yhat[4], sd=sqrt(sig2)) # pdf
plot(qt_norm, pdf_norm2, type="l", xlab="mpg", ylab="densidade",
     main="Hornet 4 Drive (linha 4)")
abline(v=c(yhat[4], y[4]), col="blue")
text(c(yhat[4], y[4]), 0.2, 
     c(expression(widehat(mpg)[4]), expression(mpg[4])), 
     pos=2, srt=90, col="blue")
# Hornet Sportabout 
pdf_norm3 = dnorm(qt_norm, mean=yhat[5], sd=sqrt(sig2)) # pdf
plot(qt_norm, pdf_norm3, type="l", xlab="mpg", ylab="densidade",
     main="Hornet Sportabout (linha 5)")
abline(v=c(yhat[5], y[5]), col="darkgreen")
text(c(yhat[5], y[5]), 0.2, 
     c(expression(widehat(mpg)[5]), expression(mpg[5])), 
     pos=2, srt=90, col="darkgreen")
```
- Logo, a verossimilhan√ßa (produto de todas probabilidades) ser√° maior quanto mais pr√≥ximos forem os valores ajustados dos seus respectivos valores observados.


##### 5. Calculando a Log-Verossimilhan√ßa

A log-verossimilhan√ßa √© a soma do log de todas probabilidades:

{{<math>}}$$ \mathcal{l}(\beta_0, \beta_1, \sigma^2) = \sum^{N}_{i=1}{\ln\left[ f(y_i | x_i, \beta_0, \beta_1, \sigma^2) \right]} $${{</math>}}
```{r}
## Calculando a log-verossimilhanca
loglik = sum(log(ypdf))
loglik
```


##### 6. Criando a Fun√ß√£o de Log-Verossimilhan√ßa

Juntando tudo que fizemos anteriormente, podemos criar uma fun√ß√£o no R que calcular a fun√ß√£o de log-verossimilhan√ßa.

```{r}
## Criando funcao para calcular log-verossimilhanca MQO 
loglik_lm = function(params, yname, xname, data) {
  # Extraindo as vari√°veis da base em vetores
  y = as.matrix(data[yname])
  x = as.matrix(data[xname])
  
  # Extraindo os par√¢metros de params
  b0 = params[1]
  b1 = params[2]
  sig2 = params[3]
  
  ## Calculando valores ajustados de y
  yhat = b0 + b1 * x
  
  ## Calculando os pdf's de cada linha
  ypdf = dnorm(y, mean = yhat, sd = sqrt(sig2))
  
  ## Calculando a log-verossimilhanca
  loglik = sum(log(ypdf))
  
  ## Retornando o negativo da log-verossimilanca
  -loglik # Negativo, pois optim() minimiza e queremos maximizar
}
```


##### 7. Otimiza√ß√£o

Tendo a fun√ß√£o objetivo, usaremos `optim()` para *minimizar*

{{<math>}}$$ -\ln{L(\beta_0, \beta_1, \sigma^2 | y, X)} = -\sum^n_{i=1}{\ln{f(y_i | x_i, \beta_0, \beta_1, \sigma^2)}}. $${{</math>}}

Aqui, **minimizamos o negativo** da log-Verossimilhan√ßa para **maximizarmos** (fun√ß√£o`optim()` apenas minimiza).

```{r warning=FALSE}
## Maximizando a fun√ß√£o log-verossimilhan√ßa MQO
mle = optim(par = c(0, 0, 1), fn = loglik_lm,
            yname = "mpg", xname = "hp", data = mtcars,
              method = "BFGS", hessian = TRUE)

## Mostrando os resultados da otimiza√ß√£o
mle

## Calculando os erros padr√£o
# hessiano > inversa p/ V_bhat > diagnonal > raiz quadrada
mle_se = sqrt( diag( solve(mle$hessian) ) )

# Visualizando as estimativas e os erros padr√£o
cbind(mle$par, mle_se)
```


### (c) M√©todo dos Momentos
- [Computing Generalized Method of Moments and Generalized Empirical Likelihood with R (Pierre Chauss√©)](https://cran.r-project.org/web/packages/gmm/vignettes/gmm_with_R.pdf)
- [Generalized Method of Moments (GMM) in R - Part 1 (Alfred F. SAM)](https://medium.com/codex/generalized-method-of-moments-gmm-in-r-part-1-of-3-c65f41b6199)


- Para estimar via GMM precisamos construir vetores relacionados aos seguintes momentos:

{{<math>}}$$ E(\boldsymbol{\varepsilon}) = 0 \qquad \text{ e } \qquad E(\boldsymbol{\varepsilon}' \boldsymbol{x}) = 0 $${{</math>}}

Note que estes s√£o os momentos relacionados ao MQO, dado que este √© um caso particular do GMM. Os an√°logos amostrais s√£o

{{<math>}}$$ \frac{1}{N} \sum^N_{i=1}{\hat{\varepsilon}_i} = 0 \qquad \text{ e } \qquad \frac{1}{N} \sum^N_{i=1}{\hat{\varepsilon}_i.x_i} = 0 $${{</math>}}

Podemos calcular os dois momentos amostrais em uma √∫nica multiplica√ß√£o matricial. Considere:

{{<math>}}$$ \hat{\boldsymbol{\varepsilon}} = \begin{bmatrix} \varepsilon_1 \\ \varepsilon_2 \\ \vdots \\ \varepsilon_N \end{bmatrix} \qquad \text{e} \qquad \boldsymbol{x} = \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_N \end{bmatrix} $${{</math>}}

Vamos juntar uma coluna de 1's com {{<math>}}$\boldsymbol{x}${{</math>}} e definir a matriz:
{{<math>}}$$ \boldsymbol{X} = \begin{bmatrix} 1 & \varepsilon_1 \\ 1 & \varepsilon_2 \\ \vdots & \vdots \\ 1 & \varepsilon_N \end{bmatrix} $${{</math>}}

Fazendo a multiplica√ß√£o matricia entre {{<math>}}$\hat{\boldsymbol{\varepsilon}}${{</math>}} e {{<math>}}$\boldsymbol{X}${{</math>}}, temos:

{{<math>}}$$ \hat{\boldsymbol{\varepsilon}}' \boldsymbol{X}\ =\ \begin{bmatrix} \varepsilon_1 & \varepsilon_2 & \cdots & \varepsilon_N \end{bmatrix} \begin{bmatrix} 1 & x_1 \\ 1 & x_2 \\ \vdots & \vdots \\ 1 & x_N \end{bmatrix}\ =\ \begin{bmatrix}  \frac{1}{N} \sum^N_{i=1}{\hat{\varepsilon}} & \frac{1}{N} \sum^N_{i=1}{\hat{\varepsilon}.x_i} \end{bmatrix} $${{</math>}}

Note que o vetor resultante s√£o exatamente os momentos amostrais.



#### Otimiza√ß√£o Num√©rica para GMM

##### 1. Chute de valores iniciais para {{<math>}}$\beta_0${{</math>}} e {{<math>}}$\beta_1${{</math>}}
- Vamos criar um vetor com poss√≠veis valores de {{<math>}}$\beta_0, \beta_1${{</math>}}:
```{r}
params = c(30, -0.06)
yname = "mpg"
xname = "hp"
data = mtcars
```

##### 2. Sele√ß√£o da base de dados e vari√°veis
```{r}
# Extraindo as vari√°veis da base em vetores
y = as.matrix(data[yname])
x = as.matrix(data[xname])
X = cbind(1, x)

# Extraindo os par√¢metros de params
b0 = params[1]
b1 = params[2]
sig2 = params[3]
```

##### 3. C√°lculo dos valores ajustados e dos res√≠duos
```{r}
## Valores ajustados de y
yhat = b0 + b1 * x

## Res√≠duos
e_hat = y - yhat
```


##### 4. Cria√ß√£o da matriz de momentos
- Note que {{<math>}}$\hat{\boldsymbol{\varepsilon}}' X${{</math>}} um vetor dos momentos amostrais, mas a fun√ß√£o `gmm()` exige uma matriz com **multiplica√ß√£o elemento a elemento** do res√≠duo {{<math>}}$\hat{\boldsymbol{\varepsilon}}${{</math>}} com as covariadas {{<math>}}$\boldsymbol{X}${{</math>}} (neste caso: constante e hp), na forma:

{{<math>}}$$ \hat{\boldsymbol{\varepsilon}} \times \boldsymbol{X}\ =\ \begin{bmatrix} \varepsilon_1 \\ \varepsilon_2 \\ \vdots \\ \varepsilon_N \end{bmatrix} \times \begin{bmatrix} 1 & x_1 \\ 1 & x_2 \\ \vdots & \vdots \\ 1 & x_N \end{bmatrix}\ =\ \begin{bmatrix} \varepsilon_1 & \varepsilon_1.x_1  \\ \varepsilon_2 & \varepsilon_2.x_2 \\ \vdots & \vdots \\ \varepsilon_N & \varepsilon_N.x_N \end{bmatrix} $${{</math>}}
Note que, para fazer o GMM no R, n√£o devemos tirar a m√©dia de cada coluna (a pr√≥pria fun√ß√£o `gmm()` far√° isso).


```{r}
# Matriz de momentos
m = as.numeric(e_hat) * X 
head(m) # 6 primeiras linhas
apply(m, 2, sum) # m√©dia de cada coluna
```
- Note que, como multiplicamos a constante igual a 1 com os desvios {{<math>}}$\varepsilon${{</math>}}, a 1¬™ coluna corresponde ao momento {{<math>}}$E(\varepsilon)=0${{</math>}} (mas sem tomar a esperan√ßa).
- J√° as colunas 2 e 3 correspodem ao momento {{<math>}}$E(\varepsilon'X)=0${{</math>}} para as vari√°veis _hp_ e _wt_ (tamb√©m sem tomar a esperan√ßa).
- Logicamente, para estimar por GMM, precisamos escolher os par√¢metros {{<math>}}$\theta = \{ \beta_0, \beta_1 \}${{</math>}} que, ao tomar a esperan√ßa em cada um destas colunas, se aproximem ao m√°ximo de zero. Isso ser√° feito via fun√ß√£o `gmm()` (semelhante √† fun√ß√£o `optim()`)


##### 5. Cria√ß√£o de fun√ß√£o com os momentos
- Vamos criar uma fun√ß√£o que tem como input um vetor de par√¢metros (`params`) e uma base de dados (`data`), e que retorna uma matriz em que cada coluna representa um momento.
- Essa fun√ß√£o incluir√° todos os comandos descritos nos itens 1 a 4 (que, na verdade, apenas foram feitos por did√°tica).
```{r}
mom_ols = function(params, list) {
  # No GMM, s√≥ pode ter 1 input que n√£o sejam os par√¢metros
  # Por isso, foi inclu√≠do uma lista com 3 argumentos
  yname = list[[1]]
  xname = list[[2]]
  data = list[[3]]
  
  # Extraindo as vari√°veis da base em vetores
  y = as.matrix(data[yname])
  x = as.matrix(data[xname])
  X = cbind(1, x)
  
  # Extraindo os par√¢metros de params
  b0 = params[1]
  b1 = params[2]
  sig2 = params[3]
  
  ## Valores ajustados de y
  yhat = b0 + b1 * x
  
  ## Res√≠duos
  e_hat = y - yhat
  
  ## Matriz de momentos
  m = as.numeric(e_hat) * X
  m # output da fun√ß√£o
}
```


##### 6. Otimiza√ß√£o via fun√ß√£o `gmm()`
- A fun√ß√£o `gmm()`, assim como a `optim()`, recebe uma fun√ß√£o como argumento.
- No entanto, ao inv√©s de retornar um valor, a fun√ß√£o que entra no `gmm()` retorna uma matriz, cujas m√©dias das colunas queremos aproximar de zero. 
```{r}
library(gmm)

gmm_lm = gmm(g=mom_ols, 
             x=list(yname="mpg", xname="hp", data=mtcars), # argumentos fun√ß√£o
             t0=c(0,0), # chute inicial de params
             wmatrix = "optimal", # matriz de pondera√ß√£o
             optfct = "nlminb" # fun√ß√£o de otimiza√ß√£o
             )

summary(gmm_lm)$coefficients
```



</br>

{{< cta cta_text="üëâ Proceed to Multiple Regression" cta_link="../sec8" >}}
