---
date: "2018-09-09T00:00:00Z"
# icon: book
# icon_pack: fas
linktitle: FGLS / WLS
summary: FGLS...
title: Feasible Generalized Least Squares
weight: 10
output: md_document
type: book
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# wd = "~/../OneDrive/FEA-RP/Disciplinas/REC5004_Econometria-I/Monitoria-FHN/PNADc" # Aspire
# wd = "~/../FEA-RP/Disciplinas/REC5004_Econometria-I/Monitoria-FHN/PNADc" # Nitro
```


## Matriz de variâncias-covariâncias dos erros

Relembre que a matriz de variâncias-covariâncias dos erros é dada por:

{{<math>}}$$ cov(\boldsymbol{\varepsilon}) = \underset{N \times N}{\boldsymbol{\Sigma}} = 
\left[ \begin{array}{cccc}
var(\varepsilon_{1}) & cov(\varepsilon_{1}, \varepsilon_{2}) & \cdots & cov(\varepsilon_{1}, \varepsilon_{N}) \\
cov(\varepsilon_{2}, \varepsilon_{1}) & var(\varepsilon_{2}) & \cdots & cov(\varepsilon_{2}, \varepsilon_{N}) \\
\vdots & \vdots & \ddots & \vdots \\
cov(\varepsilon_{N}, \varepsilon_{1}) & cov(\varepsilon_{N}, \varepsilon_{2}) & \cdots & var(\varepsilon_{N}) 
\end{array} \right]$${{</math>}}

Como assumimos amostragem aleatória, a covariância entre dois indivíduos distintos {{<math>}}($i \neq j$){{</math>}} é  
{{<math>}}$$ cov(\varepsilon_{i}, \varepsilon_{j}) = 0,  \qquad \text{para todo } i \neq j.$${{</math>}}

Logo, 
{{<math>}}$$ \boldsymbol{\Sigma} = 
\left[ \begin{array}{cccc}
var(\varepsilon_{1}) & 0 & \cdots & 0 \\
0 & var(\varepsilon_{2}) & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & var(\varepsilon_{N}) 
\end{array} \right]$${{</math>}}


Para MQO, assumíamos homocedasticidade e, portanto, a diagonal principal era toda preenchida por um mesmo {{<math>}}$ var(\varepsilon_i) = \sigma^2,\ \forall i${{</math>}}.
Na presença de **heteroscedasticidade**, segue que {{<math>}}$ var(\varepsilon_i) = \sigma^2_i,\ \forall i${{</math>}} e, logo:

{{<math>}}$$ \boldsymbol{\Sigma} = 
\left[ \begin{array}{cccc}
\sigma^2_1 & 0 & \cdots & 0 \\
0 & \sigma^2_2 & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & \sigma^2_N 
\end{array} \right] \ \neq\ \sigma^2 I_N $${{</math>}}

Disto, segue que:

{{<math>}}$$ \boldsymbol{\Sigma}^{-1} = 
\left[ \begin{array}{cccc}
1/\sigma^2_1 & 0 & \cdots & 0 \\
0 & 1/\sigma^2_2 & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & 1/\sigma^2_N 
\end{array} \right]$${{</math>}}
e
{{<math>}}$$\boldsymbol{\Sigma}^{-0.5} = 
\left[ \begin{array}{cccc}
1/\sigma_1 & 0 & \cdots & 0 \\
0 & 1/\sigma_2 & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & 1/\sigma_N 
\end{array} \right]$${{</math>}}


</br>


## Estimador MQGF

- O estimador de MQGF, considerando dados em corte transversal, é dado por
{{<math>}}$$ {\hat{\boldsymbol{\beta}}}_{\scriptscriptstyle{MQGF}} = (\boldsymbol{X}' {\boldsymbol{\Sigma}}^{-1} \boldsymbol{X})^{-1} (\boldsymbol{X}' {\boldsymbol{\Sigma}}^{-1} \boldsymbol{y}) $${{</math>}}

- A matriz de variâncias-covariâncias do estimador é dada por
{{<math>}}$$ V(\hat{\boldsymbol{\beta}}_{\scriptscriptstyle{MQGF}}) = (\boldsymbol{X}' \boldsymbol{\Sigma}^{-1} \boldsymbol{X})^{-1} $${{</math>}}

- O problema é que desconhecemos {{<math>}}$\boldsymbol{\Sigma}${{</math>}}, e precisamos fazer mais premissas sobre a matriz de variâncias-covariâncias dos erros para estimar {{<math>}}$\boldsymbol{\hat{\Sigma}}${{</math>}}.



</br>

## Estimador MQP

- [Seção 4.1 de Heiss (2020)](http://www.urfie.net/downloads/PDF/URfIE_web.pdf)
- [Weighted Least Squares (Yibi Huang)](https://www.stat.uchicago.edu/~yibi/teaching/stat224/L14.pdf)
- Um caso especial de MQGF é o estimador de Mínimos Quadrados Ponderados (MQP/WLS), que considera que as variância de um indivíduo é proporcional às variâncias dos demais a partir uma função de variáveis explicativas, {{<math>}}$g(\boldsymbol{x}'_i) ${{</math>}}, conhecida a priori:
{{<math>}}$$ Var(\varepsilon_i | \boldsymbol{x}'_i) = \sigma^2.g(\boldsymbol{x}'_i) $${{</math>}}

- Por exemplo, considere que a variância das mulheres é o dobro da variância dos homens ({{<math>}}$\sigma^2_M = 2.\sigma^2_H ${{</math>}}), então:
{{<math>}}$$ g(\text{female}_i) = \left\{ \begin{matrix} 
2, &\text{se female}_i = 1 \\
1, &\text{se female}_i = 0
\end{matrix} \right. $${{</math>}}

- Logo, a matriz de variâncias-covariâncias dos erros pode ser simplificada:
{{<math>}}\begin{align} \boldsymbol{\Sigma} &= 
\left[ \begin{array}{cccc}
\sigma^2_M & \cdots & 0 & 0 & \cdots & 0 \\
\vdots & \ddots & \vdots & \vdots & \ddots & \vdots \\
0 & \cdots & \sigma^2_M & 0 & \cdots & 0 \\
0 & \cdots & 0 & \sigma^2_H & \cdots & 0 \\
\vdots & \ddots & \vdots & \vdots & \ddots & \vdots \\
0 & \cdots & 0 & 0 & \cdots & \sigma^2_H \\
\end{array} \right] \\
&= \left[ \begin{array}{cccc}
2\sigma^2 & \cdots & 0 & 0 & \cdots & 0 \\
\vdots & \ddots & \vdots & \vdots & \ddots & \vdots \\
0 & \cdots & 2\sigma^2 & 0 & \cdots & 0 \\
0 & \cdots & 0 & \sigma^2 & \cdots & 0 \\
\vdots & \ddots & \vdots & \vdots & \ddots & \vdots \\
0 & \cdots & 0 & 0 & \cdots & \sigma^2 \\
\end{array} \right] \\ 
&= \left[ \begin{array}{cccc}
2\sigma^2 & \cdots & 0 & 0 & \cdots & 0 \\
\vdots & \ddots & \vdots & \vdots & \ddots & \vdots \\
0 & \cdots & 2\sigma^2 & 0 & \cdots & 0 \\
0 & \cdots & 0 & \sigma^2 & \cdots & 0 \\
\vdots & \ddots & \vdots & \vdots & \ddots & \vdots \\
0 & \cdots & 0 & 0 & \cdots & \sigma^2 \\
\end{array} \right] \\
&= \sigma^2 \left[ \begin{array}{cccc}
2 & \cdots & 0 & 0 & \cdots & 0 \\
\vdots & \ddots & \vdots & \vdots & \ddots & \vdots \\
0 & \cdots & 2 & 0 & \cdots & 0 \\
0 & \cdots & 0 & 1 & \cdots & 0 \\
\vdots & \ddots & \vdots & \vdots & \ddots & \vdots \\
0 & \cdots & 0 & 0 & \cdots & 1 \\
\end{array} \right] 
\end{align}{{</math>}}
.

- Logo, podemos também escrever:

{{<math>}}$$ \boldsymbol{\Sigma}^{-1} = 
\frac{1}{\sigma^2} \left[ \begin{array}{cccc}
\frac{1}{2} & \cdots & 0 & 0 & \cdots & 0 \\
\vdots & \ddots & \vdots & \vdots & \ddots & \vdots \\
0 & \cdots & \frac{1}{2} & 0 & \cdots & 0 \\
0 & \cdots & 0 & \frac{1}{1} & \cdots & 0 \\
\vdots & \ddots & \vdots & \vdots & \ddots & \vdots \\
0 & \cdots & 0 & 0 & \cdots & \frac{1}{1} \\
\end{array} \right] \equiv 
\frac{1}{\sigma^2} \boldsymbol{W}, $${{</math>}}
em que {{<math>}}$ \boldsymbol{W} ${{</math>}} é uma matriz de pesos, e também
{{<math>}}$$ \boldsymbol{\Sigma}^{-0.5} = 
\frac{1}{\sigma} \left[ \begin{array}{cccc}
\frac{1}{\sqrt{2}} & \cdots & 0 & 0 & \cdots & 0 \\
\vdots & \ddots & \vdots & \vdots & \ddots & \vdots \\
0 & \cdots & \frac{1}{\sqrt{2}} & 0 & \cdots & 0 \\
0 & \cdots & 0 & \frac{1}{\sqrt{1}} & \cdots & 0 \\
\vdots & \ddots & \vdots & \vdots & \ddots & \vdots \\
0 & \cdots & 0 & 0 & \cdots & \frac{1}{\sqrt{1}} \\
\end{array} \right] \equiv 
\frac{1}{\sigma} \boldsymbol{W}^{0.5} $${{</math>}}

</br>

- Disto, podemos obter o estimador {{<math>}}$\hat{\boldsymbol{\beta}}_{\scriptscriptstyle{MQP}}${{</math>}}:

{{<math>}}\begin{align} \hat{\boldsymbol{\beta}}_{\scriptscriptstyle{MQGF}} &= (\boldsymbol{X}' {\boldsymbol{\Sigma}}^{-1} \boldsymbol{X})^{-1} (\boldsymbol{X}' {\boldsymbol{\Sigma}}^{-1} \boldsymbol{y}) \\
&= \left(\boldsymbol{X}' \frac{1}{\sigma^2} \boldsymbol{W} \boldsymbol{X} \right)^{-1} \left(\boldsymbol{X}' \frac{1}{\sigma^2} \boldsymbol{W} \boldsymbol{y} \right) \\
&= \sigma^2 \left(\boldsymbol{X}' \boldsymbol{W} \boldsymbol{X} \right)^{-1} \frac{1}{\sigma^2} \left(\boldsymbol{X}' \boldsymbol{W} \boldsymbol{y} \right) \\
\hat{\boldsymbol{\beta}}_{\scriptscriptstyle{MQP}} &\equiv \left(\boldsymbol{X}' \boldsymbol{W} \boldsymbol{X} \right)^{-1} \left(\boldsymbol{X}' \boldsymbol{W} \boldsymbol{y} \right)
\end{align}{{</math>}}


- A matriz de variâncias-covariâncias do estimador de MQP é dada por

{{<math>}}\begin{align} V(\hat{\boldsymbol{\beta}}_{\scriptscriptstyle{MQGF}}) &= \left(\boldsymbol{X}' \boldsymbol{\Sigma}^{-1} \boldsymbol{X} \right)^{-1} \\
&= \left(\boldsymbol{X}' \frac{1}{\sigma^2} \boldsymbol{W} \boldsymbol{X} \right)^{-1} \\
V(\hat{\boldsymbol{\beta}}_{\scriptscriptstyle{MQP}}) &= \sigma^2 \left(\boldsymbol{X}' \boldsymbol{W} \boldsymbol{X} \right)^{-1} \end{align}{{</math>}}


<!-- - Note que, para calcularmos {{<math>}}$\hat{\boldsymbol{\beta}}_{\scriptscriptstyle{MQP}}${{</math>}} precisamos estimar {{<math>}}$\sigma^2${{</math>}}. Porém, a variância dos erros é estimada a partir de resíduos, {{<math>}}$\hat{\boldsymbol{\varepsilon}}${{</math>}}, que por sua vez são obtidos a partir de estimativas {{<math>}}$\hat{\boldsymbol{\beta}}${{</math>}}. -->

- A variância dos erros, {{<math>}}$\sigma^2${{</math>}}, pode ser estimada usando:
{{<math>}}$$ \hat{\sigma}^2 = \frac{\hat{\boldsymbol{\varepsilon}}' \boldsymbol{W} \hat{\boldsymbol{\varepsilon}}}{N-K-1} $${{</math>}}


</br>

- Também podemos transformar as variáveis e resolver por MQO, pré-multiplicando {{<math>}}$\boldsymbol{X}${{</math>}} e {{<math>}}$\boldsymbol{y}${{</math>}} por {{<math>}}$ \boldsymbol{W}^{0.5}${{</math>}}, e definindo:
{{<math>}}$$\tilde{\boldsymbol{X}} \equiv \boldsymbol{W}^{0.5} \boldsymbol{X} \qquad \text{e} \qquad \tilde{\boldsymbol{y}} \equiv \boldsymbol{W}^{0.5} \boldsymbol{y}$${{</math>}}

- No exemplo em que a variância da mulher é o dobro da variância do homem, temos:
{{<math>}}\begin{align} \boldsymbol{W}^{0.5} \boldsymbol{y} &= \begin{bmatrix}
2^{0.5} & \cdots & 0 & 0 & \cdots & 0 \\
\vdots & \ddots & \vdots & \vdots & \ddots & \vdots \\
0 & \cdots & 2^{0.5} & 0 & \cdots & 0 \\
0 & \cdots & 0 & 1^{0.5} & \cdots & 0 \\
\vdots & \ddots & \vdots & \vdots & \ddots & \vdots \\
0 & \cdots & 0 & 0 & \cdots & 1^{0.5} \\
\end{bmatrix} \begin{bmatrix} y_1 \\ \vdots \\ y_M \\ y_{M+1} \\ \vdots \\ y_N \end{bmatrix}\\
&= \begin{bmatrix}
\frac{1}{\sqrt{2}} & \cdots & 0 & 0 & \cdots & 0 \\
\vdots & \ddots & \vdots & \vdots & \ddots & \vdots \\
0 & \cdots & \frac{1}{\sqrt{2}} & 0 & \cdots & 0 \\
0 & \cdots & 0 & \frac{1}{\sqrt{1}} & \cdots & 0 \\
\vdots & \ddots & \vdots & \vdots & \ddots & \vdots \\
0 & \cdots & 0 & 0 & \cdots & \frac{1}{\sqrt{1}} \\
\end{bmatrix} \begin{bmatrix}  y_1 \\ \vdots \\ y_M \\ y_{M+1} \\ \vdots \\ y_N \end{bmatrix} \\
&= \begin{bmatrix} \frac{1}{\sqrt{2}} y_1 \\ \vdots \\ \frac{1}{\sqrt{2}}  y_M \\ \frac{1}{\sqrt{1}}  y_{M+1} \\ \vdots \\ \frac{1}{\sqrt{1}} y_N \end{bmatrix} \end{align}{{</math>}}
em que {{<math>}}$M${{</math>}} é o número de mulheres na base de dados.

- Note que as variáveis {{<math>}}$\boldsymbol{y}${{</math>}} e {{<math>}}$\boldsymbol{X}${{</math>}} ficam multiplicadas pelo inverso da raiz de seus respectivos pesos, quando as pré-multiplicamos por {{<math>}}$\boldsymbol{W}${{</math>}}.


- Observe também que os estimadores são equivalentes:
{{<math>}}\begin{align} \hat{\boldsymbol{\beta}}_{\scriptscriptstyle{MQP}} &= \left(\boldsymbol{X}' \boldsymbol{W} \boldsymbol{X} \right)^{-1} \left(\boldsymbol{X}' \boldsymbol{W} \boldsymbol{y} \right) \\
&= \left(\boldsymbol{X}' \boldsymbol{W}^{0.5} \boldsymbol{W}^{0.5} \boldsymbol{X} \right)^{-1} \left(\boldsymbol{X}' \boldsymbol{W}^{0.5} \boldsymbol{W}^{0.5} \boldsymbol{y} \right) \\
&= \left(\boldsymbol{X}' {\boldsymbol{W}^{0.5}}^{\prime} \boldsymbol{W}^{0.5} \boldsymbol{X} \right)^{-1} \left(\boldsymbol{X}' {\boldsymbol{W}^{0.5}}^{\prime} \boldsymbol{W}^{0.5} \boldsymbol{y} \right) \\
&= \left( \left[ \boldsymbol{W}^{0.5} \boldsymbol{X} \right]' \boldsymbol{W}^{0.5} \boldsymbol{X} \right)^{-1} \left(\left[ \boldsymbol{W}^{0.5} \boldsymbol{X} \right]' \boldsymbol{W}^{0.5} \boldsymbol{y} \right) \\
&= ( \tilde{\boldsymbol{X}}' \tilde{\boldsymbol{X}} )^{-1} (\tilde{\boldsymbol{X}}' \tilde{\boldsymbol{y}} ) \equiv \tilde{\hat{\boldsymbol{\beta}}}_{\scriptscriptstyle{MQO}} \end{align}{{</math>}}
e
{{<math>}}\begin{align} V(\hat{\boldsymbol{\beta}}_{\scriptscriptstyle{MQP}}) &= \sigma^2 \left(\boldsymbol{X}' \boldsymbol{W} \boldsymbol{X} \right)^{-1} \\
&= \sigma^2 \left(\boldsymbol{X}' {\boldsymbol{W}^{0.5}} \boldsymbol{W}^{0.5}  \boldsymbol{X} \right)^{-1} \\
&= \sigma^2 \left(\boldsymbol{X}' {\boldsymbol{W}^{0.5}}^{\prime} \boldsymbol{W}^{0.5}  \boldsymbol{X} \right)^{-1} \\
&= \sigma^2 \left(\left[ \boldsymbol{W}^{0.5} \boldsymbol{X} \right]' \boldsymbol{W}^{0.5}  \boldsymbol{X} \right)^{-1} \\
V(\tilde{\hat{\boldsymbol{\beta}}}_{\scriptscriptstyle{MQO}}) &= \sigma^2 (\tilde{\boldsymbol{X}}' \tilde{\boldsymbol{X}} )^{-1}
\end{align}{{</math>}}

em que usamos {{<math>}}$\boldsymbol{W}^{0.5} = {\boldsymbol{W}^{0.5}}^{\prime}${{</math>}}.



### Estimação via `lm()`

<!-- - Usaremos o Exemplo 8.6 (Equação de Riqueza Financeira) do Wooldridge (2016), que quer estimar o seguinte modelo: -->
<!-- {{<math>}}$$\text{nettfa} = \beta_0 + \beta_1 \text{inc} + \beta_2 \text{agesq} + \beta_3 \text{male} + \beta_4 \text{e401k} + \varepsilon $${{</math>}} -->
<!-- em que: -->
<!--   - _nettfa_: saúde financeira líquida em $1.000 -->
<!--   - _inc_: é a renda em $1.000 -->
<!--   - _agesq_: é a idade ao quadrado -->
<!--   - _male_: dummy de homem = 1 e mulher = 0 -->
<!--   - _e401k_: dummy se é elegível à previdência 401k -->
<!-- - Suspeita-se que a variância do erro seja proporcional à renda (_inc_), tal que -->
<!-- {{<math>}}$$ Var(\varepsilon|\text{inc}) = \sigma^2 \text{inc}, $${{</math>}} -->
<!-- ou seja, a variância do erro eleva-se com o aumento da renda. -->

<!-- </br> -->

<!-- - No R, carregaremos a base de dados `k401ksubs` do pacote `wooldridge` -->
<!-- - Para estimar o MQP via `lm()`, precisamos informar o inverso do peso no argumento `weights` -->
<!-- - Vamos usar apenas os indivíduos que moram sozinhos `fsize==1` -->


<!-- ```{r} -->
<!-- data("k401ksubs", package = "wooldridge") -->
<!-- k401ksubs = k401ksubs[k401ksubs$fsize==1,] -->

<!-- # Estimações -->
<!-- k401k.ols = lm(nettfa ~ inc + agesq + male + e401k, k401ksubs) -->
<!-- k401k.wls = lm(nettfa ~ inc + agesq + male + e401k, weights=1/inc, k401ksubs) -->


<!-- # Resumindo 5 estimações em única tabela -->
<!-- stargazer::stargazer(k401k.ols, k401k.wls, -->
<!--                      digits=4, type="text", omit.stat="f") -->
<!-- ``` -->

- Aqui usaremos um exemplo parecido com o que simulamos em uma seção anterior.
- Vamos criar observações do seguinte modelo real com presença de heterocedasticidade:
{{<math>}}$$ y = \tilde{\beta}_0 + \tilde{\beta}_1 x + \tilde{\varepsilon}, \qquad \tilde{\varepsilon} \sim N(0, (10x)^2) $${{</math>}}
logo
{{<math>}}$$ Var(\tilde{\varepsilon}_i | x_i) = \sigma^2 (10x_i)^2 \quad \implies\quad sd(\tilde{\varepsilon}_i | x_i) = \sigma (10x_i) $${{</math>}}
- Para estimar o MQP via `lm()`, precisamos informar os pesos no argumento `weights`

```{r}
# Definindo parâmetros
b0til = 50
b1til = -5
N = 100

# Gerando x e y por simulação
set.seed(123)
x = runif(N, 1, 9) # Gerando 100 obs. de x
e_til = rnorm(N, 0, 10*x) # Erros: 100 obs. de média 0 e desv pad 10x
y = b0til + b1til*x + e_til # calculando observações y
plot(x, y)
```

- Agora, vamos estimar por MQO e MQP o seguinte modelo empírico
{{<math>}}$$ y = \beta_0 + \beta_1 x + \varepsilon $${{</math>}}

```{r warning=FALSE}
# Estimações
ols = lm(y ~ x) # estimação por MQO
wls = lm(y ~ x, weights=1/x^2) # estimação por MQP
stargazer::stargazer(ols, wls, digits=2, type="text", omit.stat="f")
```

- Note que tanto faz colocar `1/x^2` ou `1/(10*x)^2` nos pesos, pois o que importa são os pesos relativos de variância entre as observações
- Veja também que a estimação por MQP foi mais eficiente - produziu erros padrão menores, dado que conhecíamos, a priori, que a variância do erro era proporcional à variável _x_.
- Na prática, é difícil conhecer/defender uma forma exata da heterocedasticidade, já que não conhecemos o modelo real.



### Estimação Analítica

a) Criando vetores/matrizes e definindo _N_ e _K_
```{r}
# Criando o vetor y
y = as.matrix(y) # transformando coluna de data frame em matriz

# Criando a matriz de covariadas X com primeira coluna de 1's
X = as.matrix( cbind(1, x) ) # juntando 1's com x

# Pegando valores K
K = ncol(X) - 1
```


b) Matriz de pesos {{<math>}}$\boldsymbol{W}${{</math>}}

- É a matriz cuja diagonal principal é preenchida pelos valores inversos de _x_

```{r}
W = diag(1/x^2)
round(W[1:10,1:10], 2)
```

c) Estimativas MQP {{<math>}}$\hat{\boldsymbol{\beta}}_{\scriptscriptstyle{MQP}}${{</math>}}

{{<math>}}$$ \hat{\boldsymbol{\beta}}_{\scriptscriptstyle{MQP}} = (\boldsymbol{X}' \boldsymbol{W} \boldsymbol{X})^{-1} \boldsymbol{X}' \boldsymbol{W} \boldsymbol{y} $${{</math>}}

```{r}
bhat_WLS = solve( t(X) %*% W %*% X ) %*% t(X) %*% W %*% y
bhat_WLS
``` 

d) Valores ajustados {{<math>}}$\hat{\boldsymbol{y}}_{\scriptscriptstyle{MQP}}${{</math>}}
```{r}
yhat_WLS = X %*% bhat_WLS
head(yhat_WLS)
``` 


e) Resíduos {{<math>}}$\hat{\boldsymbol{\varepsilon}}_{\scriptscriptstyle{MQP}}${{</math>}}
```{r}
ehat_WLS = y - yhat_WLS
head(ehat_WLS)
``` 

f) Estimativa da variância do erro {{<math>}}$\hat{\sigma}^2_{\scriptscriptstyle{MQP}}${{</math>}}
{{<math>}}$$\hat{\sigma}^2 =  \frac{\hat{\boldsymbol{\varepsilon}}' \boldsymbol{W} \hat{\boldsymbol{\varepsilon}}}{N - K - 1} $${{</math>}}

```{r}
sig2hat_WLS = as.numeric( t(ehat_WLS) %*% W %*% ehat_WLS / (N-K-1) )
sig2hat_WLS
```

h) Matriz de Variâncias-Covariâncias do Estimador

{{<math>}}$$ \widehat{\text{Var}}(\hat{\boldsymbol{\beta}}_{\scriptscriptstyle{MQP}}) = (\boldsymbol{X}' \boldsymbol{W} \boldsymbol{X})^{-1} $${{</math>}}

```{r}
Vbhat_WLS = sig2hat_WLS * solve( t(X) %*% W %*% X )
Vbhat_WLS
```


i) Erros-padrão do estimador {{<math>}}$\text{se}(\hat{\boldsymbol{\beta}}_{\scriptscriptstyle{MQP}})${{</math>}}

É a raiz quadrada da diagonal principal da Matriz de Variâncias-Covariâncias do Estimador
```{r}
se_WLS = sqrt( diag(Vbhat_WLS) )
se_WLS
```

j) Estatística _t_

{{<math>}}$$ t_{\hat{\beta}_k} = \frac{\hat{\beta}_k}{\text{se}(\hat{\beta}_k)} 
$$ {{</math>}}

```{r}
# Cálculo da estatística t
t_WLS = bhat_WLS / se_WLS
t_WLS
```

k) P-valor

{{<math>}}$$ p_{\hat{\beta}_k} = 2.\Phi_{t_{(N-K-1)}}(-|t_{\hat{\beta}_k}|), $${{</math>}}

```{r}
# p-valor
p_WLS = 2 * pt(-abs(t_WLS), N-K-1)
p_WLS
```

l) Tabela-resumo
```{r}
data.frame(bhat_WLS, se_WLS, t_WLS, p_WLS) # resultado MQP
summary(wls)$coef # resultado MQP via lm()
```



#### Transformando e estimando por MQO
- Agora, vamos transformar as variáveis e resolver por MQO, pré-multiplicando {{<math>}}$\boldsymbol{X}${{</math>}} e {{<math>}}$\boldsymbol{y}${{</math>}} por {{<math>}}$ \boldsymbol{W}^{0.5}${{</math>}}, e definindo:

{{<math>}}$$\tilde{\boldsymbol{X}} \equiv \boldsymbol{W}^{0.5} \boldsymbol{X} \qquad \text{e} \qquad \tilde{\boldsymbol{y}} \equiv \boldsymbol{W}^{0.5} \boldsymbol{y}$${{</math>}}


b') Matriz de pesos {{<math>}}$\boldsymbol{W}^{0.5}${{</math>}}

- É a matriz cuja diagonal principal é preenchida pelas raízes quadradas dos pesos

```{r}
W_0.5 = diag(1/x)
round(W_0.5[1:10,1:10], 2)
```

b'') Variáveis transformadas {{<math>}}$\tilde{\boldsymbol{y}}${{</math>}} e {{<math>}}$\tilde{\boldsymbol{X}}${{</math>}}
```{r}
ytil = W_0.5 %*% y
Xtil = W_0.5 %*% X

plot(x, ytil, ylim=c(-125,175), 
     main=expression(paste("Gráfico ", x ," \u00D7 ", tilde(y))),
     xlab=expression(x), ylab=expression(tilde(y))) # plot xtil e ytil
plot(x, y, ylim=c(-125,175), 
     main=expression(paste("Gráfico ", x ," \u00D7 ", y)),
     xlab=expression(x), ylab=expression(y)) # plot x e y
```


c') Estimativas MQO {{<math>}}$\tilde{\hat{\boldsymbol{\beta}}}_{\scriptscriptstyle{MQO}}${{</math>}}

{{<math>}}$$ \tilde{\hat{\boldsymbol{\beta}}}_{\scriptscriptstyle{MQo}} = (\tilde{\boldsymbol{X}}' \tilde{\boldsymbol{X}})^{-1} \tilde{\boldsymbol{X}}' \tilde{\boldsymbol{y}} $${{</math>}}

```{r}
bhat_OLS = solve( t(Xtil) %*% Xtil ) %*% t(Xtil) %*% ytil
bhat_OLS
``` 

d') Valores ajustados {{<math>}}$\tilde{\hat{\boldsymbol{y}}}_{\scriptscriptstyle{MQO}}${{</math>}}
```{r}
yhat_OLS = Xtil %*% bhat_OLS
head(yhat_OLS)
``` 


e') Resíduos {{<math>}}$\tilde{\hat{\boldsymbol{\varepsilon}}}_{\scriptscriptstyle{MQO}}${{</math>}}
```{r}
ehat_OLS = ytil - yhat_OLS
head(ehat_OLS)
``` 

f') Estimativa da variância do erro {{<math>}}$\tilde{\hat{\sigma}}^2_{\scriptscriptstyle{MQO}}${{</math>}}
{{<math>}}$$\tilde{\hat{\sigma}}^2 =  \frac{\tilde{\hat{\boldsymbol{\varepsilon}}}' \tilde{\hat{\boldsymbol{\varepsilon}}}}{N - K - 1} $${{</math>}}

```{r}
sig2hat_OLS = as.numeric( t(ehat_OLS) %*% ehat_OLS / (N-K-1) )
sig2hat_OLS
```

h') Matriz de Variâncias-Covariâncias do Estimador

{{<math>}}$$ \widehat{\text{Var}}(\hat{\boldsymbol{\beta}}_{\scriptscriptstyle{MQO}}) = (\boldsymbol{X}' \boldsymbol{W} \boldsymbol{X})^{-1} $${{</math>}}

```{r}
Vbhat_OLS = sig2hat_OLS * solve( t(Xtil) %*% Xtil )
Vbhat_OLS
```


i') Erros-padrão do estimador {{<math>}}$\text{se}(\hat{\boldsymbol{\beta}}_{\scriptscriptstyle{MQP}})${{</math>}}

É a raiz quadrada da diagonal principal da Matriz de Variâncias-Covariâncias do Estimador
```{r}
se_OLS = sqrt( diag(Vbhat_OLS) )
se_OLS
```

j') Estatística _t_

{{<math>}}$$ t_{\hat{\beta}_k} = \frac{\hat{\beta}_k}{\text{se}(\hat{\beta}_k)} 
$$ {{</math>}}

```{r}
# Cálculo da estatística t
t_OLS = bhat_OLS / se_OLS
t_OLS
```

k') P-valor

{{<math>}}$$ p_{\hat{\beta}_k} = 2.\Phi_{t_{(N-K-1)}}(-|t_{\hat{\beta}_k}|), $${{</math>}}

```{r}
# p-valor
p_OLS = 2 * pt(-abs(t_OLS), N-K-1)
p_OLS
```

l') Tabela-resumo
```{r}
data.frame(bhat_OLS, se_OLS, t_OLS, p_OLS) # resultado MQO transformado
summary(wls)$coef # resultado MQP via lm()
```




</br>


{{< cta cta_text="👉 Proceed to Instrumental Variable" cta_link="../sec11" >}}


