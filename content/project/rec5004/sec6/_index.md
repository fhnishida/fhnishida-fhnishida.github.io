---
date: "2018-09-09T00:00:00Z"
# icon: book
# icon_pack: fas
linktitle: Simple Regression
summary: This page covers topics such as simple MQO regression and assumptions violations. Also includes examples and code snippets to illustrate the concepts discussed.
title: Simple Regression
weight: 6
output: md_document
type: book
---





## Regress√£o simples por MQO
- [Se√ß√£o 2.1 de Heiss (2020)](http://www.urfie.net/read/index.html#page/93)
- Considere o seguinte modelo emp√≠rico
$$ y = \beta_0 + \beta_1 x + \varepsilon \tag{2.1} $$
- Os estimadores de m√≠nimos quadrados ordin√°rios (MQO), segundo Wooldridge (2006, Se√ß√£o 2.2) √© dado por

{{<math>}}\begin{align}
    \hat{\beta}_0 &= \bar{y} - \hat{\beta}_1 \bar{x} \tag{2.2}\\
    \hat{\beta}_1 &= \frac{cov(x,y)}{var(x)} \tag{2.3}
\end{align}{{</math>}}

- E os valores ajustados/preditos, {{<math>}}$\hat{y}${{</math>}} √© dado por
$$ \hat{y} = \hat{\beta}_0 + \hat{\beta}_1 x \tag{2.4} $$
e os res√≠duos podem ser obtidos por 
$$ \hat{\varepsilon} = y - \hat{y} $$

### Exemplo 2.3: Sal√°rio de Diretores Executivos e Retornos de A√ß√µes

- Considere o seguinte modelo de regress√£o simples
$$ \text{salary} = \beta_0 + \beta_1 \text{roe} + \varepsilon $$
em que `salary` √© a remunera√ß√£o de um diretor executivo em milhares de d√≥lares e `roe` √© o retorno sobre o patrim√¥nio l√≠quido em percentual.


#### Estima√ß√£o Anal√≠tica ("na m√£o")


```r
# Carregando a base de dados do pacote 'wooldridge'
data(ceosal1, package="wooldridge")

cov(ceosal1$salary, ceosal1$roe) # covari√¢ncia entre salary e roe
```

```
## [1] 1342.538
```

```r
var(ceosal1$roe) # vari√¢ncia do retorno sobre o patrim√¥nio l√≠quido
```

```
## [1] 72.56499
```

```r
mean(ceosal1$roe) # m√©dia do retorno sobre o patrim√¥nio l√≠quido
```

```
## [1] 17.18421
```

```r
mean(ceosal1$salary) # m√©dia do sal√°rio
```

```
## [1] 1281.12
```

```r
# C√°lculo "na m√£o" das estimativas de MQO
b1hat = cov(ceosal1$salary, ceosal1$roe) / var(ceosal1$roe) # por (2.3)
b1hat
```

```
## [1] 18.50119
```

```r
b0hat = mean(ceosal1$salary) - b1hat*mean(ceosal1$roe) # por (2.2)
b0hat
```

```
## [1] 963.1913
```

- Vemos que um incremento de uma unidade (porcento) no retorno sobre o patrim√¥nio l√≠quido (_roe_), aumentar 18 unidades (milhares de d√≥lares) nos sal√°rios dos diretores executivos.


#### Estima√ß√£o via `lm()`
- Uma maneira mais conveniente de fazer a estima√ß√£o por MQO √© usando a fun√ß√£o `lm()`
- Em um modelo univariado, inserimos dois vetores (vari√°veis dependente e independente) separados por um til (`~`):

```r
lm(ceosal1$salary ~ ceosal1$roe)
```

```
## 
## Call:
## lm(formula = ceosal1$salary ~ ceosal1$roe)
## 
## Coefficients:
## (Intercept)  ceosal1$roe  
##       963.2         18.5
```

- Tamb√©m podemos deixar de usar o prefixo `ceosal1$` antes dos nomes do vetores ao preenchermos o argumento `data = ceosal1`

```r
lm(salary ~ roe, data=ceosal1)
```

```
## 
## Call:
## lm(formula = salary ~ roe, data = ceosal1)
## 
## Coefficients:
## (Intercept)          roe  
##       963.2         18.5
```

- Podemos usar a fun√ß√£o `lm()` para incluir uma reta de regress√£o no gr√°fico

```r
# Gr√°fico de dispers√£o (scatter)
plot(ceosal1$roe, ceosal1$salary)

# Adicionando a reta de regress√£o
abline(lm(salary ~ roe, data=ceosal1), col="red")
```

<img src="/project/rec5004/sec6/_index_files/figure-html/unnamed-chunk-4-1.png" width="672" />


</br>

## Coeficientes, Valores Ajustados e Res√≠duos
- [Se√ß√£o 2.2 de Heiss (2020)](http://www.urfie.net/read/index.html#page/98)
- Podemos "guardar" os resultados da estima√ß√£o em um objeto (da classe `list`) e, depois, extrair informa√ß√µes dele.

```r
# atribuindo o resultado da regress√£o em um objeto
reg = lm(salary ~ roe, data=ceosal1)

# verificando os "nomes" das informa√ß√µes contidas no objeto
names(reg)
```

```
##  [1] "coefficients"  "residuals"     "effects"       "rank"         
##  [5] "fitted.values" "assign"        "qr"            "df.residual"  
##  [9] "xlevels"       "call"          "terms"         "model"
```

- Podemos usar a fun√ß√£o `coef()` para extrairmos um data frame com os coeficientes da regress√£o

```r
# Extraindo vetor de coeficientes da regress√£o
bhat = coef(reg)
bhat
```

```
## (Intercept)         roe 
##   963.19134    18.50119
```

```r
b0hat = bhat["(Intercept)"] # ou bhat[1]
b1hat =  bhat["roe"] # ou bhat[2]
```

- Dados estes par√¢metros estimados, podemos calcular os valores ajustados/preditos, {{<math>}}$\hat{y}${{</math>}}, e os desvios, {{<math>}}$\hat{\varepsilon}${{</math>}}, para cada observa√ß√£o {{<math>}}$i=1, ..., n${{</math>}}:

{{<math>}}\begin{align}
    \hat{y}_i &= \hat{\beta}_0 + \hat{\beta}_1 . x_i \tag{2.5} \\
    \hat{\varepsilon}_i &= y_i - \hat{y}_i \tag{2.6}
\end{align}{{</math>}}


```r
# Extraindo vari√°veis de ceosal1 em vetores
sal = ceosal1$salary
roe = ceosal1$roe

# Calculando os valores ajustados/preditos
sal_hat = b0hat + (b1hat * roe)

# Calculando os desvios
ehat = sal - sal_hat

# Visualizando as 6 primerias linhas de sal, roe, sal_hat e ehat
head( cbind(sal, roe, sal_hat, ehat) )
```

```
##       sal  roe  sal_hat      ehat
## [1,] 1095 14.1 1224.058 -129.0581
## [2,] 1001 10.9 1164.854 -163.8543
## [3,] 1122 23.5 1397.969 -275.9692
## [4,]  578  5.9 1072.348 -494.3483
## [5,] 1368 13.8 1218.508  149.4923
## [6,] 1145 20.0 1333.215 -188.2151
```

- Com as fun√ß√µes `fitted()` e `resid()` podemos extrair os valores ajustados e os res√≠duos do objeto com resultado da regress√£o:

```r
head( cbind(fitted(reg), resid(reg)) )
```

```
##       [,1]      [,2]
## 1 1224.058 -129.0581
## 2 1164.854 -163.8543
## 3 1397.969 -275.9692
## 4 1072.348 -494.3483
## 5 1218.508  149.4923
## 6 1333.215 -188.2151
```

```r
# Ou tamb√©m
head( cbind(reg$fitted, reg$residuals) )
```

```
##       [,1]      [,2]
## 1 1224.058 -129.0581
## 2 1164.854 -163.8543
## 3 1397.969 -275.9692
## 4 1072.348 -494.3483
## 5 1218.508  149.4923
## 6 1333.215 -188.2151
```


- Na se√ß√£o 2.3 de Wooldridge (2006), vemos que a estima√ß√£o por MQO usa as seguintes hip√≥teses:
{{<math>}}\begin{align}
    &\sum^n_{i=1}{\hat{\varepsilon}_i} = 0 \quad \implies \quad \bar{\hat{\varepsilon}} = 0 \tag{2.7} \\
    &\sum^n_{i=1}{x_i \hat{\varepsilon}_i} = 0 \quad \implies \quad cov(x,\hat{\varepsilon}) = 0 \tag{2.8}
\end{align}{{</math>}}

- Podemos verific√°-los em nosso exemplo:

```r
# Verificando (2.7)
mean(ehat) # bem pr√≥ximo de 0
```

```
## [1] -2.666235e-14
```

```r
# Verificando (2.8)
cov(ceosal1$roe, ehat) # bem pr√≥ximo de 0
```

```
## [1] -7.012777e-13
```


<!-- - **IMPORTANTE**: Isso s√≥ quer dizer que o MQO escolhe {{<math>}}$\hat{\beta}_0${{</math>}} e {{<math>}}$\hat{\beta}_1${{</math>}} tais que 2.7 e 2.8 sejam verdadeiros. -->
<!-- - Isto **N√ÉO** quer dizer que, para o modelo real as seguintes hip√≥teses sejam verdadeiras: -->
<!-- {{<math>}}\begin{align} -->
<!--     &E(\varepsilon) = 0 \tag{2.7'} \\ -->
<!--     &E(x\varepsilon) = 0 \quad \Longrightarrow \quad cov(x, \varepsilon) = 0 \tag{2.8'} -->
<!-- \end{align}{{</math>}} -->
<!-- - De fato, se 2.7' e 2.8' n√£o forem v√°lidos, a estima√ß√£o por MQO (que assume 2.7 e 2.8) ser√° viesada. -->


</br>

## Transforma√ß√µes log
- [Se√ß√£o 2.4 de Heiss (2020)](http://www.urfie.net/read/index.html#page/103)
- Tamb√©m podemos fazer estima√ß√µes transformando vari√°veis em n√≠vel para logaritmo.
- √â especialmente importante para transformar modelos n√£o-lineares em lineares - quando o par√¢metro est√° no expoente ao inv√©s estar multiplicando:
  
$$ y = A K^\alpha L^\beta\quad \overset{\text{log}}{\rightarrow}\quad \log(y) = \log(A) + \alpha \log(K) + \beta \log(L) $$

- Tamb√©m √© frequentemente utilizada em vari√°veis dependentes {{<math>}}$y \ge 0${{</math>}}


<center><img src="../tab_2-3.png"></center>

- H√° duas maneiras de fazer a transforma√ß√£o log:
    - Criar um novo vetor/coluna com a vari√°vel em log, ou
    - Usar a fun√ß√£o `log()` diretamente no vetor dentro da fun√ß√£o `lm()`


### Exemplo 2.11: Sal√°rio de Diretores Executivos e Vendas das Empresas
- Considere as vari√°veis:
    - `wage`: sal√°rio anual em milhares de d√≥lares
    - `sales`: vendas em milh√µes de d√≥lares


- _Modelo n√≠vel-n√≠vel_:

```r
# Carregando a base de dados
data(ceosal1, package="wooldridge")

# Estimando modelo n√≠vel-n√≠vel
lm(salary ~ sales, data=ceosal1)$coef
```

```
##  (Intercept)        sales 
## 1.174005e+03 1.547053e-02
```

- _Modelo log-n√≠vel_:

```r
# Estimando modelo log-n√≠vel
lm(log(salary) ~ sales, data=ceosal1)$coef
```

```
## (Intercept)       sales 
## 6.84665e+00 1.49825e-05
```

- _Modelo log-log_:

```r
# Estimando modelo log-log
lm(log(salary) ~ log(sales), data=ceosal1)$coef
```

```
## (Intercept)  log(sales) 
##   4.8219965   0.2566717
```


</br>

## Regress√£o a partir da origem ou sobre uma constante
- [Se√ß√£o 2.5 de Heiss (2020)](http://www.urfie.net/read/index.html#page/103)
- Para esstimar o modelo sem o intercepto (constante), precisamos adicionar um `0` ou `-1` nos regressores na fun√ß√£o `lm()`:

```r
data(ceosal1, package="wooldridge")
lm(salary ~ 0 + roe, data=ceosal1)
```

```
## 
## Call:
## lm(formula = salary ~ 0 + roe, data = ceosal1)
## 
## Coefficients:
##   roe  
## 63.54
```

- Ao regredirmos uma vari√°vel dependente sobre uma constante (1), obtemos a m√©dia desta vari√°vel.

```r
lm(salary ~ 1, data=ceosal1)
```

```
## 
## Call:
## lm(formula = salary ~ 1, data = ceosal1)
## 
## Coefficients:
## (Intercept)  
##        1281
```

```r
mean(ceosal1$salary, na.rm=TRUE)
```

```
## [1] 1281.12
```


</br>


<!-- ## Qualidade do ajuste -->
<!-- - [Se√ß√£o 2.3 de Heiss (2020)](http://www.urfie.net/read/index.html#page/101) -->
<!-- - A soma de quadrados total (SST), a soma de quadrados explicada (SSE) e a soma de quadrados dos res√≠duos (SSR) podem ser escritos como: -->

<!-- {{<math>}}\begin{align} -->
<!--     SST &= \sum^n_{i=1}{(y_i - \bar{y})^2} = (n-1) . var(y) \tag{2.10}\\ -->
<!--     SSE &= \sum^n_{i=1}{(\hat{y}_i - \bar{y})^2} = (n-1) . var(\hat{y}) \tag{2.11}\\ -->
<!--     SSR &= \sum^n_{i=1}{(\hat{\varepsilon}_i - 0)^2} = (n-1) . var(\hat{\varepsilon}) \tag{2.12} -->
<!-- \end{align}{{</math>}} -->
<!-- em que {{<math>}}$var(x) = \frac{1}{n-1} \sum^n_{i=1}{(x_i - \bar{x})^2}${{</math>}}. -->

<!-- - Wooldridge (2006) define o coeficiente de determina√ß√£o como: -->
<!-- {{<math>}}\begin{align} -->
<!--     R^2 &= \frac{SSE}{SST} = 1 - \frac{SSR}{SST}\\ -->
<!--         &= \frac{var(\hat{y})}{var(y)} = 1 - \frac{var(\hat{\varepsilon})}{var(y)} \tag{2.13} -->
<!-- \end{align}{{</math>}} -->
<!-- pois {{<math>}}$SST = SSE + SSR${{</math>}}. -->

<!-- ```{r} -->
<!-- # Calculando SST, SSE e SSR -->
<!-- SST = t(sal - mean(sal)) %*% (sal - mean(sal)) # produto interno y'y -->
<!-- SSE = t(sal_hat - mean(sal)) %*% (sal_hat - mean(sal)) # produto interno yhat'yhat -->
<!-- SSR = t(ehat) %*% ehat # produto interno ehat'ehat -->

<!-- # Calculando R^2 -->
<!-- SSE/SST -->
<!-- var(sal_hat)/var(sal) # "SSE/SST" -->
<!-- 1 - SSR/SST -->
<!-- 1 - var(ehat)/var(sal) # 1 - "SSR/SST" -->
<!-- ``` -->

<!-- - Para obter o {{<math>}}$R^2${{</math>}} de forma mais conveniente, pode-se usar a fun√ß√£o `summary()` sobre o objeto de resultado da regress√£o. Esta fun√ß√£o fornece uma visualiza√ß√£o dos resultados mais detalhada, incluindo o {{<math>}}$R^2${{</math>}}: -->
<!-- ```{r} -->
<!-- summary(reg) -->
<!-- ``` -->


</br>



## Viola√ß√µes de hip√≥tese
- [Subse√ß√£o 2.7.3 de Heiss (2020)](http://www.urfie.net/read/index.html#page/113), mas usando exemplo distinto.
- [Simulating a linear model (John Hopkins/Coursera)](https://www.coursera.org/learn/r-programming/lecture/u7in9/simulation-simulating-a-linear-model)
- Na pr√°tica, fazemos regress√µes a partir de observa√ß√µes contidas em bases de dados e n√£o sabemos qual √© o _modelo real_ que gerou essas observa√ß√µes.
- No R, podemos supor esse _modelo real_ e simular suas observa√ß√µes no R para analisar o que ocorre quando h√° viola√ß√£o de uma premissa de um modelo econom√©trico.
- Usaremos como exemplo a rela√ß√£o das horas de pr√°tica em culin√°ria com o n√∫mero de queimaduras na cozinha.


### Sem viola√ß√£o de hip√≥tese: Exemplo 1
- Sejam {{<math>}}$y${{</math>}} o n√∫mero de queimaduras na cozinha e {{<math>}}$x${{</math>}} o n√∫mero de horas gastas aprendendo a cozinhar.
- Suponha o _modelo real_:
$$ y = \tilde{\beta}_0 + \tilde{\beta}_1 x + \tilde{\varepsilon}, \qquad \tilde{\varepsilon} \sim N(0, 2^2) \tag{1}$$
em que {{<math>}}$\tilde{\beta}_0=50${{</math>}} e {{<math>}}$\tilde{\beta}_1=-5${{</math>}}.

1. Iremos definir {{<math>}}$\tilde{\beta}_0${{</math>}} e {{<math>}}$\tilde{\beta}_1${{</math>}}, e gerar, por simula√ß√£o, as observa√ß√µes de {{<math>}}$x${{</math>}} e {{<math>}}$y${{</math>}}:
    - Geraremos valores aleat√≥rios {{<math>}}$x \sim U(1, 9)${{</math>}}. Isso √© apenas para facilitar, n√£o importa a distribui√ß√£o de {{<math>}}$x${{</math>}}. 

```r
b0til = 50
b1til = -5
N = 500 # N√∫mero de observa√ß√µes

set.seed(1)
e_til = rnorm(N, 0, 2) # Desvios: 500 obs. de m√©dia 0 e desv pad 2
x = runif(N, 1, 9) # Gerando 500 obs. de x
y = b0til + b1til*x + e_til # calculando observa√ß√µes y

plot(x, y)
```

<img src="/project/rec5004/sec6/_index_files/figure-html/unnamed-chunk-15-1.png" width="672" />
    
  - Simulamos as observa√ß√µes {{<math>}}$x${{</math>}} e {{<math>}}$y${{</math>}} que s√£o, na pr√°tica, as informa√ß√µes que observamos nas bases de dados.

2. Estimaremos, por MQO, os par√¢metros {{<math>}}$\hat{\beta}_0${{</math>}} e {{<math>}}$\hat{\beta}_1${{</math>}} a partir das observa√ß√µes simuladas de {{<math>}}$y${{</math>}} e {{<math>}}$x${{</math>}}:
    - Um pesquisador sup√¥s a rela√ß√£o entre as vari√°veis pelo seguinte _modelo emp√≠rico_:
    $$ y = \beta_0 + \beta_1 x + \varepsilon, \tag{1a}$$
    assumindo que {{<math>}}$E[\varepsilon] = 0${{</math>}} e {{<math>}}$cov(\varepsilon, x)= 0=0${{</math>}}.
    - Para estimar o modelo por MQO, usamos a fun√ß√£o `lm()`
    

```r
lm(y ~ x) # regredindo por MQO a var. dependente y pela var. x
```

```
## 
## Call:
## lm(formula = y ~ x)
## 
## Coefficients:
## (Intercept)            x  
##      49.794       -4.948
```

- Note que foi poss√≠vel recuperar os par√¢metros reais:
  - {{<math>}}$\hat{\beta}_0 = 49,794 \approx 50 = \tilde{\beta}_0${{</math>}} e
  - {{<math>}}$\hat{\beta}_1 = -4,948 \approx -5 = \tilde{\beta}_1${{</math>}}.


```r
plot(x, y) # Figura de x contra y
abline(a=50, b=-5, col="red") # reta do modelo real
abline(lm(y ~ x), col="blue") # reta estimada a partir das observa√ß√µes
```

<img src="/project/rec5004/sec6/_index_files/figure-html/unnamed-chunk-17-1.png" width="672" />

### Sem viola√ß√£o de hip√≥tese: Exemplo 2
- Agora, no _modelo real_, suponha que o n√∫mero de queimaduras {{<math>}}$y${{</math>}} √© determinado tanto pela quantidade de horas de aprendizado {{<math>}}$x${{</math>}} e pela quantidade de horas gastas cozinhando {{<math>}}$z${{</math>}}:

$$ y = \tilde{\beta}_0 + \tilde{\beta}_1 x + \tilde{\beta}_2 z + \tilde{\varepsilon}, \qquad \tilde{\varepsilon} \sim N(0, 2^2) \tag{2} $$
em que {{<math>}}$\beta_0=50${{</math>}}, {{<math>}}$\beta_1=-5${{</math>}} e {{<math>}}$\beta_2=3${{</math>}}. Apenas para facilitar, usaremos geraremos valores aleat√≥rios de {{<math>}}$x \sim U(1, 9)${{</math>}} e {{<math>}}$z \sim U(11, 15)${{</math>}}. Note que {{<math>}}$z${{</math>}}, por constru√ß√£o, **n√£o** √© correlacionada com {{<math>}}$x${{</math>}} no _modelo real_.

- Primeiro, vamos simular as observa√ß√µes:

```r
b0til = 50
b1til = -5
b2til = 3
N = 500 # N√∫mero de observa√ß√µes

set.seed(1)
e_til = rnorm(N, 0, 2) # Desvios: 500 obs. de m√©dia 0 e desv pad 2
x = runif(N, 1, 9) # Gerando 500 obs. de x
z = runif(N, 11, 15) # Gerando 500 obs. de y
y = b0til + b1til*x + b2til*z + e_til # calculando observa√ß√µes y
```

- Considere que um pesquisador suponha a rela√ß√£o entre as vari√°veis pelo seguinte _modelo emp√≠rico_:
    $$ y = \beta_0 + \beta_1 x + \varepsilon, \tag{2a}$$
    assumindo que {{<math>}}$E[\varepsilon] = 0${{</math>}} e {{<math>}}$cov(\varepsilon, x)= 0${{</math>}}.

- Note que o pesquisador deixou a vari√°vel de horas cozinhando {{<math>}}$z${{</math>}} fora do modelo, ent√£o ela acaba "entrando" no erro da estima√ß√£o.
- No entanto, como {{<math>}}$z${{</math>}} n√£o tem rela√ß√£o com {{<math>}}$x${{</math>}}, ent√£o isso n√£o afeta a estimativa de {{<math>}}$\hat{\beta}_1${{</math>}}:

```r
cor(x, z) # correla√ß√£o de x e z -> pr√≥xima de 0
```

```
## [1] 0.022555
```

```r
lm(y ~ x) # estima√ß√£o por MQO
```

```
## 
## Call:
## lm(formula = y ~ x)
## 
## Coefficients:
## (Intercept)            x  
##      88.628       -4.913
```
- Note que {{<math>}}$\hat{\beta}_1 = -4,913 \approx -5 = \tilde{\beta}_1${{</math>}}, portanto a estima√ß√£o por MQO conseguiu recuperar o par√¢metro real, apesar do pesquisador n√£o ter inclu√≠do {{<math>}}$z${{</math>}} no modelo.
- Grande parte dos estudos econ√¥micos tentam estabelecer a rela√ß√£o/causalidade entre {{<math>}}$y${{</math>}} e alguma vari√°vel de interesse {{<math>}}$x${{</math>}}, ent√£o n√£o √© necess√°rio incluir todas poss√≠veis vari√°veis que impactam {{<math>}}$y${{</math>}}, desde que {{<math>}}$cov(\varepsilon, x) = 0${{</math>}}. Ou seja, que nenhuma vari√°vel explicativa correlacionada com {{<math>}}$x${{</math>}} tenha ``ficado de fora'' e, portanto, compondo o termo de erro.



### Viola√ß√£o de cov(e,x) = 0
- Agora, suponha que, no _modelo real_, quanto mais horas a pessoa pratica culin√°ria, mais ele cozinha (ou seja, {{<math>}}$x${{</math>}} est√° relacionada com {{<math>}}$z${{</math>}}).
    - Considere que {{<math>}}$z = 2,5x + \varepsilon, \quad \varepsilon \sim N(0; (0,25)^2)${{</math>}}:
    

```r
set.seed(1)
e_til = rnorm(N, 0, 2) # Desvios: 500 obs. de m√©dia 0 e desv pad 2
x = runif(N, 1, 9) # Gerando 500 obs. de x
z = 2.5*x + rnorm(N, 0, 0.25) # Gerando 500 obs. de z
y = b0til + b1til*x + b2til*z + e_til # calculando observa√ß√µes y
cor(x, z) # correla√ß√£o de x e z
```

```
## [1] 0.9990112
```

- Note que, agora, {{<math>}}$x${{</math>}} e {{<math>}}$z${{</math>}} s√£o consideravalmente correlacionados
- Vamos estimar o _modelo emp√≠rico_:
    $$ y = \beta_0 + \beta_1 x + \varepsilon,$$
    assumindo que {{<math>}}$E[\varepsilon] = 0${{</math>}} e {{<math>}}$cov(\varepsilon, x)= 0${{</math>}}.
    

```r
lm(y ~ x) # estima√ß√£o por MQO
```

```
## 
## Call:
## lm(formula = y ~ x)
## 
## Coefficients:
## (Intercept)            x  
##      49.709        2.566
```

- Observe que {{<math>}}$\hat{\beta}_1 = 2,56 \neq -5 = \tilde{\beta}_1${{</math>}}. Isto se d√° porque {{<math>}}$z${{</math>}} n√£o foi inclu√≠do no modelo e, portanto, ele acaba compondo o res√≠duo {{<math>}}$\hat{\varepsilon}${{</math>}}. Como {{<math>}}$z${{</math>}} √© correlacionado com {{<math>}}$x${{</math>}}, ent√£o {{<math>}}$cov(\varepsilon, x)\neq 0${{</math>}} (violando a hip√≥tese do MQO).
- Observe que, se inclu√≠ssemos a vari√°vel {{<math>}}$z${{</math>}} na estima√ß√£o, conseguir√≠amos recuperar {{<math>}}$\hat{\beta}_1 \approx \tilde{\beta}_1${{</math>}}:


```r
lm(y ~ x + z)
```

```
## 
## Call:
## lm(formula = y ~ x + z)
## 
## Coefficients:
## (Intercept)            x            z  
##      49.798       -5.235        3.114
```

### Viola√ß√£o de E(e) = 0, por√©m constante
- Agora, consideraremos que {{<math>}}$E[\varepsilon] = k${{</math>}}, sendo {{<math>}}$k \neq 0${{</math>}} uma constante.
- Assuma que {{<math>}}$k = 10${{</math>}}:

```r
b0til = 50
b1til = -5
k = 10

set.seed(1)
e_til = rnorm(N, k, 2) # Desvios: 500 obs. de m√©dia k e desv pad 2
x = runif(N, 1, 9) # Gerando 500 obs. de x
y = b0til + b1til*x + e_til # calculando observa√ß√µes y
```
- Caso um pesquisador assuma {{<math>}}$E[\varepsilon] = 0${{</math>}}, segue que:

```r
lm(y ~ x) # estima√ß√£o por MQO
```

```
## 
## Call:
## lm(formula = y ~ x)
## 
## Coefficients:
## (Intercept)            x  
##      59.794       -4.948
```
- Note que o fato de {{<math>}}$E[\varepsilon] \neq 0${{</math>}} afeta apenas a estima√ß√£o de {{<math>}}$\hat{\beta}_0 \neq \tilde{\beta}_0${{</math>}}, por√©m n√£o afeta a de {{<math>}}$\hat{\beta}_1 \approx \tilde{\beta}_1${{</math>}}, que √© normalmente o par√¢metro de interesse em estudos econ√¥micos.


### Viola√ß√£o de var(e|x) = constante (homocedasticidade)
- Agora, consideraremos que {{<math>}}$\varepsilon \sim N(0, (5x)^2)${{</math>}}, ou seja, a vari√¢ncia cresce com {{<math>}}$x${{</math>}} -- {{<math>}}$var(\varepsilon|x) \neq ${{</math>}} constante (n√£o vale homocedasticidade).


```r
b0til = 50
b1til = -5
N = 500

x = runif(N, 1, 9) # Gerando 500 obs. de x
e_til = rnorm(N, 0, 5*x) # Desvios: 500 obs. de m√©dia 0 e desv pad 5x
y = b0til + b1til*x + e_til # calculando observa√ß√µes y

plot(x, y) # visualizando heteroscedasticidade
```

<img src="/project/rec5004/sec6/_index_files/figure-html/unnamed-chunk-25-1.png" width="672" />

```r
lm(y ~ x) # estima√ß√£o por MQO
```

```
## 
## Call:
## lm(formula = y ~ x)
## 
## Coefficients:
## (Intercept)            x  
##      52.446       -5.606
```
- Note que, mesmo com heterocesdasticidade, √© poss√≠vel recuperar {{<math>}}$\hat{\beta}_1 \approx \tilde{\beta}_1${{</math>}} quando {{<math>}}$N${{</math>}} for grande (ainda √© consistente). Mas, observe tamb√©m que, se a amostra for pequena, mais prov√°vel √© que {{<math>}}$\hat{\beta}_1 \neq \tilde{\beta}_1${{</math>}}. Teste alguns {{<math>}}$N${{</math>}} menores.


</br>



{{< cta cta_text="üëâ Proceed to Optimization" cta_link="../sec7" >}}
