---
date: "2018-09-09T00:00:00Z"
# icon: book
# icon_pack: fas
linktitle: Regress√£o M√∫ltipla
summary: Learn how to use Wowchemy's docs layout for publishing online courses, software
  documentation, and tutorials.
title: Regress√£o M√∫ltipla
weight: 8
output: md_document
type: book
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# wd = "~/../OneDrive/FEA-RP/Disciplinas/REC5004_Econometria-I/Monitoria-FHN/PNADc" # Aspire
# wd = "~/../FEA-RP/Disciplinas/REC5004_Econometria-I/Monitoria-FHN/PNADc" # Nitro
```

## Regress√£o M√∫ltipla na Pr√°tica (via `lm()`)

- [Se√ß√£o 3.1 de Heiss (2020)](http://www.urfie.net/read/index.html#page/115)

- Para estimar um modelo multivariado no R, podemos usar a fun√ß√£o `lm()`:
  - O til (`~`) separa a a vari√°vel dependente das vari√°veis independentes
  - As vari√°veis independentes precisam ser separadas por um `+`
  - A constante ({{<math>}}$\beta_0${{</math>}}) √© inclu√≠da automaticamente pela fun√ß√£o `lm()` -- para retir√°-la, precisa incluir a "vari√°vel independente" `0` na f√≥rmula.


#### Exemplo 3.1: Determinantes da Nota M√©dia em Curso Superior nos EUA (Wooldridge, 2006)
- Sejam as vari√°veis
    - `colGPA` (_college GPA_): a nota m√©dia em um curso superior,
    - `hsGPA` (_high school GPA_): a nota m√©dio do ensino m√©dio, e
    - `ACT` (_achievement test score_): a nota de avalia√ß√£o de conhecimentos para ingresso no ensino superior.
- Usando a base `gpa1` do pacote `wooldridge`, vamos estimar o seguinte modelo:

$$ \text{colGPA} = \beta_0 + \beta_1 \text{hsGPA} + \beta_2 \text{ACT} + u $$

```{r}
# Acessando a base de dados gpa1
data(gpa1, package = "wooldridge")

# Estimando o modelo
GPAres = lm(colGPA ~ hsGPA + ACT, data = gpa1)
GPAres
```

- Podemos obter um resumo mais detalhado dos resultados usando a fun√ß√£o `summary()`:
```{r}
summary(GPAres)
```

- Note que o {{<math>}}R$^2${{</math>}} √© de 0,1764, ou seja, os dois regressores (`hsGPA` e `ACT`) explicam 17,64\% da vari√¢ncia da nota m√©dia em um curso superior (`colGPA`).



## MQO na forma matricial

### Nota√ß√µes

- Para mais detalhes sobre a forma matricial do MQO, ver Ap√™ndice E de Wooldridge (2006)
- Considere o modelo multivariado com {{<math>}}$K${{</math>}} regressores para a observa√ß√£o {{<math>}}$i${{</math>}}:
$$ y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + ... + \beta_K x_{iK} + u_i, \qquad i=1, 2, ..., N \tag{E.1} $$
em que {{<math>}}$N${{</math>}} √© o n√∫mero de observa√ß√µes.

- Defina o vetor-coluna de par√¢metros, {{<math>}}$\boldsymbol{\beta}${{</math>}}, e o vetor-linha de vari√°veis independentes da observa√ß√£o {{<math>}}$i${{</math>}}, {{<math>}}$\boldsymbol{x}_i${{</math>}} (min√∫sculo):
{{<math>}}$$ \underset{1 \times K}{\mathbf{x}_i} = \left[ \begin{matrix} 1 & x_{i1} & x_{i2} & \cdots & x_{iK}  \end{matrix} \right]  \qquad \text{e} \qquad  \underset{(K+1) \times 1}{\boldsymbol{\beta}} = \left[ \begin{matrix} \beta_0 \\ \beta_1 \\ \beta_2 \\ \vdots \\ \beta_K \end{matrix} \right],$${{</math>}}

- Note que o produto interno {{<math>}}$\mathbf{x}_i \boldsymbol{\beta}${{</math>}} √©:

{{<math>}}$$ \underset{1 \times 1}{\mathbf{x}_i \boldsymbol{\beta}} = \left[ \begin{matrix} 1 & x_{i1} & x_{i2} & \cdots & x_{iK}  \end{matrix} \right]  \left[ \begin{matrix} \beta_0 \\ \beta_1 \\ \beta_2 \\ \vdots \\ \beta_K \end{matrix} \right] = 1.\beta_0 + x_{i1} \beta_1  + x_{i2} \beta_2 + \cdots + x_{iK} \beta_K,$${{</math>}}

- Logo, a equa√ß√£o (3.1) pode ser reescrita como

$$ y_i = \underbrace{\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + ... + \beta_K x_{iK}}_{\mathbf{x}_i \boldsymbol{\beta}} + u_i = \mathbf{x}_i \boldsymbol{\beta} + u_i, \qquad i=1, 2, ..., N \tag{E.2} $$

- Considere {{<math>}}$\mathbf{X}${{</math>}} a matriz de todas {{<math>}}$N${{</math>}} observa√ß√µes para as {{<math>}}$K+1${{</math>}} vari√°veis explicativas:

{{<math>}}$$ \underset{N \times (K+1)}{\mathbf{X}} = \left[ \begin{matrix} \mathbf{x}_1 \\ \mathbf{x}_2 \\ \vdots \\ \mathbf{x}_N \end{matrix} \right] = \left[ \begin{matrix} 1 & x_{11} & x_{12} & \cdots & x_{1K}   \\ 1 & x_{21} & x_{22} & \cdots & x_{2K} \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ 1 & x_{N1} & x_{N2} & \cdots & x_{NK} \end{matrix} \right] , $${{</math>}}

- Agora, podemos "empilhar" as equa√ß√µes (3.2) para todo {{<math>}}$i=1, 2, ..., N${{</math>}} e obtemos:

{{<math>}}\begin{align} \mathbf{y} = \mathbf{X} \boldsymbol{\beta} + \mathbf{u} &= \left[ \begin{matrix} 1 & x_{11} & x_{12} & \cdots & x_{1K}   \\ 1 & x_{21} & x_{22} & \cdots & x_{2K} \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ 1 & x_{N1} & x_{N2} & \cdots & x_{NK} \end{matrix} \right] \left[ \begin{matrix} \beta_0 \\ \beta_1 \\ \beta_2 \\ \vdots \\ \beta_K \end{matrix} \right] + \left[ \begin{matrix}u_1 \\ u_2 \\ \vdots \\ u_N \end{matrix} \right] \\
&= \left[ \begin{matrix} \beta_0. 1 + \beta_1 x_{11} + \beta_2 x_{12} + ... + \beta_K x_{1K} + u_1 \\ \beta_0 .1 + \beta_1 x_{21} + \beta_2 x_{22} + ... + \beta_K x_{2K} + u_2 \\ \vdots \\ \beta_0. 1 + \beta_1 x_{N1} + \beta_2 x_{N2} + ... + \beta_K x_{NK} + u_N \end{matrix} \right] = \left[ \begin{matrix}y_1 \\ y_2 \\ \vdots \\ y_N \end{matrix} \right] \tag{E.3} \end{align}{{</math>}}


em que:
{{<math>}}$$  \underset{N\times 1}{\mathbf{y}} = \left[ \begin{matrix} y_1 \\ y_2 \\ \vdots \\ y_N \end{matrix} \right]  \qquad \text{e} \qquad \underset{N\times 1}{\mathbf{u}} = \left[ \begin{matrix}u_1 \\ u_2 \\ \vdots \\ u_N \end{matrix} \right]. $${{</math>}}

- O estimador de MQO {{<math>}}$\hat{\boldsymbol{\beta}}${{</math>}} pode ser obtido pela seguinte f√≥rmula (Wooldridge, 2006):

$$ \hat{\boldsymbol{\beta}} = \left[ \begin{matrix} \hat{\beta}_0 \\ \hat{\beta}_1 \\ \hat{\beta}_2 \\ \vdots \\ \hat{\beta}_K \end{matrix} \right] = (\mathbf{X}'\mathbf{X})^{-1} \mathbf{X}' \mathbf{y} \tag{3.2} $$


### Aplicando no R

- No R, queremos encontrar as estimativas para {{<math>}}$\hat{\boldsymbol{\beta}}${{</math>}}:
  - Transposta...




### Estima√ß√£o Anal√≠tica
- [ResEcon 703](https://github.com/woerman/ResEcon703) - Week 2 (University of Massachusetts Amherst)

#### 1. Construir matrizes de covariadas {{<math>}}$X${{</math>}} e da vari√°vel dependente {{<math>}}$y${{</math>}}
```{r message=FALSE, warning=FALSE}
library(dplyr)

## Criando reg_data a partir de vari√°veis de mtcars + uma coluna de 1's (p/ constante)
reg_data = mtcars %>%
    select(mpg, hp, wt) %>%  # Selecionando as vari√°veis dependente e independentes
    mutate(constante = 1)  # Criando coluna de constante

## Selecionando vari√°veis independentes no objeto X e convertendo em matriz (n x k)
X = reg_data %>%
    select(constante, hp, wt) %>%  # Selecionando as covariadas X (incluindo constante)
    as.matrix()  # Transformando em matrix

## Selecionando a vari√°vel dependente no objeto y e convertendo em matriz (n x 1)
y = reg_data %>%
    select(mpg) %>%  # Selecionando as covariadas X (incluindo constante)
    as.matrix()  # Transformando em matrix

## Visualiza√ß√£o das primeiras linhas de y e X
head(X)
head(y)
```


#### 2. Estimador {{<math>}}$\hat{\beta}${{</math>}}
O estimador de MQO √© dado por:
$$ \hat{\beta} = (X'X)^{-1} X' y $$

```{r}
## Estimando os parametros beta
beta_hat = solve(t(X) %*% X) %*% t(X) %*% y # solve() calcula a inversa
beta_hat
```


#### 3. Calcular os valores ajustados {{<math>}}$\hat{y}${{</math>}}
$$ \hat{y} = X\hat{\beta} $$
```{r}
## Calculando os valores ajustados de y
y_hat = X %*% beta_hat
colnames(y_hat) = "mpg_hat"
head(y_hat)
```


#### 4. Calcular os res√≠duos {{<math>}}$e${{</math>}}
$$ \varepsilon = y - \hat{y} $$
```{r}
## Calculando os residuos
e = y - y_hat
colnames(e) = "e"
head(e)
```


#### 5. Calcular a vari√¢ncia do termo de erro {{<math>}}$\hat{\sigma}^2${{</math>}}
$$ \hat{\sigma}^2 = \frac{\varepsilon'\varepsilon}{n-k} $$
```{r}
## Estimando variancia do termo de erro
sigma2 = t(e) %*% e / (nrow(X) - ncol(X))
sigma2
```


#### 6. Calcular a matriz de covari√¢ncias {{<math>}}$\widehat{cov}(\widehat{\beta})${{</math>}}
$$ \widehat{cov}(\hat{\beta}) = \hat{\sigma}^2 (X'X)^{-1} $$
```{r}
## Estimando a matriz de variancia/covariancia das estimativas beta
vcov_hat = c(sigma2) * solve(t(X) %*% X)
vcov_hat
```


#### 7. Calcular erros padr√£o, estat√≠sticas t, e p-valores
```{r}
## Calculando erros padrao das estimativas beta
std_err = sqrt(diag(vcov_hat)) # Raiz da diagonal da matriz de covari√¢ncias

## Calculando estatisticas t das estimativas beta
t_stat = beta_hat / std_err

## Calculando p-valores das estimativas beta
p_value = 2 * pt(q = -abs(t_stat), df = nrow(X) - ncol(X)) # 2 x acumulada at√© estat√≠stica t negativa

## Organizando os resultados da regressao em uma matriz
results = cbind(beta_hat, std_err, t_stat, p_value)

## Nomeando as colunas da matriz de resultados
colnames(results) = c('Estimate', 'Std. Error', 't stat', 'Pr(>|t|)')
results
```




{{< cta cta_text="üëâ Seguir para Otimiza√ß√£o Num√©rica" cta_link="../sec9" >}}
