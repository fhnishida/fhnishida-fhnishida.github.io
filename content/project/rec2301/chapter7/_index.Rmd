---
date: "2018-09-09T00:00:00Z"
# icon: book
# icon_pack: fas
linktitle: OLS Estimation
summary: Learn how to use Wowchemy's docs layout for publishing online courses, software
  documentation, and tutorials.
title: OLS Estimation
weight: 7
output: md_document
type: book
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

√â necess√°rio carregar o pacote `dplyr` para manipula√ß√£o da base de dados abaixo.
```{r message=FALSE, warning=FALSE}
library(dplyr)
```


## Base de dados `mtcars`

Usaremos dados extra√≠dos da _Motor Trend_ US magazine de 1974, que analisa o
consumo de combust√≠vel e 10 aspectos t√©cnicos de 32 autom√≥veis.

No _R_, a base de dados j√° est√° incorporada ao programa e pode ser acessada pelo
c√≥digo `mtcars`, contendo a seguinte estrutura:

> - _mpg_: milhas por gal√£o
> - _cyl_: n√∫mero de cilindros 
> - _disp_: deslocamento do motor
> - _hp_: cavalos-vapor bruto
> - _drat_: raz√£o eixo traseiro
> - _wt_: peso (1000 libras)
> - _qsec_: tempo de 1/4 de milha
> - _vs_: motor (0 = forma de V, 1 = reto)
> - _am_: transmiss√£o (0 = autom√°tico, 1 = manual)
> - _gear_: n√∫mero de marchas


Fa√ßamos um resumo da base de dados:

```{r}
### Examinaremos a base da dados mtcars
## Estrutura de mtcars
str(mtcars)

## Selecionando 3 vari√°veis e resumindo 
mtcars %>% 
  select(mpg, hp, wt) %>% 
  summary()

## Plotando consumo de combust√≠vel (mpg) por pot√™ncia do carro (hp)
plot(mtcars$mpg, mtcars$hp, xlab="Milhas por gal√£o (mpg)", ylab="Cavalos-vapor (hp)")

## Plotando consumo de combust√≠vel (mpg) por peso do carro (wt)
plot(mtcars$mpg, mtcars$wt, xlab="Milhas por gal√£o (mpg)", ylab="Libras (wt)")
```

Queremos estimar o seguinte modelo:
{{<math>}}$$ \text{mpg} = \beta_0 + \beta_1 \text{hp} + \beta_2 \text{wt} + \varepsilon  $${{</math>}}


## Estima√ß√£o por OLS

### Usando a fun√ß√£o `lm()`

```
lm(formula, data, subset, weights, na.action,
   method = "qr", model = TRUE, x = FALSE, y = FALSE, qr = TRUE,
   singular.ok = TRUE, contrasts = NULL, offset, ...)
   
formula: an object of class "formula" (or one that can be coerced to that class): a symbolic description of the model to be fitted.
data: an optional data frame, list or environment (or object coercible by as.data.frame to a data frame) containing the variables in the model.
weights: an optional vector of weights to be used in the fitting process. Should be NULL or a numeric vector.
```
- Em `formula` voc√™ ir√° inserir a vari√°vel dependente separada por `~` das vari√°veis independentes. Estas ser√£o separadas por um `+`. Neste exemplo: `formula = mpg ~ hp + wt`
  - Para incluir uma intera√ß√£o entre vari√°veis independentes, usa-se `<var1>:<var2>`. Por exemplo: `formula = mpg ~ hp + wt + hp:am`
  - Para fazer uma altera√ßao em alguma vari√°vel dentro da fun√ß√£o `lm()`, usa-se `I()`. Por exemplo, para incluir tamb√©m o peso do carro, _wt_, ao quadrado: `formula = mpg ~ hp + wt + I(wt^2)`
- Em `data`, ser√° atribu√≠da uma base de dados e, em `weights`, √© poss√≠vel atribuir pondera√ß√µes para cada observa√ß√£o (bastante comum em bases providas pelo IBGE, como PNAD e POF)

Vamos regredir o consumo de combust√≠vel (_mpg_) pela pot√™ncia (_hp_) e pelo peso (_wt_) do carro:
```{r}
## Rodar a regressao OLS
fit_ols1 = lm(formula = mpg ~ hp + wt, data = mtcars)

## Resumir os resultados da regressao
summary(fit_ols1)
```

- √â poss√≠vel extrair informa√ß√µes da regress√£o:
```{r}
names(fit_ols1) # verificar todas informa√ß√µes contidas no objeto da regress√£o
fit_ols1$coefficients # Estimativas
summary(fit_ols1)$coefficients # Estimativas pelo summary()
head(fit_ols1$fitted.values) # valores ajustados
head(fit_ols1$residuals) # res√≠duos
```

#### Vari√°veis categ√≥ricas
- Ao inserir **vari√°veis categ√≥ricas** na regress√£o, criam-se _dummies_ para cada categoria e retira-se uma delas para evitar problema de multicolinearidade:
- No exemplo abaixo, transformaremos a vari√°vel de cilindros (_cyl_), que possui apenas 3 valores (4, 6, e 8), na classe _factor_.
```{r}
mtcars_factor1 = mtcars %>% mutate(cyl = factor(cyl))
summary( lm(mpg ~ cyl, data=mtcars_factor1) )$coef
```
- Note que criou apenas 2 _dummies_ de cilindros (_cyl6_ e _cyl8_), sendo que as suas estimativas s√£o negativas e significativas em rela√ß√£o ao carro com 4 cilindros (_cyl4_) que √© a caregoria omitida para evitar multicolinearidade.
- Podemos tamb√©m definir a categoria a ser omitida usando as fun√ß√µes:
  - `factor(..., levels=c(...))`, quando transforma um vetor em factor (omite o 1¬∫)
  - `relevel(..., ref="")`, quando o vetor j√° √© da classe factor e quer apenas alterar os n√≠veis
- Para omitir _cyl8_ fazemos:
```{r}
# Via relevel() - via factor j√° existente
mtcars_factor1$cyl = relevel(mtcars_factor1$cyl, ref="8")
summary( lm(mpg ~ cyl, data=mtcars_factor1) )$coef

# Via factor() - omite o 1¬∫ n√≠vel
mtcars_factor2 = mtcars %>% mutate(cyl = factor(cyl, levels = c(8, 4, 6)))
summary( lm(mpg ~ cyl, data=mtcars_factor2) )$coef
```
- Observe que as estimativas est√£o positivas agora, dado que a refer√™ncia agora √© _cyl8_

#### Exportando output em LaTeX
- Podemos usar os pacotes `stargazer` e `textreg` para gerar o output da regress√£o em LaTeX
- As fun√ß√µes t√™m os mesmos nomes dos pacotes:
```r
stargazer::stargazer(fit_ols1)
texreg::texreg(fit_ols1)
```
- Depois, copie o output e jogue num programa de LaTeX.


### Estima√ß√£o Anal√≠tica
- [ResEcon 703](https://github.com/woerman/ResEcon703) - Week 2 (University of Massachusetts Amherst)

#### 1. Construir matrizes de covariadas $X$ e da vari√°vel dependente $y$
```{r}
## Criando reg_data a partir de vari√°veis de mtcars + uma coluna de 1's (p/ constante)
reg_data = mtcars %>% 
  select(mpg, hp, wt) %>%  # Selecionando as vari√°veis dependente e independentes
  mutate(constante = 1)  # Criando coluna de constante

## Selecionando vari√°veis independentes no objeto X e convertendo em matriz (n x k)
X = reg_data %>% 
  select(constante, hp, wt) %>%  # Selecionando as covariadas X (incluindo constante)
  as.matrix()  # Transformando em matrix

## Selecionando a vari√°vel dependente no objeto y e convertendo em matriz (n x 1)
y = reg_data %>% 
  select(mpg) %>%  # Selecionando as covariadas X (incluindo constante)
  as.matrix()  # Transformando em matrix

## Visualiza√ß√£o das primeiras linhas de y e X
head(X)
head(y)
```


#### 2. Estimador $\hat{\beta}$
O estimador de OLS √© dado por:
{{<math>}}$$ \hat{\beta} = (X'X)^{-1} X' y $${{</math>}}

```{r}
## Estimando os parametros beta
beta_hat = solve(t(X) %*% X) %*% t(X) %*% y # solve() calcula a inversa
beta_hat
```


#### 3. Calcular os valores ajustados $\hat{y}$
{{<math>}}$$ \hat{y} = X\hat{\beta} $${{</math>}}
```{r}
## Calculando os valores ajustados de y
y_hat = X %*% beta_hat
colnames(y_hat) = "mpg_hat"
head(y_hat)
```


#### 4. Calcular os res√≠duos $e$
{{<math>}}$$ \varepsilon = y - \hat{y} $${{</math>}}
```{r}
## Calculando os residuos
e = y - y_hat
colnames(e) = "e"
head(e)
```


#### 5. Calcular a vari√¢ncia do termo de erro $s^2$
{{<math>}}$$ \hat{\sigma}^2 = \frac{e'e}{n-k} $${{</math>}}
```{r}
## Estimando variancia do termo de erro
sigma2 = t(e) %*% e / (nrow(X) - ncol(X))
sigma2
```


#### 6. Calcular a matriz de covari√¢ncias $\hat{Cov}(\widehat{\beta})$
{{<math>}}$$ \widehat{Cov}(\hat{\beta}) = \hat{\sigma}^2 (X'X)^{-1} $${{</math>}}
```{r}
## Estimando a matriz de variancia/covariancia das estimativas beta
vcov_hat = c(sigma2) * solve(t(X) %*% X)
vcov_hat
```


#### 7. Calcular erros padr√£o, estat√≠sticas t, e p-valores
```{r}
## Calculando erros padrao das estimativas beta
std_err = sqrt(diag(vcov_hat)) # Raiz da diagonal da matriz de covari√¢ncias

## Calculando estatisticas t das estimativas beta
t_stat = beta_hat / std_err

## Calculando p-valores das estimativas beta
p_value = 2 * pt(q = -abs(t_stat), df = nrow(X) - ncol(X)) # 2 x acumulada at√© estat√≠stica t negativa

## Organizando os resultados da regressao em uma matriz
results = cbind(beta_hat, std_err, t_stat, p_value)

## Nomeando as colunas da matriz de resultados
colnames(results) = c('Estimate', 'Std. Error', 't stat', 'Pr(>|t|)')
results
```


### Estima√ß√£o Num√©rica

#### 1. Criar fun√ß√£o perda que calcula a soma dos desvios quadr√°ticos
- A fun√ß√£o para calcular a soma dos desvios quadr√°ticos recebe como inputs:
  - um **vetor** de poss√≠veis valores para {{<math>}}$\beta_0${{</math>}}, {{<math>}}$\beta_1${{</math>}} e {{<math>}}$\beta_2${{</math>}}
  - uma base de dados
```{r}
desv_quad = function(params, data) {
  # Extraindo os par√¢metros para objetos
  beta_0 = params[1]
  beta_1 = params[2]
  beta_2 = params[3]
  
  mpg_ajustado = beta_0 + beta_1*data$hp + beta_2*data$wt # valores ajustados
  desvios = data$mpg - mpg_ajustado # desvios = observados - ajustados
  sum(desvios^2)
}
```


#### 2. Otimiza√ß√£o
- Agora encontraremos os par√¢metros que minimizam a fun√ß√£o perda
{{<math>}}$$ \text{argmin}_{\theta \in \Theta} \sum_{i=1}^{N}\left( \text{mpg}_i - \widehat{\text{mpg}}_i \right)^2 $${{</math>}}
- Para isto usaremos a fun√ß√£o `optim()` que retorna os par√¢metros que minimizam uma fun√ß√£o (equivalente ao _argmin_):
```yaml
optim(par, fn, gr = NULL, ...,
      method = c("Nelder-Mead", "BFGS", "CG", "L-BFGS-B", "SANN", "Brent"),
      lower = -Inf, upper = Inf,
      control = list(), hessian = FALSE)

par: Initial values for the parameters to be optimized over.
fn: A function to be minimized (or maximized), with first argument the vector of parameters over which minimization is to take place. It should return a scalar result.
method: The method to be used. See ‚ÄòDetails‚Äô. Can be abbreviated.
hessian: Logical. Should a numerically differentiated Hessian matrix be returned?
```
- Colocaremos como input:
  - a fun√ß√£o perda criada `desv_quad()`
  - um chute inicial dos par√¢metros
    - Note que a estima√ß√£o pode ser mais ou menos sens√≠vel ao valores iniciais, dependendo do m√©todo de otimiza√ß√£o utilizado
    - O mais comum √© encontrar como chute inicial um vetor de zeros `c(0, 0, 0)`, por ser mais neutro em rela√ß√£o ao sinal das estimativas
    - Em Econometria III, prof. Laurini recomendou usar m√©todo "Nelder-Mead" (padr√£o) com um chute inicial de zeros e, depois, usar suas estimativas como chute inicial para o m√©todo "BFGS".
  - Por padr√£o, temos o argumento `hessian = FALSE`, coloque `TRUE` para calcularmos o erro padr√£o, estat√≠stica t e p-valor das estimativas

```{r}
# Estima√ß√£o por BFGS
theta_ini = c(0, 0, 0) # Chute inicial de beta_0, beta_1 e beta_2

fit_ols2 = optim(par=theta_ini, fn=desv_quad, data=mtcars,
                  method="BFGS", hessian=TRUE)
fit_ols2
```

## M√©todos de Otimiza√ß√£o B√°sicos
- Essa se√ß√£o tem o objetivo para dar uma intui√ß√£o sobre m√©todos de otimiza√ß√£o e est√° baseada em Hamilton (1994), se√ß√£o 5.7.
- Veremos os m√©todos de _grid search_ (discretiza√ß√£o) e _steepest ascent_ que s√£o m√©todos simples, mas que representam fam√≠lias de m√©todos de otimiza√ß√£o.
- Para maior profundidade no assunto, sugiro cursar disciplina de Economia Computacional.

### _Grid Search_

- O m√©todo mais simples de otimiza√ß√£o num√©rica √© o _grid search_ (discretiza√ß√£o).
- Como o R n√£o lida com problemas com infinitos valores, uma forma lidar com isso √© discretizando diversos poss√≠veis valores dos par√¢metros de escolha dentro de intervalos.
- Para cada poss√≠vel combina√ß√£o de par√¢metros, calculam-se diversos valores a partir da fun√ß√£o objetivo. De todos os valores calculados, escolhe-se a combina√ß√£o de par√¢metros que maximizam (ou minimizam) a fun√ß√£o objetivo.
- O exemplo abaixo considera apenas um par√¢metro de escolha {{<math>}}$\theta${{</math>}} e, para cada ponto escolhido dentro do intervalo {{<math>}}$[-1, 1]${{</math>}}, calcula-se a fun√ß√£o objetivo:

<center><img src="../grid_search.png"></center>

- Este √© um m√©todo robusto a fun√ß√µes com descontinuidades e quinas (n√£o diferenci√°veis), e menos sens√≠vel a chutes de valores iniciais. (ver m√©todo abaixo)
- Por√©m, por ter que fazer c√°lculo da fun√ß√£o objetivo para in√∫meros pontos, tende a ser menos eficiente.


### _Steepest Ascent_

- Conforme o n√∫mero de par√¢metros do modelo cresce, aumenta o n√∫mero de poss√≠veis combina√ß√µes entre par√¢metros e torna o processo computacional cada vez mais lento.
- Uma forma mais eficiente de encontrar o conjunto de par√¢metros que otimizam a fun√ß√£o objetivo √© por meio do m√©todo _steepest ascent_.
- Seja {{<math>}}$\theta^*${{</math>}} o conjunto de par√¢metros que maximiza a fun√ß√£o objetivo:
  1. Comece com alguns valores iniciais dos par√¢metros, {{<math>}}$\theta^0${{</math>}}
  2. Calcula-se o gradiente e avalia-se a possibilidade de "andar para cima" a um valor mais alto
  3. Caso possa, ande na dire√ß√£o correta a {{<math>}}$\theta_1${{</math>}}
  4. Repita os passos (2) e (3), andando de {{<math>}}$\theta^s${{</math>}} para {{<math>}}$\theta^{s+1}${{</math>}} at√©
  atingir o m√°ximo com {{<math>}}$\theta^*${{</math>}}.

<center><img src="../steepest_ascent.png"></center>

- Note que esse m√©todo de otimiza√ß√£o √© sens√≠vel ao conjunto de par√¢metros iniciais e a descontinuidades da fun√ß√£o objetivo.
- Por outro lado, √© um m√©todo mais eficiente (calcula uma fun√ß√£o objetivo dado os par√¢metros a cada passo que d√°) e tende a ser tamb√©m mais preciso nas estima√ß√µes.



## Estima√ß√£o por MLE
- [ResEcon 703](https://github.com/woerman/ResEcon703) - Week 6 (University of Massachusetts Amherst)


### Intui√ß√£o do c√°lculo da fun√ß√£o de verossimilhan√ßa
- Apenas para ilustrar a constru√ß√£o da fun√ß√£o de verossimilhan√ßa, considere um modelo logit em que queremos estimar os indiv√≠duos precisam escolher se usam carro ou √¥nibus para deslocamento.
- Para estimar as probabilidades de usar carro (e, por consequ√™ncia, de √¥nibus), vamos utilizar as informa√ß√µes dos pre√ßos que os indiv√≠duos pagam pela gasolina e pela passagem de √¥nibus.
- Note que os valores abaixo foram todos inventados.
- Considere um conjunto de par√¢metros {{<math>}}$\theta^A = \{ \beta^A_0, \beta^A_1, \beta^A_2 \}${{</math>}} que gerem as seguintes probabilidades de usar carro e de √¥nibus (√∫ltimas 2 colunas):

|             | **Pre√ßo Gasolina** | **Pre√ßo Bus** | **Escolha** | **_Prob(Car  $| \theta^A$)_** | **_Prob(Bus $| \theta^A$)_** |
|:-----------:|:------------------:|:----------------:|:-----------:|:-----------------:|:------------------:|
| Indiv. 1 |        6,93        |       5,00       |    Car    |        **0,63**       |        0,37        |
| Indiv. 2 |        7,01        |       2,50       |    Bus   |        0,33       |        **0,67**        |
| Indiv. 3 |        6,80        |       2,50       |    Bus   |        0,25       |        **0,75**        |
| Indiv. 4 |        6,75        |       5,00       |    Car    |        **0,73**       |        0,27        |

- Logo, a verossimilhan√ßa, dado os par√¢metros {{<math>}}$\theta^A${{</math>}} √©
{{<math>}}$$ \mathcal{L}(\theta^A) = 0,63 \times 0,67 \times 0,75 \times 0,73 = 0,231 $${{</math>}}

- Agora, considere $\theta^B = \{ \beta^B_0, \beta^B_1, \beta^B_2 \}$ que gerem as seguintes probabilidades:

|             | **Pre√ßo Gasolina** | **Pre√ßo Bus** | **Escolha** | **_Prob(Car  $| \theta^B$)_** | **_Prob(Bus $| \theta^B$)_** |
|:-----------:|:------------------:|:----------------:|:-----------:|:-----------------:|:------------------:|
| Indiv. 1 |        6,93        |       5,00       |    Car    |        **0,54**       |        0,46        |
| Indiv. 2 |        7,01        |       2,50       |    Bus   |        0,43       |        **0,57**        |
| Indiv. 3 |        6,80        |       2,50       |    Bus   |        0,35       |        **0,65**        |
| Indiv. 4 |        6,75        |       5,00       |    Car    |        **0,58**       |        0,42        |

- Ent√£o, a verossimilhan√ßa, dado {{<math>}}$\theta^B${{</math>}}, √©

{{<math>}}$$ \mathcal{L}(\theta^B) = 0,54 \times 0,57 \times 0,65 \times 0,58 = 0,116 $${{</math>}}

- Como {{<math>}}$\mathcal{L}(\theta^A) = 0,231 > 0,116 = \mathcal{L}(\theta^B)${{</math>}}, ent√£o os par√¢metros {{<math>}}$\theta^A${{</math>}} se mostram mais adequados em rela√ß√£o a {{<math>}}$\theta^B${{</math>}}
- Na m√°xima verossimilhan√ßa (MLE), √© escolhido o conjunto de par√¢metros {{<math>}}$\theta^*${{</math>}} que maximiza a fun√ß√£o de verossimilhan√ßa (ou log-verossimilhan√ßa).
- No modelo logit, as probabilidades usadas para calcular a verossimilhan√ßa s√£o as pr√≥prias proabilidades de escolha por uma alternativa, dado um conjunto de par√¢metros.
- J√° no modelo linear, usamos a fun√ß√£o de densidade de probabilidade para avaliar a "dist√¢ncia" de cada observa√ß√£o, {{<math>}}$y_i${{</math>}}, em rela√ß√£o ao seu valor ajustado {{<math>}}$\hat y_i${{</math>}}, dado um conjunto de par√¢metros.



### Otimiza√ß√£o Num√©rica para MLE
A fun√ß√£o `optim()` do R ser√° usada novamente para desempenhar a otimiza√ß√£o num√©rica. Precisamos usar como input:

- Alguns valores inicias dos par√¢metros, {{<math>}}$\theta^0${{</math>}}
- Uma fun√ß√£o que tome esses par√¢metros como um argumento e calcule a 
log-verossimilhan√ßa, {{<math>}}$\ln{L(\theta)}${{</math>}}.

A fun√ß√£o log-verossimilhan√ßa √© dada por
{{<math>}}$$ \ln{L(\beta, \sigma^2 | y, X)} = \sum^n_{i=1}{\ln{f(y_i | x_i, \beta, \sigma^2)}}, $${{</math>}}
em que a distribui√ß√£o condicional de cada {{<math>}}$y_i${{</math>}} √©
{{<math>}}$$ y_i | x_i \sim \mathcal{N}(\beta'x_i, \sigma^2) $${{</math>}}

1. Construir matriz {{<math>}}$X${{</math>}} e vetor {{<math>}}$y${{</math>}}
2. Calcular os valores ajustados de {{<math>}}$y${{</math>}}, {{<math>}}$\hat{y} - \beta'x_i${{</math>}}, que √© a m√©dia de cada {{<math>}}$y_i${{</math>}}
3. Calcular a densidade para cada {{<math>}}$y_i${{</math>}}, {{<math>}}$f(y_i | x_i, \beta, \sigma^2)${{</math>}}
4. Calcular a log-verossimilhan√ßa, {{<math>}}$\ln{L(\beta, \sigma^2 | y, X)} = \sum^n_{i=1}{\ln{f(y_i | x_i, \beta, \sigma^2)}}${{</math>}}


#### 1. Chute de valores iniciais para $\beta_0, \beta_1, \beta_2$ e $\sigma^2$
- Note que, diferente da estima√ß√£o por OLS, um dos par√¢metros a ser estimado via MLE √© a vari√¢ncia ({{<math>}}$\sigma^2${{</math>}}).
```{r}
params = c(35, -0.02, -3.5, 1)
# (beta_0, beta_1 , beta_2, sigma2)
```

#### 2. Sele√ß√£o da base de dados e vari√°veis
```{r}
## Adicionando colunas de 1's para o termo constante
data = mtcars
beta_0 = params[1]
beta_1 = params[2]
beta_2 = params[3]
sigma2 = params[4]
```

#### 3. C√°lculo dos valores ajustados e das densidades
```{r}
## Calculando valores ajustados de y
y_hat = beta_0 + beta_1*data$hp + beta_2*data$wt
```

#### 4. C√°lculo das densidades
{{<math>}}$$ f(y_i | x_i, \beta, \sigma^2) $${{</math>}}
```{r}
## Calculando os pdf's de cada outcome
y_pdf = dnorm(data$mpg, mean = y_hat, sd = sqrt(sigma2))

head(y_pdf) # Primeiros valores da densidade
prod(y_pdf) # Verossimilhan√ßa
```

- Para entender melhor o que estamos fazendo aqui, relembre que, na estima√ß√£o por m√°xima verossimilhan√ßa, assume-se que
{{<math>}}$$\varepsilon | X \sim N(0, \sigma^2)$${{</math>}}
- No exemplo abaixo, podemos ver que, para cada {{<math>}}$x${{</math>}}, temos um valor ajustado {{<math>}}$\hat{y} = \beta_0 + \beta_1 x${{</math>}} e seus desvios {{<math>}}$\varepsilon${{</math>}} s√£o normalmente distribu√≠dos com a mesma vari√¢ncia {{<math>}}$\sigma^2${{</math>}}

<center><img src="../mle.jpg"></center>

- Agora, vamos juntar o data frame `mtcars` com os valores ajustados `mpg_hat` e as densidades `y_pdf`:
```{r}
mpg_hat = y_hat # atribuindo y_hat a um objeto com nome mais adequado

# Juntando as bases e visualizando os primeiros valores
bd_joined = cbind(mtcars, mpg_hat, y_pdf) %>%
  select(hp, wt, mpg, mpg_hat, y_pdf)
head(bd_joined)
```
- Como pode ser visto na base de dados juntada e nos gr√°ficos abaixo, quanto mais pr√≥ximo o valor ajustado for do valor observado de cada observa√ß√£o, maior ser√° a densidade/probabilidade.
```{r}
# Criando gr√°fico para os 2 primeiros carros (Mazda RX4 e Mazda RX 4 Wag)
qt_norm = seq(20, 27, by=0.1) # valores de mpg ("escores Z")

# Mazda RX4
pdf_norm1 = dnorm(qt_norm, mean=bd_joined$mpg_hat[1], sd=sqrt(sigma2)) # pdf
plot(qt_norm, pdf_norm1, type="l", xlab="mpg", ylab="densidade", main="Mazda RX4")
abline(v=c(bd_joined$mpg_hat[1], bd_joined$mpg[1]), col="red")
text(c(bd_joined$mpg_hat[1], bd_joined$mpg[1]), 0.2, 
     c(expression(widehat(mpg)[1]), expression(mpg[1])), 
     pos=2, srt=90, col="red")

# Mazda RX4 Wag 
pdf_norm2 = dnorm(qt_norm, mean=bd_joined$mpg_hat[2], sd=sqrt(sigma2)) # pdf
plot(qt_norm, pdf_norm2, type="l", xlab="mpg", ylab="densidade", main="Mazda RX4 Wag")
abline(v=c(bd_joined$mpg_hat[2], bd_joined$mpg[2]), col="blue")
text(c(bd_joined$mpg_hat[2], bd_joined$mpg[2]), 0.2, 
     c(expression(widehat(mpg)[2]), expression(mpg[2])), 
     pos=2, srt=90, col="blue")
```
- Logo, a verossimilhan√ßa (produto de todas probabilidades) ser√° maior quanto mais pr√≥ximos forem os valores ajustados dos seus respectivos valores observados.


#### 5. Calculando a Log-Verossimilhan√ßa
{{<math>}}$$ \mathcal{l}(\beta, \sigma^2) = \sum^{N}_{i=1}{\ln\left[ f(y_i | x_i, \beta, \sigma^2) \right]} $${{</math>}}
```{r}
## Calculando a log-verossimilhanca
loglik = sum(log(y_pdf))
loglik
```


#### 6. Criando a Fun√ß√£o de Log-Verossimilhan√ßa
```{r}
## Criando funcao para calcular log-verossimilhanca OLS 
loglik_lm = function(params, data) {
  # Pegando os par√¢metros
  beta_0 = params[1]
  beta_1 = params[2]
  beta_2 = params[3]
  sigma2 = params[4]
  
  ## Calculando valores ajustados de y
  y_hat = beta_0 + beta_1*data$hp + beta_2*data$wt
  
  ## Calculando os pdf's de cada outcome
  y_pdf = dnorm(data$mpg, mean = y_hat, sd = sqrt(sigma2))
  
  ## Calculando a log-verossimilhanca
  loglik = sum(log(y_pdf))
  
  ## Retornando o negativo da log-verossimilanca - optim() n√£o maximiza
  -loglik
}
```


#### 7. Otimiza√ß√£o

Tendo a fun√ß√£o objetivo, usaremos `optim()` para *minimizar*
{{<math>}}$$ -\ln{L(\beta, \sigma^2 | y, X)} = -\sum^n_{i=1}{\ln{f(y_i | x_i, \beta, \sigma^2)}}. $${{</math>}}
Aqui, **minimizamos o negativo** da log-Verossimilhan√ßa para **maximizarmos** (fun√ß√£o`optim()` apenas minimiza).

```{r warning=FALSE}
## Maximizando a fun√ß√£o log-verossimilhan√ßa OLS
mle = optim(par = c(0, 0, 0, 1), fn = loglik_lm, data = mtcars,
              method = "BFGS", hessian = TRUE)

## Mostrando os resultados da otimiza√ß√£o
mle

## Calculando os erros padr√£o
mle_se = mle$hessian %>% # hessiano
  solve() %>% # toma a inversa para obter a matriz de var/cov
  diag() %>% # pega a diagonal da matriz
  sqrt() # calcula a raiz quadrada

# Visualizando as estimativas e os erros padr√£o
cbind(mle$par, mle_se)
```



## Estima√ß√£o por GMM
- [Computing Generalized Method of Moments and Generalized Empirical Likelihood with R (Pierre Chauss√©)](https://cran.r-project.org/web/packages/gmm/vignettes/gmm_with_R.pdf)
- [Generalized Method of Moments (GMM) in R - Part 1 (Alfred F. SAM)](https://medium.com/codex/generalized-method-of-moments-gmm-in-r-part-1-of-3-c65f41b6199)


- Para estimar via GMM precisamos construir vetores relacionados aos seguintes momentos:
{{<math>}}$$ E(\varepsilon) = 0 \qquad \text{ e } \qquad E(\varepsilon'X) = 0 $${{</math>}}
em que $X$ √© a matriz de covariadas e $\varepsilon$ √© o desvio. Note que estes s√£o os momentos relacionados ao OLS, dado que este √© um caso particular do GMM.


- Relembre que estamos usando a base de dados `mtcars` para estimar o modelo linear:
{{<math>}}$$ \text{mpg} = \beta_0 + \beta_1 \text{hp} + \beta_2 \text{wt} + \varepsilon $${{</math>}}
que relaciona o consumo de combust√≠vel (em milhas por gal√£o - _mpg_) com a pot√™ncia (_hp_) e o peso (em mil libras - _wt_) do carro.


### Otimiza√ß√£o Num√©rica para GMM

#### 1. Chute de valores iniciais para {{<math>}}$\{ \beta_0, \beta_1, \beta_2 \}${{</math>}}

- Vamos criar um vetor com poss√≠veis valores de {{<math>}}$\{ \beta_0, \beta_1, \beta_2 \}${{</math>}}:

```{r}
library(dplyr)

params = c(35, -0.02, -3.5)
beta_0 = params[1]
beta_1 = params[2]
beta_2 = params[3]
```

#### 2. Sele√ß√£o da base de dados e vari√°veis
```{r}
data = mtcars %>% mutate(constant=1) # Criando vari√°vel de constante

## Selecionando colunas para X (covariadas) e convertendo para matrix
X = data %>% 
  select("constant", "hp", "wt") %>% 
  as.matrix()

## Selecionando variavel para y e convertendo para matrix
y = data %>% 
  select("mpg") %>% 
  as.matrix()
```

#### 3. C√°lculo dos valores ajustados e dos desvios
```{r}
## Valores ajustados e desvios
y_hat = X %*% params
# equivalente a: y_hat = beta_0 + beta_1 * X[,"hp"] + beta_2 * X[,"wt"]

e = y - y_hat
```

#### 4. Cria√ß√£o da matriz de momentos

- Note que {{<math>}}$E(\varepsilon' X)${{</math>}} √© uma multiplica√ß√£o matricial, mas a fun√ß√£o `gmm()` exige que como input os vetores com multiplica√ß√£o elemento a elemento do res√≠duo {{<math>}}$\varepsilon${{</math>}} com as covariadas {{<math>}}$X${{</math>}} (neste caso: constante, hp, wt)

```{r}
m = X * as.vector(e) # matriz de momentos (sem tomar esperan√ßa)
head(m)
```


- Note que, como multiplicamos a constante igual a 1 com os desvios {{<math>}}$\varepsilon${{</math>}}, a 1¬™ coluna corresponde ao momento {{<math>}}$E(\varepsilon)=0${{</math>}} (mas sem tomar a esperan√ßa).

- J√° as colunas 2 e 3 correspodem ao momento {{<math>}}$E(\varepsilon'X)=0${{</math>}} para as vari√°veis _hp_ e _wt_ (tamb√©m sem tomar a esperan√ßa).

- Logicamente, para estimar por GMM, precisamos escolher os par√¢metros {{<math>}}$\theta = \{ \beta_0, \beta_1, \beta_2 \}${{</math>}} que, ao tomar a esperan√ßa em cada um destas colunas, se aproximem ao m√°ximo de zero. Isso ser√° feito via fun√ß√£o `gmm()` (semelhante √† fun√ß√£o `optim()`)



#### 5. Cria√ß√£o de fun√ß√£o com os momentos
- Vamos criar uma fun√ß√£o que tem como input um vetor de par√¢metros (`params`) e uma base de dados (`data`), e que retorna uma matriz em que cada coluna representa um momento.
- Essa fun√ß√£o incluir√° todos os comandos descritos nos itens 1 a 4 (que, na verdade, apenas foram feitos por did√°tica).
```{r}
mom_ols = function(params, data) {
  ## Adicionando colunas de 1's para o termo constante
  data = data %>% mutate(constant = 1)
  
  ## Selecionando colunas para X (covariadas) e convertendo para matrix
  X = data %>% 
    select("constant", "hp", "wt") %>% 
    as.matrix()
  
  ## Selecionando variavel para y e convertendo para matrix
  y = data %>% 
    select("mpg") %>% 
    as.matrix()
  
  ## Valores ajustados e desvios
  y_hat = X %*% params
  e = y - y_hat
  
  m = as.vector(e) * X # matriz de momentos (vetor - multiplica√ß√£o por elemento)
  m
}
```


#### 6. Otimiza√ß√£o via fun√ß√£o `gmm()`
- A fun√ß√£o `gmm()`, assim como a `optim()`, recebe uma fun√ß√£o como argumento.
- No entanto, ao inv√©s de retornar um valor, a fun√ß√£o que entra no `gmm()` retorna uma matriz, cujas m√©dias das colunas queremos aproximar de zero. 
```{r}
library(gmm)

gmm_lm = gmm(mom_ols, mtcars, c(0,0,0),
             wmatrix = "optimal", # matriz de pondera√ß√£o
             optfct = "nlminb" # fun√ß√£o de otimiza√ß√£o
             )

summary(gmm_lm)$coefficients
```




{{< cta cta_text="üëâ Proceed to Panel Data" cta_link="../chapter8" >}}
