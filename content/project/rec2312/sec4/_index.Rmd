---
date: "2018-09-09T00:00:00Z"
# icon: book
# icon_pack: fas
linktitle: Dados em Painel
summary: Learn how to use Wowchemy's docs layout for publishing online courses, software
  documentation, and tutorials.
title: Estima√ß√£o com Dados em Painel
weight: 4
output: md_document
type: book
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# wd = "~/../OneDrive/FEA-RP/Disciplinas/REC5004_Econometria-I/Monitoria-FHN/PNADc" # Aspire
# wd = "~/../FEA-RP/Disciplinas/REC5004_Econometria-I/Monitoria-FHN/PNADc" # Nitro
```

## Nota√ß√µes
- Se√ß√£o 2.1.1 de "Panel Data Econometrics with R" (Croissant \& Millo, 2018)
- A maioria das nota√ß√µes foram adaptadas de acordo com as notas de aula de Econometria I.

Para a observa√ß√£o do indiv√≠duo {{<math>}}$i \in \{1, ..., N\}${{</math>}} no per√≠odo {{<math>}}$t \in \{1, ..., T\}${{</math>}}, podemos escrever o modelo a ser estimado como:

{{<math>}}$$ y_{it} = \boldsymbol{x}'_{it} \boldsymbol{\beta} + \varepsilon_{it} \tag{1} $$ {{</math>}}
em que {{<math>}}$\boldsymbol{\beta}${{</math>}} √© um vetor-coluna de {{<math>}}$K${{</math>}} par√¢metros

{{<math>}}$$ \boldsymbol{\beta} = \left[ \begin{array}{c} \beta_1 \\ \beta_2 \\ \vdots \\ \beta_K \end{array} \right], $${{</math>}}

{{<math>}}$y_{it}${{</math>}} √© a vari√°vel dependente, {{<math>}}$\boldsymbol{x}'_{it}${{</math>}} √© o vetor-linha de dimens√£o {{<math>}}$K${{</math>}}:

{{<math>}}$$ \boldsymbol{x}'_{it} = \left[ \begin{array}{c} x^1_{it} & x^2_{it} & \cdots & x^K_{it} \end{array} \right],  $${{</math>}}

e o erro {{<math>}}$\varepsilon_{it}${{</math>}} pode ser escrito como:

{{<math>}}\[ \varepsilon_{it} = u_i + v_{it},  \]{{</math>}}
sendo {{<math>}}$u_i${{</math>}} o erro individual para o indiv√≠duo {{<math>}}$i${{</math>}} e {{<math>}}$v_{it}${{</math>}} √© o erro idiossincr√°tico (residual).

Empilhando as equa√ß√µes (1) para todo indiv√≠duo {{<math>}}$i = 1, 2, ..., N${{</math>}} e todo per√≠odo {{<math>}}$t = 1, 2, ..., T ${{</math>}}, temos

{{<math>}}$$ \underbrace{\boldsymbol{y}}_{NT \times 1} = \left[ \begin{array}{c}
    y_{11} \\ y_{12} \\ \vdots \\ y_{1T} \\ y_{21} \\ y_{22} \\ \vdots \\ y_{2T} \\ \vdots \\ y_{N1} \\ y_{N2} \\ \vdots \\ y_{NT}
\end{array} \right] \qquad \text{ e } \qquad 
\underbrace{\boldsymbol{X}}_{NT \times K} = \left[ \begin{array}{c}
    \boldsymbol{x}'_{11} \\ \boldsymbol{x}'_{12} \\ \vdots \\ \boldsymbol{x}'_{1K} \\
    \boldsymbol{x}'_{21} \\ \boldsymbol{x}'_{22} \\ \vdots \\ \boldsymbol{x}'_{2K} \\
    \vdots \\
    \boldsymbol{x}'_{N1} \\ \boldsymbol{x}'_{N2} \\ \vdots \\ \boldsymbol{x}'_{NK} 
    \end{array} \right]
  = \left[ \begin{array}{cccc}
    x^1_{11} & x^2_{11} & \cdots & x^K_{11} \\
    x^1_{12} & x^2_{12} & \cdots & x^K_{12} \\
    \vdots & \vdots & \ddots & \vdots \\
    x^1_{1T} & x^2_{1T} & \cdots & x^K_{1T} \\
    x^1_{21} & x^2_{21} & \cdots & x^K_{21} \\
    x^1_{22} & x^2_{22} & \cdots & x^K_{22} \\
    \vdots & \vdots & \ddots & \vdots \\
    x^1_{2T} & x^2_{2T} & \cdots & x^K_{2T} \\
    \vdots & \vdots & \ddots & \vdots \\
    x^1_{N1} & x^2_{N1} & \cdots & x^K_{N1} \\
    x^1_{N2} & x^2_{N2} & \cdots & x^K_{N2} \\
    \vdots & \vdots & \ddots & \vdots \\
    x^1_{NT} & x^2_{NT} & \cdots & x^K_{NT}
\end{array} \right] $$ {{</math>}}



Note que o modelo anterior n√£o possui uma constante, {{<math>}}$\alpha${{</math>}}. Podemos incluir uma constante no modelo (1) e reescrever esse novo modelo como:
{{<math>}} \begin{align} y_{it} &= 1.\alpha + \boldsymbol{x}'_{it} \boldsymbol{\beta} + \varepsilon_{it} \tag{2} \end{align} {{</math>}}

Denote {{<math>}}$\boldsymbol{\iota}${{</math>}} um vetor-coluna de 1's de tamanho {{<math>}}$NT${{</math>}}:
{{<math>}}$$ \boldsymbol{\iota} = \left[ \begin{array}{c} 1 \\ 1 \\ \vdots \\ 1 \end{array} \right] $${{</math>}}

Empilhando todas equa√ß√µes (2) para todo {{<math>}}$i${{</math>}} e {{<math>}}$t${{</math>}}, segue que
{{<math>}}\[ \boldsymbol{y} = \boldsymbol{\iota} \alpha + X \boldsymbol{\beta} + \boldsymbol{\varepsilon} \]{{</math>}}
ou, usando

{{<math>}}$$ \underbrace{\boldsymbol{\gamma}}_{(K+1) \times 1} \equiv \left[ \begin{array}{c} \alpha \\ \boldsymbol{\beta} \end{array} \right] = \left[ \begin{array}{c} \alpha \\ \beta_1 \\ \beta_2 \\ \vdots \\ \beta_K \end{array} \right] \quad \text{ e } \quad 
\underbrace{\boldsymbol{Z}}_{NT \times (K+1)} \equiv \left[ \begin{array}{c} \boldsymbol{\iota} & \boldsymbol{X} \end{array} \right]
  = \left[ \begin{array}{cccc}
    x^1_{11} & x^2_{11} & \cdots & x^K_{11} \\
    x^1_{12} & x^2_{12} & \cdots & x^K_{12} \\
    \vdots & \vdots & \ddots & \vdots \\
    x^1_{1T} & x^2_{1T} & \cdots & x^K_{1T} \\
    x^1_{21} & x^2_{21} & \cdots & x^K_{21} \\
    x^1_{22} & x^2_{22} & \cdots & x^K_{22} \\
    \vdots & \vdots & \ddots & \vdots \\
    x^1_{2T} & x^2_{2T} & \cdots & x^K_{2T} \\
    \vdots & \vdots & \ddots & \vdots \\
    x^1_{N1} & x^2_{N1} & \cdots & x^K_{N1} \\
    x^1_{N2} & x^2_{N2} & \cdots & x^K_{N2} \\
    \vdots & \vdots & \ddots & \vdots \\
    x^1_{NT} & x^2_{NT} & \cdots & x^K_{NT}
\end{array} \right], $$ {{</math>}}

podemos reescrever como
{{<math>}}\[ \boldsymbol{y} = \boldsymbol{Z} \boldsymbol{\gamma} + \boldsymbol{\varepsilon}. \]{{</math>}}


</br>
## Transforma√ß√µes _between_ e _within_
- Se√ß√£o 2.1.2 de "Panel Data Econometrics with R" (Croissant \& Millo, 2018)

Denote {{<math>}}$I_p${{</math>}} a matriz identidade de dimens√£o {{<math>}}$p${{</math>}}, e {{<math>}}$\boldsymbol{\iota}_q${{</math>}} um vetor de 1's de tamanho {{<math>}}$q${{</math>}}. 

A matriz de transforma√ß√£o **inter-indiv√≠duos (_between_)** √© denotada por:
{{<math>}}\[ B\ =\ I_N \otimes \boldsymbol{\iota}_T (\boldsymbol{\iota}'_T \boldsymbol{\iota}_T)^{-1} \boldsymbol{\iota}'_T \]{{</math>}}
Note que a matriz {{<math>}}$B${{</math>}} √© equivalente a {{<math>}}$N${{</math>}} nas notas de aula de Econometria I.


Por exemplo, para {{<math>}}$N = 2${{</math>}} e {{<math>}}$T = 3${{</math>}}, segue que:

{{<math>}}\begin{align*}
    B &= \left( \begin{array}{cc} 1 & 0 \\ 0 & 1 \end{array} \right) \otimes \left[ \left( \begin{array}{c} 1 \\ 1 \\ 1 \end{array} \right) \frac{1}{3} \left( \begin{array}{ccc} 1 & 1 & 1 \end{array} \right) \right] \\
    &= \left( \begin{array}{cc} 1 & 0 \\ 0 & 1 \end{array} \right) \otimes  \left( \begin{array}{ccc} 1/3 & 1/3 & 1/3 \\ 1/3 & 1/3 & 1/3 \\ 1/3 & 1/3 & 1/3 \end{array} \right)  \\
    &= \left( \begin{array}{cc} 1 \left( \begin{array}{ccc} 1/3 & 1/3 & 1/3 \\ 1/3 & 1/3 & 1/3 \\ 1/3 & 1/3 & 1/3 \end{array} \right) & 0 \left( \begin{array}{ccc} 1/3 & 1/3 & 1/3 \\ 1/3 & 1/3 & 1/3 \\ 1/3 & 1/3 & 1/3 \end{array} \right) \\ 0 \left( \begin{array}{ccc} 1/3 & 1/3 & 1/3 \\ 1/3 & 1/3 & 1/3 \\ 1/3 & 1/3 & 1/3 \end{array} \right) & 1 \left( \begin{array}{ccc} 1/3 & 1/3 & 1/3 \\ 1/3 & 1/3 & 1/3 \\ 1/3 & 1/3 & 1/3 \end{array} \right) \end{array} \right) \\
    &= \left( \begin{array}{rrrrrr} 
        1/3 & 1/3 & 1/3 & 0 & 0 & 0 \\
        1/3 & 1/3 & 1/3 & 0 & 0 & 0 \\
        1/3 & 1/3 & 1/3 & 0 & 0 & 0 \\
        0 & 0 & 0 & 1/3 & 1/3 & 1/3 \\
        0 & 0 & 0 & 1/3 & 1/3 & 1/3 \\
        0 & 0 & 0 & 1/3 & 1/3 & 1/3
    \end{array} \right)_{NT \times NT},
\end{align*}{{</math>}}

em que {{<math>}}$\otimes${{</math>}} √© o produto de Kronecker. Ent√£o, temos
{{<math>}}\[ (Bx^k)' = \left( \bar{x}^k_1, \cdots, \bar{x}^k_1, \bar{x}^k_2, \cdots, \bar{x}^k_2, \cdots,\bar{x}^k_N, \cdots, \bar{x}^k_N \right). \]{{</math>}}

Para calcular no R, vamos definir:
```{r}
N = 2 # n√∫mero de indiv√≠duos
T = 3 # n√∫meros de per√≠odos

iota = matrix(rep(1, T), T, 1) # vetor coluna de 1's de tamanho T
iota

I_N = diag(N) # matriz identidade de tamanho N
I_N
```

Vamos obter {{<math>}}$\boldsymbol{\iota} (\boldsymbol{\iota}' \boldsymbol{\iota})^{-1} \boldsymbol{\iota}'${{</math>}}
```{r}
# Para obter matriz T x T preenchida por 1/T, sendo T = 3, temos que:
t(iota) %*% iota # produto interno de iotas = quantidade T
solve(t(iota) %*% iota) # tomar a inversa = 1/T
iota %*% solve(t(iota) %*% iota) %*% t(iota) # pr√© e p√≥s-multiplicar por iotas
```

Agora, vamos calcular {{<math>}}$B\ =\ I_N \otimes \boldsymbol{\iota} (\boldsymbol{\iota}' \boldsymbol{\iota})^{-1} \boldsymbol{\iota}'${{</math>}} usando o operador de produto de Kronecker `%x%`:
```{r}
B = I_N %x% (iota %*% solve(t(iota) %*% iota) %*% t(iota))
round(B, 3)
```

Agora, vamos definir uma matriz de covariadas `X` e p√≥s-multiplicar pela matriz `B`
```{r}
K = 3 # n√∫mero de covariadas
X = matrix(1:(N*T*K), N*T, K) # matriz covariadas NT x K
Z = cbind(rep(1, N*T), X) # incluindo coluna de 1's
Z

B %*% Z # matriz de m√©dias das covariadas dado indiv√≠duo (NT x K)
```
Note que:

- dada uma vari√°vel {{<math>}}$k${{</math>}}, temos um √∫nico valor (m√©dia) dentro de um mesmo indiv√≠duo;
- coluna 1's permaneceu igual ap√≥s a transforma√ß√£o _between_.


J√° a matriz de transforma√ß√£o **intra-indiv√≠duos (_within_)** √© dada por:
{{<math>}}\[ W\ =\ I_{NT} - B\ =\ I_{NT} - \left[ I_N \otimes \boldsymbol{\iota} (\boldsymbol{\iota}' \boldsymbol{\iota})^{-1} \boldsymbol{\iota}' \right]. \]{{</math>}}

Note que a matriz {{<math>}}$W${{</math>}} √© equivalente a {{<math>}}$M${{</math>}} nas notas de aula de Econometria I.

Por exemplo, para {{<math>}}$N = 2${{</math>}} e {{<math>}}$T = 3${{</math>}}, segue que:

{{<math>}}\begin{align*}
    W &= I_{NT} - B \\
    &= \left( \begin{array}{cccccc} 
        1 & 0 & 0 & 0 & 0 & 0 \\
        0 & 1 & 0 & 0 & 0 & 0 \\
        0 & 0 & 1 & 0 & 0 & 0 \\
        0 & 0 & 0 & 1 & 0 & 0 \\
        0 & 0 & 0 & 0 & 1 & 0 \\
        0 & 0 & 0 & 0 & 0 & 1
    \end{array} \right) - \left( \begin{array}{rrrrrr} 
        1/3 & 1/3 & 1/3 & 0 & 0 & 0 \\
        1/3 & 1/3 & 1/3 & 0 & 0 & 0 \\
        1/3 & 1/3 & 1/3 & 0 & 0 & 0 \\
        0 & 0 & 0 & 1/3 & 1/3 & 1/3 \\
        0 & 0 & 0 & 1/3 & 1/3 & 1/3 \\
        0 & 0 & 0 & 1/3 & 1/3 & 1/3
    \end{array} \right) \\
    &= \left( \begin{array}{rrrrrr} 
         2/6 & -1/3 & -1/3 &    0 &    0 &    0 \\
        -1/3 &  2/6 & -1/3 &    0 &    0 &    0 \\
        -1/3 & -1/3 &  2/6 &    0 &    0 &    0 \\
           0 &    0 &    0 &  2/6 & -1/3 & -1/3 \\
           0 &    0 &    0 & -1/3 &  2/6 & -1/3 \\
           0 &    0 &    0 & -1/3 & -1/3 &  2/6
    \end{array} \right)_{NT \times NT}, 
\end{align*}{{</math>}}

```{r}
I_NT = diag(N*T) # matriz identidade com NT elementos na diagonal
W = I_NT - B # matriz de transforma√ß√£o within
W
```
```{r}
round(W %*% Z, 3) # matriz de desvios das m√©dias das covariadas dado indiv√≠duo (NT x K)
```
Observe que:

- dada uma vari√°vel {{<math>}}$k${{</math>}}, temos os desvios em rela√ß√£o √† m√©dia de um mesmo indiv√≠duo;
- por isso, a amostra de tamanho {{<math>}}$NT${{</math>}}, torna-se uma amostra de {{<math>}}$N${{</math>}} observa√ß√µes distintas;
- coluna 1's virou de 0's ap√≥s a transforma√ß√£o _within_.
- coluna de 0's, no R, ficou muito pr√≥xima de 0 ({{<math>}}$1,11 \times 10^{-16}${{</math>}}), ent√£o foi necess√°rio arredondar.


</br>
## Matriz de covari√¢ncias dos erros
- Se√ß√£o 2.2 de "Panel Data Econometrics with R" (Croissant \& Millo, 2018)

A matriz de covari√¢ncias dos erros depende apenas de dois par√¢metros: {{<math>}}$\sigma^2_u${{</math>}} e {{<math>}}$\sigma^2_v${{</math>}}. Ent√£o:

- Vari√¢ncia de um erro: 
{{<math>}}\[ E(\varepsilon^2_{it}) = \sigma^2_u + \sigma^2_v \]{{</math>}}
- Covari√¢ncia de dois erros de um mesmo indiv√≠duo {{<math>}}$i${{</math>}} em dos per√≠odos  {{<math>}}$t \neq s${{</math>}}:
{{<math>}}\[ E(\varepsilon_{it} \varepsilon_{is}) = \varepsilon^2_u \]{{</math>}}
- Covari√¢ncia entre dois diferentes indiv√≠duos ({{<math>}}$i \neq j${{</math>}}): 
{{<math>}}\[ E(\varepsilon_{it} \varepsilon_{jt}) = E(\varepsilon_{it} \varepsilon_{js}) = 0 \]{{</math>}}

A matriz de vari√¢ncia dos erros pode ser expressa por:
{{<math>}}\[ \Sigma = \sigma^2_v I_{NT} + \sigma^2_u [I_N \otimes \boldsymbol{\iota} (\boldsymbol{\iota}'\boldsymbol{\iota})^{-1} \boldsymbol{\iota}'] \]{{</math>}}
ou, em termos de {{<math>}}$B${{</math>}} e {{<math>}}$W${{</math>}},
{{<math>}}\[ \Sigma = \sigma^2_v W + T \sigma^2_u B \]{{</math>}}


</br>
## Estimadores OLS em painel
- Supomos que ambos componentes de erros s√£o n√£o-correlacionados com as covariadas:
{{<math>}}\[ E(u|x) = E(v|x) = 0 \]{{</math>}}
- A variabilidade em um painel tem 2 componentes:
    - a _between_ ou inter-indiv√≠duos, em que a variabilidade das vari√°veis s√£o mensuradas em m√©dias individuais, como {{<math>}}$\bar{z}_i${{</math>}} ou na forma matricial {{<math>}}$BZ${{</math>}}
    - a _within_ ou intra-indiv√≠duos, em que a variabilidade das vari√°veis s√£o mensuradas em desvios das m√©dias individuais, como {{<math>}}$z_{it} - \bar{z}_i${{</math>}} ou na forma matricial {{<math>}}$WZ = Z - BZ${{</math>}}
    - Lembre-se que {{<math>}}$z_i \equiv (1, X_i)${{</math>}} e {{<math>}}$Z \equiv (\boldsymbol{\iota}, X)${{</math>}}
- H√° tr√™s estimadores por OLS que podem ser utilizados:
    1. *Pooled OLS (MQE)*: usando a base de dados bruta (empilhada)
    2. *Estimador Between*: usando as m√©dias individuais
    3. *Estimador Within (Efeitos Fixos)*: usando os desvios das m√©dias individuais


### Pooled OLS (MQE)
- Se√ß√£o 2.1.1 de "Panel Data Econometrics with R" (Croissant \& Millo, 2018)

O modelo a ser estimado √©
{{<math>}}\[ y\ =\ Z\gamma + \varepsilon\ =\ \alpha \boldsymbol{\iota} + X \beta + \varepsilon \]{{</math>}}

- O estimador {{<math>}}$\hat{\gamma} = (\hat{\alpha}, \hat{\beta})${{</math>}} √© dado por
{{<math>}}\[ \hat{\gamma}_{OLS} = (Z'Z)^{-1} Z' y \]{{</math>}}
- A matriz de covari√¢ncias pode ser obtida usando
{{<math>}}\[ V(\hat{\gamma}_{OLS}) = (Z'Z)^{-1} Z' \Sigma Z (Z'Z)^{-1} \]{{</math>}}



### Estimador _Between_
O modelo a ser estimado √© o OLS pr√©-multiplicado por {{<math>}}$B = I_N \otimes \boldsymbol{\iota} (\boldsymbol{\iota}' \boldsymbol{\iota})^{-1} \boldsymbol{\iota}'${{</math>}}:
{{<math>}}\[ By\ =\ BZ\gamma + B\varepsilon\ =\ \alpha \boldsymbol{\iota} + BX \beta + B\varepsilon \]{{</math>}}

- O estimador {{<math>}}$\hat{\beta}${{</math>}} √© dado por
{{<math>}}\[ \hat{\gamma}_{B}\ =\ (Z' B Z )^{-1} Z' B y \]{{</math>}}
- A matriz de covari√¢ncias pode ser obtida usando

{{<math>}}\begin{align*}
    V(\hat{\gamma}_{B}) &= (Z'BZ)^{-1} Z' B\Sigma B Z (Z'BZ)^{-1} \\
    &= \sigma^2_l (Z' B Z)^{-1},
\end{align*}{{</math>}}
em que {{<math>}}\[\sigma^2_l = \sigma^2_v + T \sigma^2_u \]{{</math>}}

- O estimador n√£o-viesado de {{<math>}}$\sigma^2_l${{</math>}} √©
{{<math>}}\[ \hat{\sigma}^2_l = \frac{\hat{\varepsilon}' B \hat{\varepsilon}}{N-K-1} \]{{</math>}}

- O estimador _between_ tamb√©m pode ser estimado por OLS, transformando as vari√°veis por pr√©-multiplica√ß√£o da matriz _between_ ({{<math>}}$B${{</math>}}):
{{<math>}}\[ \tilde{Z} \equiv BZ \qquad \text{ e } \qquad \tilde{y} = By \]{{</math>}} 
tal que 
{{<math>}}\[ \hat{\gamma} = ( \tilde{Z}' \tilde{Z} )^{-1} \tilde{Z}' \tilde{y} \]{{</math>}}
e assim por diante.
- Note que, a rotina padr√£o de OLS retorna {{<math>}}$\hat{\sigma}^2_l = \frac{\hat{\varepsilon}' B \hat{\varepsilon}}{NT-K-1}${{</math>}} e, portanto, √© necess√°rio fazer ajuste dos graus de liberdade multiplicando a matriz de covari√¢ncias dos erros por {{<math>}}$(NT-K-1) / (N-K-1)${{</math>}}.  


### Estimador _Within_ (Efeitos Fixos)
- Tamb√©m conhecido como estimador de **Efeitos Fixos**
- N√£o assume que {{<math>}}$E(u | X) = 0${{</math>}}
- Estima efeitos individuais para, "limpando" efeito inter-indiv√≠duos nas demais covariadas

O modelo a ser estimado √© o OLS pr√©-multiplicado por {{<math>}}$W = I_{NT} - B${{</math>}}:
{{<math>}}\[ Wy\ =\ WZ\gamma + W\varepsilon\ =\ WX \beta + Wv. \]{{</math>}}
Note que a transforma√ß√£o within remove vetor de 1's associado ao intercepto, al√©m das covariadas invariantes no tempo e o termo de erro individual {{<math>}}$u${{</math>}} (sobrando apenas {{<math>}}$\varepsilon = v${{</math>}}).

- O estimador {{<math>}}$\hat{\beta}${{</math>}} √© dado por
{{<math>}}\[ \hat{\beta}_{W}\ =\ (X' W X )^{-1} X' W y \]{{</math>}}
- A matriz de covari√¢ncias pode ser obtida usando
{{<math>}}\begin{align*}
    V(\hat{\beta}_{W}) &= (X'WX)^{-1} X' W\Sigma W X (X'WX)^{-1} \\
    &= \sigma^2_v (X' W X)^{-1}.
\end{align*}{{</math>}}

- O estimador n√£o-viesado de {{<math>}}$\sigma^2_v${{</math>}} √©
{{<math>}}\[ \hat{\sigma}^2_v = \frac{\hat{\varepsilon}' W \hat{\varepsilon}}{NT-K-N} \]{{</math>}}

- O estimador _within_ tamb√©m pode ser estimado por OLS, transformando as vari√°veis por pr√©-multiplica√ß√£o da matriz _within_ ({{<math>}}$W${{</math>}}):
{{<math>}}\[ \tilde{Z} \equiv WZ \qquad \text{ e } \qquad \tilde{y} = Wy \]{{</math>}} 
tal que 
{{<math>}}\[ \hat{\gamma} = ( \tilde{Z}' \tilde{Z} )^{-1} \tilde{Z}' \tilde{y} \]{{</math>}}
e assim por diante.
- Note que, a rotina padr√£o de OLS retorna {{<math>}}$\hat{\sigma}^2_v = \frac{\hat{\varepsilon}' W \hat{\varepsilon}}{NT-K-1}${{</math>}} e, portanto, √© necess√°rio fazer ajuste dos graus de liberdade multiplicando a matriz de covari√¢ncias dos erros por {{<math>}}$(NT-K-1) / (NT-K-N)${{</math>}}.


### Estima√ß√£o OLS
#### Estima√ß√£o via `plm()`
Para ilustrar as estima√ß√µes OLS dos estimadores vistos anteriormente, usaremos a base de dados `TobinQ` do pacote `pder`, que conta com dados de 188 firmas americanos por 35 anos (6580 observa√ß√µes).
```{r}
library(dplyr)
data("TobinQ", package = "pder")
summary(TobinQ %>% select(cusip, year, ikn, qn))
```
- `cusip`: Identificador da empresa
- `year`: Ano
- `ikn`: Investimento dividido pelo capital
- `qn`: Q de Tobin (raz√£o entre valor da firma e o custo de reposi√ß√£o de seu capital f√≠sico). Se {{<math>}}$Q > 1${{</math>}}, ent√£o o lucro investimento √© maior do que seu custo.

Queremos estimar o seguinte modelo:
{{<math>}}\[ \text{ikn} = \alpha + \text{qn} \beta + \varepsilon \]{{</math>}}


Usaremos a fun√ß√£o `plm()` (do pacote de mesmo nome) para estimar modelos lineares em dados em painel. Seus principais argumentos s√£o:

- `formula`: equa√ß√£o do modelo
- `data`: base de dados em `data.frame` (precisa preencher `index`) ou `pdata.frame` (formato pr√≥prio do pacote que j√° indexa as colunas de indiv√≠duos e de tempo)
- `model`: estimador a ser computado 'pooling' (MQE), 'between', 'within' (Efeitos Fixos) e 'random' (Efeitos Aleat√≥rios/GLS)
- `index`: vetor de nomes dos identificadores de indiv√≠duo e de tempo

```{r warning=FALSE}
library(plm)

# Transformando no formato pdata frame, com indentificador de indiv√≠duo e de tempo
pTobinQ = pdata.frame(TobinQ, index=c("cusip", "year"))

# Estima√ß√µes
Q.pooling = plm(ikn ~ qn, pTobinQ, model = "pooling")
Q.between = plm(ikn ~ qn, pTobinQ, model = "between")
Q.within = plm(ikn ~ qn, pTobinQ, model = "within")

summary(Q.within) # output da estima√ß√£o within

# Resumindo 3 estima√ß√µes em √∫nica tabela
stargazer::stargazer(Q.pooling, Q.between, Q.within, type="text")
```
- Observe que:
    - as vari√°veis entram ns estima√ß√£o sem nenhuma transforma√ß√£o as diferentes quantidades de observa√ß√µes e
    - cada m√©todo possui diferentes graus de liberdade


##### Estimando _Within_ e _Between_ via Pooled OLS
- N√≥s podemos construir as vari√°veis de m√©dia e de desvios de m√©dia diretamente no data frame e estimar o _between_ e _within_ via (pooled) OLS
```{r}
TobinQ = TobinQ %>% group_by(cusip) %>% # agrupando por cusip
    mutate(
        ikn_bar = mean(ikn), # "transforma√ß√£o" between de ikn
        qn_bar = mean(qn), # "transforma√ß√£o" between de qn
        ikn_desv = ikn - ikn_bar, # "transforma√ß√£o" within de ikn
        qn_desv = qn - qn_bar # "transforma√ß√£o" within de qn
    ) %>% ungroup()

pTobinQ = pdata.frame(TobinQ, index=c("cusip", "year"))

Q.pooling_between = plm(ikn_bar ~ qn_bar, pTobinQ, model = "pooling")

summary(Q.pooling_between)$coef # between via pooled OLS
summary(Q.between)$coef # between
```
- Note que, embora as estimativas sejam as mesmas, acabamos subestimando os erros padr√£o e, portanto, superestimando os valores t.
- Ao estimar o _between_ via pooled OLS, ele n√£o faz os ajustes dos graus de liberdade nas vari√¢ncias das estimativas
- Logo, vamos ajustar os graus de liberdade multiplicando a vari√¢ncia das estimativas por {{<math>}}$(NT - K - 1)${{</math>}} e dividindo por {{<math>}}$(N - K - 1)${{</math>}}
```{r}
std_error = summary(Q.pooling_between)$coef[, "Std. Error"]
variance = std_error^2
adj_variance = variance * (188*35 - 1 - 1) / (188 - 1 - 1)
adj_std_error = sqrt(adj_variance)
adj_std_error
```


##### Efeitos Fixos da Estima√ß√£o _Within_
- Para o estimador _within_, podemos usar a fun√ß√£o `fixef()` para computar os efeitos individuais. √â poss√≠vel calcular 3 tipos por meio do argumento `type`:
    - `level`: valor padr√£o que retorna os interceptos individuais ({{<math>}}$\hat{\alpha} + \hat{u}_{i}${{</math>}})
    - `dfirst`: em desvios do 1¬∫ indiv√≠duo ({{<math>}}$\hat{\alpha}${{</math>}} √© o intercepto do 1¬∫ indiv√≠duo)
    - `dmean`: em desvios da m√©dia de efeitos individuais ({{<math>}}$\hat{\alpha}${{</math>}} √© a m√©dia)

```{r}
# 6 primeiros efeitos individuais de cada tipo
for (t in c("level", "dfirst", "dmean")) {
    print( head( fixef(Q.within, type=t) ) )
}
```
- Note que, como o `dfirst` retorna valores em rela√ß√£o ao 1¬∫ indiv√≠duo, este n√£o aparece no output do `fixef()`, diferente dos demais.
- No caso linear, o estimador _within_ √© equivalente √† estima√ß√£o por OLS com inclus√£o de dummies para cada indiv√≠duo:
```{r}
# Estimando OLS com dummies individuais - factor() tranforma cusip em var. categ.
Q.dummies = lm(ikn ~ qn + factor(cusip), pTobinQ)

# Comparando as estimativas de qn
cbind(Q.within$coef, Q.dummies$coef["qn"])

# Comparando efeitos fixos (dfirst) e dummies
cbind(head(fixef(Q.within, type="dfirst")),
      Q.dummies$coef[3:8])
```


#### Estima√ß√£o Anal√≠tica Pooled OLS (MQE)
Igual a estima√ß√£o anal√≠tica de MQO vista anteriormente.


#### Estima√ß√£o Anal√≠tica _Between_
```{r}
data("TobinQ", package="pder")
TobinQ = TobinQ %>% mutate(constant = 1) # criando vetor de 1's

y = TobinQ %>% select(ikn) %>% as.matrix() # vetor y
X = TobinQ %>% select(qn) %>% as.matrix() # vetor X
Z = cbind(TobinQ$constant, X) # vetor Z = (iota, X)

N = TobinQ %>% select(cusip) %>% unique() %>% nrow()
T = TobinQ %>% select(year) %>% unique() %>% nrow()
iota_T = rep(1, T)

# Calculando matrizes de tranforma√ß√£o B e W
B = diag(N) %x% (iota_T %*% solve(t(iota_T) %*% iota_T) %*% t(iota_T))
W = diag(N*T) - B
```


{{<math>}}\[ \hat{\gamma} = (\hat{\alpha}, \hat{\beta}) = (Z' B Z)^{-1} Z' By  \]{{</math>}}

```{r}
# vetor de estimativas gamma_hat = (alpha, beta)
gamma_hat = solve(t(Z) %*% B %*% Z) %*% t(Z) %*% B %*% y
gamma_hat
```


{{<math>}}\[ \hat{y} = Z \hat{\gamma} \qquad \text{ e } \qquad  \hat{\varepsilon} = y - \hat{y} \]{{</math>}}
```{r}
# valores ajustados e erros
y_hat = Z %*% gamma_hat
e_hat = y - y_hat
```


{{<math>}}\[ \hat{\sigma}^2 = \frac{\hat{\varepsilon}' B \hat{\varepsilon}}{N-K-1} \]{{</math>}}
```{r}
## Estimando variancia do termo de erro
sigma2_l = t(e_hat) %*% B %*% e_hat / (N - ncol(Z)) # N - K - 1 graus de liberdade!
sigma2_l
```
**IMPORTANTE**: Ajustar os graus de liberdade do estimador _between_ para {{<math>}}$N - K - 1${{</math>}} (ao inv√©s de {{<math>}}$NT - K - 1${{</math>}})

{{<math>}}\[ \widehat{V}(\hat{\gamma}) = \hat{\sigma}^2_l (Z'BZ)^{-1} \]{{</math>}}
```{r}
## Estimando a matriz de variancia/covariancia das estimativas gamma
vcov_hat = c(sigma2_l) * solve(t(Z) %*% B %*% Z)
vcov_hat

## Calculando erros padrao das estimativas gamma
std_error = sqrt(diag(vcov_hat)) # Raiz da diagonal da matriz de covari√¢ncias

## Calculando estatisticas t das estimativas gamma
t_stat = gamma_hat / std_error

## Calculando p-valores das estimativas gamma
p_value = 2 * pt(q = -abs(t_stat), df = N - ncol(Z))  # N - K - 1 graus de liberdade!

## Organizando os resultados da regressao em uma matriz
results = cbind(gamma_hat, std_error, t_stat, p_value)

## Nomeando as colunas da matriz de resultados
colnames(results) = c('Estimate', 'Std. Error', 't stat', 'Pr(>|t|)')
results

summary(Q.between)$coef # comparando com estimado via plm()
```


#### Estima√ß√£o Anal√≠tica _Within_
(Exerc√≠cio)


</br>
## Estimadores GLS
- Se√ß√£o 2.3 de "Panel Data Econometrics with R" (Croissant \& Millo, 2018)
- Ao contr√°rio do estimador _within_ que retira os efeitos individuais, o estimador de **GLS** considera que os efeitos individuais como aleat√≥rios a partir de uma distribui√ß√£o espec√≠fica.
- Erros n√£o s√£o correlacionados com as covariadas, e s√£o caracterizados por uma matriz de covari√¢ncias {{<math>}}$\Sigma${{</math>}}.
- O estimador de GLS √© dado por
{{<math>}}\[ \hat{\gamma}_{GLS} = (Z' \Sigma^{-1} Z)^{-1} (Z' \Sigma^{-1} y) \tag{2.27} \]{{</math>}}

- A vari√¢ncia do estimador √© dada por
{{<math>}}\[ V(\hat{\gamma}_{GLS}) = (Z' \Sigma^{-1} Z)^{-1} \tag{2.28} \]{{</math>}}
- A matriz {{<math>}}$\Sigma${{</math>}} depende apenas de dois par√¢metros: {{<math>}}$\sigma^2_u${{</math>}} e {{<math>}}$\sigma^2_v${{</math>}}, temos:
{{<math>}}\[ \Sigma^p = (\sigma^2_l)^p B + (\sigma^2_v)^p W \tag{2.29} \]{{</math>}}

- Lembre-se que {{<math>}}\[\sigma^2_l = \sigma^2_v + T \sigma^2_u \]{{</math>}}

### GLS: jun√ß√£o Pooled OLS e _Within_
- Combina√ß√£o de Pooled OLS (Efeitos Aleat√≥rios) e de _Within_ (Efeitos Fixos)
- Pode-se computar mais eficientemente por OLS, que necessita transforma√ß√£o das vari√°veis usando a matriz {{<math>}}$\Sigma^{-0.5}${{</math>}}, tal que {{<math>}}$\Sigma^{-0.5\prime}\Sigma^{-0.5} = \Sigma^{-1}${{</math>}}.
- Denotando {{<math>}}$\tilde{y} \equiv \Sigma^{-0.5}y${{</math>}} e {{<math>}}$\tilde{Z} \equiv \Sigma^{-0.5}Z${{</math>}}, o modelo com vari√°veis transformadas √© dado por
{{<math>}}\begin{align*}
    \hat{\gamma} &= (Z' \Sigma^{-1} Z)^{-1} (Z' \Sigma^{-1} y) \tag{2.27} \\
    &= (Z' \Sigma^{-0.5\prime} \Sigma^{-0.5} Z)^{-1} (Z' \Sigma^{-0.5}\Sigma^{-0.5\prime} y) \\
    &= (\tilde{Z}'\tilde{Z})^{-1} \tilde{Z} \tilde{y}
\end{align*}{{</math>}}

Usando (2.29), {{<math>}}$p=-0.5${{</math>}} em (2.29), tem-se
{{<math>}}\[ \Sigma^{-0.5} = \frac{1}{\sigma_l} B + \frac{1}{\sigma_v} W \]{{</math>}}

Essa transforma√ß√£o evidencia uma combina√ß√£o linear entre as matrizes de transforma√ß√£o _between_ e _within_ ponderadas pelo inverso dos desvios padr√£o dos 2 componentes de erro ({{<math>}}$\sigma^2_v${{</math>}} e {{<math>}}$\sigma^2_u = (\sigma^2_v + \sigma^2_l)/T${{</math>}})

Pr√©-multiplicando as vari√°veis por {{<math>}}$\sigma_v \Sigma^{-0.5}${{</math>}} (ao inv√©s de {{<math>}}$\Sigma^{-0.5}${{</math>}} para simplifica√ß√£o e sem perda de generalidade), as covariadas transformadas para o indiv√≠duo {{<math>}}$i${{</math>}} no tempo {{<math>}}$t${{</math>}} s√£o dadas por:
{{<math>}}\[ \tilde{z}_{it}\ =\ \frac{\sigma_v}{\sigma_l} \bar{z}_{i\cdot} + (z_{it} - \bar{z}_{i\cdot})\ =\ z_{it} + \left(1 - \frac{\sigma_v}{\sigma_l} \right) \bar{z}_{i\cdot}\ \equiv\ z_{it} - \theta \bar{z}_{i\cdot} \]{{</math>}}
em que
{{<math>}}\[ \theta\ \equiv\ 1 - \frac{\sigma_v}{\sigma_l}\ \equiv\ 1 - \phi \]{{</math>}}

    
Note que, quando:

- {{<math>}}$\theta \rightarrow 1${{</math>}}, os efeitos individuais {{<math>}}$\sigma_u${{</math>}} dominam {{<math>}}$\implies${{</math>}} GLS se aproxima do estimador _within_
- {{<math>}}$\theta \rightarrow 0${{</math>}}, os efeitos individuais {{<math>}}$\sigma_u${{</math>}} somem {{<math>}}$\implies${{</math>}} GLS se aproxima do pooled OLS


### Estima√ß√£o dos Componentes de Erro
- Note que n√£o temos {{<math>}}$\sigma^2_v${{</math>}} e {{<math>}}$\sigma^2_u = (\sigma^2_v + \sigma^2_l)/T${{</math>}} e, logo, {{<math>}}$\Sigma${{</math>}} √© desconhecido.
- Se {{<math>}}$\varepsilon${{</math>}} fosse conhecido, ent√£o os estimadores para as duas vari√¢ncias seriam:
{{<math>}}\begin{align*}
    \hat{\sigma}^2_l &= \frac{\varepsilon' B \varepsilon}{N} \tag{2.34}\\
    \hat{\sigma}^2_v &= \frac{\varepsilon' W \varepsilon}{N(T-1)} \tag{2.35}
\end{align*}{{</math>}}
- Como {{<math>}}$\varepsilon${{</math>}} √© desconhecido, ent√£o usam-se res√≠duos de estimadores consistentes em seu lugar.
- O estimador obtido por esse processo √© chamado de FGLS (ou GLS Fact√≠vel)
- **Wallace e Hussain (1969)**: usam res√≠duos OLS
{{<math>}}\begin{align*}
    \hat{\sigma}^2_l &= \frac{\hat{\varepsilon}'_{OLS} B \hat{\varepsilon}_{OLS}}{N} \\
    \hat{\sigma}^2_v &= \frac{\hat{\varepsilon}'_{OLS} W \hat{\varepsilon}_{OLS}}{N(T-1)}
\end{align*}{{</math>}}
- **Amemiya (1971)**: usam res√≠duos _within_
{{<math>}}\begin{align*}
    \hat{\sigma}^2_l &= \frac{\hat{\varepsilon}'_{W} B \hat{\varepsilon}_{W}}{N}\\
    \hat{\sigma}^2_v &= \frac{\hat{\varepsilon}'_{W} W \hat{\varepsilon}_{W}}{N(T-1)}
\end{align*}{{</math>}}
Note que, a vari√¢ncia do efeito individual √© sobre-estimado quando o modelo cont√©m vari√°veis invariantes no tempo (somem com a transforma√ß√£o _within_)
- **Hausman e Taylor (1981)**: propuseram ajuste ao m√©todo de Amemiya (1971), em que {{<math>}}$\hat{\varepsilon}_W${{</math>}} s√£o regredidos em todas vari√°veis invariantes no tempo no modelo e s√£o utilizados os res√≠duos dessa regress√£o, {{<math>}}$\hat{\varepsilon}_{HT}${{</math>}}.
- **Swamy e Arora (1972)**: usam res√≠duos _between_ e _within_ para calcular, respectivamente, {{<math>}}$\hat{\sigma}^2_l${{</math>}} e {{<math>}}$\hat{\sigma}^2_v${{</math>}}
{{<math>}}\begin{align*}
    \hat{\sigma}^2_l &= \frac{\hat{\varepsilon}'_{B} B \hat{\varepsilon}_{B}}{N - K - 1}\\
    \hat{\sigma}^2_v &= \frac{\hat{\varepsilon}'_{W} W \hat{\varepsilon}_{W}}{N(T-1) - K}
\end{align*}{{</math>}}

- **Nerlove (1971)**: computam {{<math>}}$\sigma^2_u${{</math>}} emp√≠rica dos efeitos fixos do modelo _within_

{{<math>}}\begin{align*}
    \hat{u}_i &= \bar{y}_{i\cdot} - \hat{\beta}_W \bar{x}_{i\cdot} \\
    \hat{\sigma}^2_u &= \sum^N_{i=1}{(\hat{u}_i - \bar{\hat{u}}) / (N-1)} \\
    \hat{\sigma}^2_v &= \frac{\hat{\varepsilon}'_W W \hat{\varepsilon}_W}{NT}
\end{align*}{{</math>}}


### Estima√ß√£o GLS via `plm()`
- Usaremos novamente a fun√ß√£o `plm()`, mas definiremos `model = random` para que seja estimado via GLS
- em `random.method` podemos escolher o m√©todo de c√°lculo dos par√¢metros de erro:
    1. `"walhus"` para Wallace e Hussain (1969)
    2. `"amemiya"` para Amemiya (1971)
    3. `"ht"` para Hausman e Taylor (1981)
    4. `"swar"` para Swamy e Arora (1972) [padr√£o]
    5. `"nerlove"` para Nerlove (1971)

```{r}
library(plm)
data("TobinQ", package = "pder")
pTobinQ = pdata.frame(TobinQ, index=c("cusip", "year"))

# Estima√ß√µes GLS
Q.walhus = plm(ikn ~ qn, pTobinQ, model = "random", random.method = "walhus")
Q.amemiya = plm(ikn ~ qn, pTobinQ, model = "random", random.method = "amemiya")
Q.ht = plm(ikn ~ qn, pTobinQ, model = "random", random.method = "ht")
Q.swar = plm(ikn ~ qn, pTobinQ, model = "random", random.method = "swar")
Q.nerlove = plm(ikn ~ qn, pTobinQ, model = "random", random.method = "nerlove")

summary(Q.walhus) # output da estima√ß√£o GLS por Wallace e Hussain (1969)
```
Note que {{<math>}}$\theta = 73\%${{</math>}}, o que indica que, neste caso, o estimativa GLS √© mais pr√≥xima de _within_ ({{<math>}}$\theta=1${{</math>}}) do que de _between_ ({{<math>}}$\theta=0${{</math>}}). A grande quantidade de per√≠odos ({{<math>}}$T = 35${{</math>}}) provavelmente influencia este alto valor.


```{r}
# Resumindo 5 estima√ß√µes em √∫nica tabela
stargazer::stargazer(Q.walhus, Q.amemiya, Q.ht, Q.swar, Q.nerlove, type="text")
```
Os resultados s√£o praticamente id√™nticos, assim como seus {{<math>}}$\theta${{</math>}}'s:

```{r}
# Podemos visualizar o theta usando ercomp()$theta
ercomp(Q.walhus)$theta

# Criaremos uma lista com todos objetos de estima√ß√£o GLS
Q.models = list(walhus = Q.walhus, amemiya = Q.amemiya, ht = Q.ht,
                swar = Q.swar, nerlove = Q.nerlove)

# Aplicaremos sapply() com a lista criada e a fun√ß√£o ercomp()
sapply(Q.models, function(model) ercomp(model)$theta)
```


Observe que poder√≠amos ter obtido informa√ß√µes sobre as vari√¢ncias de covariadas por meio de `summary()` sobre uma vari√°vel no formato `pdata.frame`:
```{r}
pTobinQ = pdata.frame(TobinQ, index=c("cusip", "year")) # transf. em pdata.frame
summary(pTobinQ$qn) # resumo sobre vari√°vel qn
```

Tamb√©m √© poss√≠vel verificar o mesmo para a vari√¢ncia do erro na estima√ß√£o em GLS:
```{r}
ercomp(ikn ~ qn, TobinQ) # padr√£o method = "swar"
```



#### Estima√ß√£o Anal√≠tica GLS
- Aqui, faremos a estima√ß√£o anal√≠tica do GLS usando o m√©todo de Wallace e Hussain (1969).
- Consiste no uso dos desvios estimados por pooled OLS para calcular {{<math>}}$\hat{\sigma}^2_l${{</math>}}, {{<math>}}$\hat{\sigma}^2_v${{</math>}} (e consequentemente {{<math>}}$\hat{\sigma}^2_u${{</math>}}), possibilitando encontrar {{<math>}}$\hat{\Sigma}^{-1}${{</math>}} para estimar por FGLS.
- Para agilizar a estima√ß√£o, vamos estimar o pooled OLS por `plm()`:
```{r}
library(plm)
data("TobinQ", package = "pder")

# Transformando no formato pdata frame, com indentificador de indiv√≠duo e de tempo
pTobinQ = pdata.frame(TobinQ, index=c("cusip", "year"))
```

- Obtendo {{<math>}}$\hat{\varepsilon}_{OLS}${{</math>}}

```{r}
# Estima√ß√£o pooled OLS
Q.pooling = plm(ikn ~ qn, pTobinQ, model = "pooling")

# obtendo os res√≠duos OLS
e_OLS = Q.pooling$residuals %>% as.vector()
head(e_OLS)
```

- Precisamos calcular {{<math>}}$\hat{\sigma}^2_l${{</math>}}, {{<math>}}$\hat{\sigma}^2_v${{</math>}} e {{<math>}}$\hat{\sigma}^2_u${{</math>}}

```{r}
TobinQ = TobinQ %>% mutate(constant = 1) # criando vetor de 1's

y = TobinQ %>% select(ikn) %>% as.matrix() # vetor y
X = TobinQ %>% select(qn) %>% as.matrix() # vetor X
Z = cbind(TobinQ$constant, X) # vetor Z = (iota, X)

N = TobinQ %>% select(cusip) %>% unique() %>% nrow() # n√∫m. indiv√≠duos
T = TobinQ %>% select(year) %>% unique() %>% nrow() # n√∫m. per√≠odos
iota_T = rep(1, T)

# Calculando matrizes de tranforma√ß√£o B e W
B = diag(N) %x% (iota_T %*% solve(t(iota_T) %*% iota_T) %*% t(iota_T))
W = diag(N*T) - B

# Calculando os termos de erro
sigma2_l = (t(e_OLS) %*% B %*% e_OLS) / N
sigma2_nu = (t(e_OLS) %*% W %*% e_OLS) / (N * (T-1))
sigma2_u = (sigma2_l + sigma2_nu) / T

sigmas2 = cbind(sigma2_l, sigma2_nu, sigma2_u)
colnames(sigmas2) = c("sigma2_l", "sigma2_nu", "sigma2_u")
sigmas2
```

- Agora, conseguimos calcular:
{{<math>}}\[ \Sigma^{-1} = \frac{1}{\sigma^2_l}B + \frac{1}{\sigma^2_v}W \]{{</math>}}
```{r}
Omega_1 = c(sigma2_l^(-1)) * B + c(sigma2_nu^(-1)) * W
dim(Omega_1) # NT x NT
```

{{<math>}}\[ \hat{V}_{GLS} = (Z' \Sigma^{-1} Z)^{-1} \]{{</math>}}

```{r}
## Estimando a matriz de variancia/covariancia das estimativas gamma
vcov_hat = solve(t(Z) %*% Omega_1 %*% Z)

# vetor de estimativas gamma_hat = (alpha, beta)
gamma_hat = solve(t(Z) %*% Omega_1 %*% Z) %*% (t(Z) %*% Omega_1 %*% y)
gamma_hat

## Calculando erros padrao das estimativas gamma
std_err = sqrt(diag(vcov_hat)) # Raiz da diagonal da matriz de covari√¢ncias

## Calculando estatisticas t das estimativas gamma
t_stat = gamma_hat / std_err

## Calculando p-valores das estimativas gamma
p_value = 2 * pt(q = -abs(t_stat), df = nrow(Z) - ncol(Z))  # NT - K - 1 graus de liberdade

## Organizando os resultados da regressao em uma matriz
results_walhus = cbind(gamma_hat, std_err, t_stat, p_value)

## Nomeando as colunas da matriz de resultados
colnames(results_walhus) = c('Estimate', 'Std. Error', 't stat', 'Pr(>|t|)')
rownames(results_walhus) = c("(Intercept)", "qn")
results_walhus

summary(Q.walhus)$coef # comparando com estimado via plm()
```


</br>
## Comparativo dos Estimadores OLS e GLS - Exemplo
- Se√ß√£o 2.4.4 de "Panel Data Econometrics with R" (Croissant \& Millo, 2018)
- Usado por Kinal e Lahiri (1993) 
- Queremos estabelecer rela√ß√£o entre importa√ß√µes (_imports_) e produto nacional (_gnp_)
```{r}
data("ForeignTrade", package = "pder")
FT = pdata.frame(ForeignTrade, index=c("country", "year"))

# Vari√¢ncias 
ercomp(imports ~ gnp, FT) # vari√¢ncia do erro na estima√ß√£o GLS
```
- Vari√¢ncia do erro da estima√ß√£o GLS √© dada por 93\% de varia√ß√£o inter-indiv√≠duos
- O estimador GLS remove grande parte da varia√ß√£o inter-indiv√≠duos, pois subtrai, da covariada, 94\% da m√©dia individual:

{{<math>}}\[ \tilde{z}_{it}\ =\ z_{it} + \left(1 - \frac{\sigma_v}{\sigma_l} \right) \bar{z}_{i\cdot}\ \equiv\ z_{it} - \theta \bar{z}_{i\cdot}\ =\ z_{it} - 0,94 \bar{z}_{i\cdot} \]{{</math>}}

```{r}
# Estima√ß√µes
models = c("within", "random", "pooling", "between")
sapply(models, function(x) round(
    coef(summary(plm(imports ~ gnp, FT, model=x)))["gnp",], 4))
```

<center><img src="../example_panel-1.png"></center>

- GLS e _within_ s√£o bastante parecidos
- OLS, que considera varia√ß√£o inter-indiv√≠duos, √© parecido com _between_

## Estimador ML
- Se√ß√£o 3.3 de "Panel Data Econometrics with R" (Croissant \& Millo, 2018)
- Uma alternativa aos estimadores de GLS √© o de m√°xima verossimilhan√ßa (ML).
- Assume-se que a distribui√ß√£o dos dois componentes de erro s√£o normais:
{{<math>}}\[ u | X \sim N(0, \sigma^2_u) \quad \text{ e } \quad v | u, X \sim N(0, \sigma^2_v) \]{{</math>}}

- O modelo a ser estimado √© o
{{<math>}}\[ y_{it} = \alpha \boldsymbol{\iota} + \beta' x_i + u_i + v_{it} = \gamma' z_i + u_i + v_{it} \]{{</math>}}

- Ao inv√©s de estimar {{<math>}}$\sigma^2_u${{</math>}} e {{<math>}}$\sigma^2_v${{</math>}} para depois calcular {{<math>}}$\gamma${{</math>}}, ambos s√£o estimados simultaneamente.

- Denotando 
{{<math>}}$\phi = \frac{\sigma_v}{\sigma_{l}},${{</math>}}
a fun√ß√£o de log-verossimilhan√ßa para um painel balanceado √©:

{{<math>}}\[ \ln{L} = -\frac{NT}{2} \ln{2\pi} - \frac{NT}{2}\ln{\sigma^2_v} + \frac{N}{2} \ln{\phi^2} - \frac{1}{2\sigma^2_v} \left( \varepsilon' W \varepsilon + \phi^2 \varepsilon' B \varepsilon \right) \]{{</math>}}

Denotando 

{{<math>}}\[\tilde{Z}\ \equiv\ (I - \phi B) Z\ =\ Z - \phi B Z\]{{</math>}}
e resolvendo as CPO's da log-verossimilhan√ßa, segue que:

{{<math>}}\begin{align*}
    \hat{\gamma} &= (\tilde{Z}'\tilde{Z})^{-1} \tilde{Z}'\tilde{y} \tag{3.12} \\
    \hat{\sigma}^2_v &= \frac{\hat{\varepsilon}' W \hat{\varepsilon} + \hat{\phi}^2 \hat{\varepsilon}' B \hat{\varepsilon}}{NT} \tag{3.13} \\
    \hat{\phi}^2 &=\frac{\hat{\varepsilon}' W \hat{\varepsilon}}{(T-1) \hat{\varepsilon}'B\hat{\varepsilon}} \tag{3.14}
\end{align*}{{</math>}}

A estima√ß√£o pode ser feita iterativamente por FIML (Full Information Maximum Likelihood):


1. Chute inicial de {{<math>}}$\hat{\gamma}${{</math>}} (por exemplo, estimativa _within_)
2. Calcular {{<math>}}$\hat{\phi}^2${{</math>}} usando (3.14)
3. Calcular {{<math>}}$\hat{\gamma}${{</math>}} usando (3.12) 
4. Verificar converg√™ncia: se n√£o convergiu, volta para o passo 2, usando o {{<math>}}$\hat{\gamma}${{</math>}} calculado no passo 3.
5. Calcular {{<math>}}$\sigma^2_v${{</math>}} usando (3.13)


### Estima√ß√£o ML via `pglm()`

```{r message=FALSE, warning=FALSE}
library(pglm)
library(dplyr)
data("TobinQ", package = "pder")

# Transformando no formato pdata frame, com indentificador de indiv√≠duo e de tempo
pTobinQ = pdata.frame(TobinQ, index=c("cusip", "year"))

# Estima√ß√£o pooled OLS
Q.ml = pglm(ikn ~ qn, pTobinQ, family = "gaussian")
summary(Q.ml)

summary(Q.swar)$coef # Comparando com estima√ß√£o GLS-swar
```
- Note que o resultado por ML foi bem pr√≥ximo ao do obtido por GLS


</br>
## Testes de Presen√ßa de Efeitos Individuais

### Breusch-Pagan

- Se√ß√£o 4.1 de "Panel Data Econometrics with R" (Croissant \& Millo, 2018)
- √â um teste baseado em multiplicadores de Lagrange (LM) nos res√≠duos de OLS, em que {{<math>}}$H_0: \sigma^2_u = 0${{</math>}} (aus√™ncia de efeitos individuais)
- A estat√≠stica teste √© dada por 
{{<math>}}\[ LM_u = \frac{NT}{2(T-1)} \left( T \frac{\hat{\varepsilon}' B_u \hat{\varepsilon}}{\hat{\varepsilon}' \hat{\varepsilon}} - 1 \right)^2  \]{{</math>}}
que √© assintoticamente distribu√≠da como ua $\chi^2$ com 1 grau de liberdade.
- H√° algumas varia√ß√µes deste teste:
    - Breusch and Pagan (1980),
    - Gourieroux et al. (1982),
    - Honda (1985), e
    - King and Wu (1997).



### Testes F
- Se√ß√£o 4.1 de "Panel Data Econometrics with R" (Croissant \& Millo, 2018)
- Sejam a soma dos res√≠duos ao quadrado e os graus de liberdade do modelo _within_ {{<math>}}$\hat{\varepsilon}'_W\hat{\varepsilon}_W${{</math>}} e {{<math>}}$N(T-1) - K${{</math>}}, respectivamente.
- Sejam a soma dos res√≠duos ao quadrado e os graus de liberdade do modelo pooled OLS {{<math>}}$\hat{\varepsilon}'_{OLS}\hat{\varepsilon}_{OLS}${{</math>}} e {{<math>}}$NT - K - 1${{</math>}}, respectivamente.
- Sob hip√≥tese nula de que n√£o h√° efeitos individuais, a estat√≠stica teste √© dada por
{{<math>}}\[ \frac{\hat{\varepsilon}'_{OLS} W \hat{\varepsilon}_{OLS} - \hat{\varepsilon}'_W\hat{\varepsilon}_W}{\hat{\varepsilon}'_W W \hat{\varepsilon}_W} \frac{NT - K - N + 1}{N-1} \]{{</math>}}
que segue uma distribui√ß√£o F de Fisher-Snedecor com {{<math>}}$N-1${{</math>}} e {{<math>}}$NT - K - N + 1${{</math>}} graus de liberdade.


### Aplicando no R
```{r}
data("TobinQ", package = "pder")
pTobinQ = pdata.frame(TobinQ, index=c("cusip", "year"))

Q.within = plm(ikn ~ qn, pTobinQ, model = "within")
Q.gls = plm(ikn ~ qn, pTobinQ, model = "random")
Q.pooling = plm(ikn ~ qn, pTobinQ, model = "pooling")

# Teste de Breusch-Pagan/LM
plmtest(Q.pooling, effect="individual") # Honda (1985)
```
O teste LM (Breusch-Pagan) acusou efeitos individuais significativos.

```{r}
# Teste F
pFtest(Q.within, Q.pooling)
```
Assim como o teste LM, Pelo teste F, observam-se efeitos individuais significativos.


</br>
## Testes de Efeitos Correlacionados
- Se√ß√£o 4.2 de "Panel Data Econometrics with R" (Croissant \& Millo, 2018)
- Continuamos assumindo que {{<math>}}$E(v|X) = 0${{</math>}}, em que {{<math>}}$v${{</math>}} √© o termo de erro idiossincr√°tico.
- Nestes testes, verificamos se {{<math>}}$E(u|X) = 0${{</math>}}, ou seja, se os efeitos individuais s√£o ou n√£o s√£o correlacionados com as covariadas.

### Teste de Hausman
- O princ√≠pio geral do teste de Hausman consiste em comparar dois modelos {{<math>}}$A${{</math>}}e {{<math>}}$B${{</math>}} tal que
    - sob {{<math>}}$H_0${{</math>}}: {{<math>}}$A${{</math>}} e {{<math>}}$B${{</math>}} s√£o ambos consistentes, mas {{<math>}}$B${{</math>}} √© mais eficiente que {{<math>}}$A${{</math>}}
    - sob {{<math>}}$H_1${{</math>}}: apenas {{<math>}}$A${{</math>}} √© consistente
- Se {{<math>}}$H_0${{</math>}} √© verdadeiro, ent√£o os coeficientes dos dois modelos n√£o devem divergir.
- O teste √© baseado em {{<math>}}$\hat{\beta}_A - \hat{\beta}_B${{</math>}} e Hausman mostrou que, sob {{<math>}}$H_0${{</math>}}, temos {{<math>}}$cov(\hat{\beta}_A, \hat{\beta}_B) = 0${{</math>}} e, logo, a vari√¢ncia dessa diferen√ßa √© simplesmente {{<math>}}$V(\hat{\beta}_A - \hat{\beta}_B) = V(\hat{\beta}_A) - V(\hat{\beta}_B)${{</math>}}

- No contexto de dados em pain√©is, compara-se o estimador _within_ (efeitos fixos) e o de GLS (efeitos aleat√≥rios)
- Quando {{<math>}}$E(u|X) = 0${{</math>}} ambos estimadores s√£o consistentes, ou seja,
{{<math>}}\[ \hat{q} \equiv \hat{\beta}_{GLS} - \hat{\beta}_W\ \overset{p}{\rightarrow}\ 0 \]{{</math>}}
ent√£o √© prefer√≠vel usar o mais eficiente (GLS, pois usa ambas varia√ß√µes inter e intra-indiv√≠duos).

- Se {{<math>}}$E(u|X) \neq 0${{</math>}}, ent√£o {{<math>}}$\hat{q} \equiv \hat{\beta}_{GLS} - \hat{\beta}_W \neq 0${{</math>}} e apenas o modelo robusto a {{<math>}}$E(u|X) \neq 0${{</math>}} (_within_) √© consistente.
- A vari√¢ncia √© dada por 
{{<math>}}\begin{align*}
    V(\hat{q}) &= V(\hat{\beta}_{GLS} - \hat{\beta}_W) = V(\hat{\beta}_{GLS}) + V(\hat{\beta}_W) - 2 cov(\hat{\beta}_{W}, \hat{\beta}_{GLS}) \\
    &= \sigma^2_v (Z' W Z)^{-1} - (Z'\Sigma^{-1} Z)^{-1}
\end{align*}{{</math>}}
- Logo, a estat√≠stica teste se torna
{{<math>}}\[ \hat{q}'\ V(\hat{q})^{-1}\ \hat{q} \]{{</math>}}
que, sob {{<math>}}$H_0${{</math>}}, √© distribuida como {{<math>}}$\chi^2${{</math>}} com {{<math>}}$K${{</math>}} graus de liberdade.

```{r}
# Teste de Hausman
phtest(Q.within, Q.gls)
```
N√£o se rejeita a hip√≥tese nula de que ambos modelos s√£o consistentes a 5\%.



{{< cta cta_text="üëâ Seguir para Manipula√ß√£o de Dados em Painel" cta_link="../sec5" >}}

